{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearningAss4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "k3Fg5hWUA83b",
        "hNrrLL4wl8_p",
        "4Oajgk_4-VE8"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4MUoYiaDV9H"
      },
      "source": [
        "Submitted by (IDs): 205381684, 312199698"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3Fg5hWUA83b"
      },
      "source": [
        "# Imports and Downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14GVo-_uA_ib"
      },
      "source": [
        "# Global libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.io import arff\n",
        "import time\n",
        "import pandas as pd\n",
        "from numpy import vstack\n",
        "import random\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dD_3_tF2qQq"
      },
      "source": [
        "# Sklearn - Preprocessa, RF\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, OrdinalEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJsqThLlAVCc",
        "outputId": "104c69ee-532b-41c7-c7d8-46f127f4dcad"
      },
      "source": [
        "pip install arff"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting arff\n",
            "  Downloading https://files.pythonhosted.org/packages/50/de/62d4446c5a6e459052c2f2d9490c370ddb6abc0766547b4cef585913598d/arff-0.9.tar.gz\n",
            "Building wheels for collected packages: arff\n",
            "  Building wheel for arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for arff: filename=arff-0.9-cp37-none-any.whl size=4970 sha256=deea545785560cc0e1fa457556f07b01e7701f11af7c9793ec10d7dd838a3231\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/d0/70/2c73afedd3ac25c6085b528742c69b9587cbdfa67e5194583b\n",
            "Successfully built arff\n",
            "Installing collected packages: arff\n",
            "Successfully installed arff-0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ7qYDAu7LFA"
      },
      "source": [
        "# GAN libraries\n",
        "import arff\n",
        "from scipy.io.arff import loadarff\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Activation, Flatten, Dropout, LeakyReLU, Dropout, Concatenate, Input, Embedding\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJif3LH7-Pfh"
      },
      "source": [
        "# BB Model libraries\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNrrLL4wl8_p"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPWL3uUFBCxb"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go4u57TQz6WD",
        "outputId": "449f8acb-592f-4131-f26f-32368d255e77"
      },
      "source": [
        "# load content from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoMLXS9oA1kl"
      },
      "source": [
        "data_base_path = '/content/drive/MyDrive/DeepLearningAss4/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxHkrq3QBL6d"
      },
      "source": [
        "# load data into dataframe\n",
        "def load_data(path_arff):\n",
        "  dataset = arff.loadarff(data_base_path + path_arff)\n",
        "  data = pd.DataFrame(dataset[0])\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "cljMauDfBcjL",
        "outputId": "075911b5-ea32-48c3-b772-fafb549648cb"
      },
      "source": [
        "# Load data_diabetes \n",
        "data_diabetes = load_data('diabetes.arff')\n",
        "data_diabetes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preg</th>\n",
              "      <th>plas</th>\n",
              "      <th>pres</th>\n",
              "      <th>skin</th>\n",
              "      <th>insu</th>\n",
              "      <th>mass</th>\n",
              "      <th>pedi</th>\n",
              "      <th>age</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50.0</td>\n",
              "      <td>b'tested_positive'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31.0</td>\n",
              "      <td>b'tested_negative'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32.0</td>\n",
              "      <td>b'tested_positive'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21.0</td>\n",
              "      <td>b'tested_negative'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33.0</td>\n",
              "      <td>b'tested_positive'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63.0</td>\n",
              "      <td>b'tested_negative'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27.0</td>\n",
              "      <td>b'tested_negative'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30.0</td>\n",
              "      <td>b'tested_negative'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47.0</td>\n",
              "      <td>b'tested_positive'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23.0</td>\n",
              "      <td>b'tested_negative'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     preg   plas  pres  skin   insu  mass   pedi   age               class\n",
              "0     6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0  b'tested_positive'\n",
              "1     1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0  b'tested_negative'\n",
              "2     8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0  b'tested_positive'\n",
              "3     1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0  b'tested_negative'\n",
              "4     0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0  b'tested_positive'\n",
              "..    ...    ...   ...   ...    ...   ...    ...   ...                 ...\n",
              "763  10.0  101.0  76.0  48.0  180.0  32.9  0.171  63.0  b'tested_negative'\n",
              "764   2.0  122.0  70.0  27.0    0.0  36.8  0.340  27.0  b'tested_negative'\n",
              "765   5.0  121.0  72.0  23.0  112.0  26.2  0.245  30.0  b'tested_negative'\n",
              "766   1.0  126.0  60.0   0.0    0.0  30.1  0.349  47.0  b'tested_positive'\n",
              "767   1.0   93.0  70.0  31.0    0.0  30.4  0.315  23.0  b'tested_negative'\n",
              "\n",
              "[768 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5MFuyeKGE0I",
        "outputId": "7b2936b5-df36-4ad4-9029-b3e7369b9d26"
      },
      "source": [
        "data_diabetes.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "1Qd7Imy16t7B",
        "outputId": "79f5543d-cc71-402f-a041-95dde4f21143"
      },
      "source": [
        "# load data_german_credit\n",
        "data_german_credit = load_data('german_credit.arff')\n",
        "data_german_credit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>6.0</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>1169.0</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>67.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b'A12'</td>\n",
              "      <td>48.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>5951.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A92'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>22.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'2'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b'A14'</td>\n",
              "      <td>12.0</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A46'</td>\n",
              "      <td>2096.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A74'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>3.0</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>49.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A172'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>42.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A42'</td>\n",
              "      <td>7882.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A74'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A103'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A122'</td>\n",
              "      <td>45.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A153'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>24.0</td>\n",
              "      <td>b'A33'</td>\n",
              "      <td>b'A40'</td>\n",
              "      <td>4870.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>3.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A124'</td>\n",
              "      <td>53.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A153'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'2'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>b'A14'</td>\n",
              "      <td>12.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A42'</td>\n",
              "      <td>1736.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A74'</td>\n",
              "      <td>3.0</td>\n",
              "      <td>b'A92'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>31.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A172'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>30.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A41'</td>\n",
              "      <td>3857.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A91'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A122'</td>\n",
              "      <td>40.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A174'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>b'A14'</td>\n",
              "      <td>12.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>804.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A123'</td>\n",
              "      <td>38.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>45.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>1845.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A124'</td>\n",
              "      <td>23.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A153'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'2'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>b'A12'</td>\n",
              "      <td>45.0</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A41'</td>\n",
              "      <td>4576.0</td>\n",
              "      <td>b'A62'</td>\n",
              "      <td>b'A71'</td>\n",
              "      <td>3.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A123'</td>\n",
              "      <td>27.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          1     2       3       4       5  ...       17   18       19       20    21\n",
              "0    b'A11'   6.0  b'A34'  b'A43'  1169.0  ...  b'A173'  1.0  b'A192'  b'A201'  b'1'\n",
              "1    b'A12'  48.0  b'A32'  b'A43'  5951.0  ...  b'A173'  1.0  b'A191'  b'A201'  b'2'\n",
              "2    b'A14'  12.0  b'A34'  b'A46'  2096.0  ...  b'A172'  2.0  b'A191'  b'A201'  b'1'\n",
              "3    b'A11'  42.0  b'A32'  b'A42'  7882.0  ...  b'A173'  2.0  b'A191'  b'A201'  b'1'\n",
              "4    b'A11'  24.0  b'A33'  b'A40'  4870.0  ...  b'A173'  2.0  b'A191'  b'A201'  b'2'\n",
              "..      ...   ...     ...     ...     ...  ...      ...  ...      ...      ...   ...\n",
              "995  b'A14'  12.0  b'A32'  b'A42'  1736.0  ...  b'A172'  1.0  b'A191'  b'A201'  b'1'\n",
              "996  b'A11'  30.0  b'A32'  b'A41'  3857.0  ...  b'A174'  1.0  b'A192'  b'A201'  b'1'\n",
              "997  b'A14'  12.0  b'A32'  b'A43'   804.0  ...  b'A173'  1.0  b'A191'  b'A201'  b'1'\n",
              "998  b'A11'  45.0  b'A32'  b'A43'  1845.0  ...  b'A173'  1.0  b'A192'  b'A201'  b'2'\n",
              "999  b'A12'  45.0  b'A34'  b'A41'  4576.0  ...  b'A173'  1.0  b'A191'  b'A201'  b'1'\n",
              "\n",
              "[1000 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT9LEcFbjzZk",
        "outputId": "bdf97181-a660-4a3a-99b9-cfbfa7290733"
      },
      "source": [
        "data_german_credit.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHOvsmpIDrqp",
        "outputId": "c41866c7-93a6-4861-a94e-1e8034bc650e"
      },
      "source": [
        "# data german credit data types\n",
        "data_german_credit.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 21 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   1       1000 non-null   object \n",
            " 1   2       1000 non-null   float64\n",
            " 2   3       1000 non-null   object \n",
            " 3   4       1000 non-null   object \n",
            " 4   5       1000 non-null   float64\n",
            " 5   6       1000 non-null   object \n",
            " 6   7       1000 non-null   object \n",
            " 7   8       1000 non-null   float64\n",
            " 8   9       1000 non-null   object \n",
            " 9   10      1000 non-null   object \n",
            " 10  11      1000 non-null   float64\n",
            " 11  12      1000 non-null   object \n",
            " 12  13      1000 non-null   float64\n",
            " 13  14      1000 non-null   object \n",
            " 14  15      1000 non-null   object \n",
            " 15  16      1000 non-null   float64\n",
            " 16  17      1000 non-null   object \n",
            " 17  18      1000 non-null   float64\n",
            " 18  19      1000 non-null   object \n",
            " 19  20      1000 non-null   object \n",
            " 20  21      1000 non-null   object \n",
            "dtypes: float64(7), object(14)\n",
            "memory usage: 164.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5XFnUmikTLw"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9xHMys1Sh0t"
      },
      "source": [
        "# transform categorical columns to one hot vectors\n",
        "def one_hot_encoding(df):\n",
        "  return pd.get_dummies(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tindYkOyFb8o"
      },
      "source": [
        "# transform categorical-ordinal columns to ordinal numbers\n",
        "def ordinal_encoding(df):\n",
        "  ord_enc = OrdinalEncoder()\n",
        "  df_encoded = ord_enc.fit_transform(df)\n",
        "  return df_encoded, ord_enc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRPcyr_rkSn3"
      },
      "source": [
        "# scale values between -1 and 1\n",
        "def scale(df):\n",
        "  mms = MinMaxScaler(feature_range=(-1,1))\n",
        "  data_scaled = mms.fit_transform(df)\n",
        "  return data_scaled, mms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3kHigQgPzju"
      },
      "source": [
        "##### Diabetes Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dg74GXYSayU"
      },
      "source": [
        "# pre process data diabetes data - numerical values only so used only scaling\n",
        "data_diabetes_scaled, mms_diabetes = scale(data_diabetes.iloc[:,:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4klB92MPxed"
      },
      "source": [
        "##### German Credit Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfTZGggNFkDZ"
      },
      "source": [
        "# german credit data - datatypes of columns according to datasets documnetation\n",
        "categorical = [2,3,8,9,11,13,14,16,18,19]\n",
        "ordinal = [0,5,6]\n",
        "numerical = [1,4,7,10,12,15,17]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du1mnSySFpXA"
      },
      "source": [
        "data_german_credit_encoded = pd.DataFrame()\n",
        "# encode categorical features to one hot vectors\n",
        "data_german_credit_encoded = one_hot_encoding(data_german_credit.iloc[:,categorical])\n",
        "# encode categorical-ordinal features to numberic\n",
        "data_german_credit_encoded[ordinal], ord_enc_german = ordinal_encoding(data_german_credit.iloc[:, ordinal])\n",
        "# scale all values between -1 to 1\n",
        "data_german_credit_encoded[numerical] = data_german_credit.iloc[:, numerical]\n",
        "numeric_col = ordinal + numerical\n",
        "data_german_credit_encoded, mms_german = scale(data_german_credit_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ASHOqM1axY"
      },
      "source": [
        "##### Shuffle & Batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbTdf5Stu6-U"
      },
      "source": [
        "# split dataset into batches and shuffle\n",
        "def shuffle_batch(df):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(df)\n",
        "  train_dataset = dataset.shuffle(len(dataset)).batch(32)\n",
        "  return train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCskLevkSIle"
      },
      "source": [
        "# split datasets to batches and shuffle\n",
        "train_data_diabetes = shuffle_batch(data_diabetes_scaled)\n",
        "train_data_german = shuffle_batch(data_german_credit_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlFE5JU4Apnh"
      },
      "source": [
        "## Diabets Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfTQYDvfqmN9"
      },
      "source": [
        "#### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkALaSGBis_m"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "latent_dim = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh3djHv8GYVS"
      },
      "source": [
        "### GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfApNnknVYwg"
      },
      "source": [
        "https://arxiv.org/pdf/1904.09135.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JCalf_O7A9w"
      },
      "source": [
        "#### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H20fvydp7ADn"
      },
      "source": [
        "# build discriminator model\n",
        "def build_disc(input_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(256, input_shape=(input_shape,)))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(1, activation='sigmoid')) \n",
        "\t# compile model\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])  \n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unDUnwWV7DE3"
      },
      "source": [
        "#### Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvMvsZcZGX4r"
      },
      "source": [
        "# build generator model\n",
        "def build_gen(output_shape, noise_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(512, input_shape=(noise_shape,)))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(256))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(output_shape, activation='tanh')) \n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQH0gSO7rov9"
      },
      "source": [
        "#### Test Discrimnator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g0HyOjkqZPU",
        "outputId": "c8cd6942-b08a-4b70-bca7-3290b855e448"
      },
      "source": [
        "# test discriminator without training the generatorto see if it succeeds in the classification task\n",
        "model_disc = build_disc(8)\n",
        "model_gen = build_gen(8, latent_dim)\n",
        "noise = tf.random.normal([128, latent_dim])\n",
        "X_fake = model_gen.predict(noise)\n",
        "x, y = vstack((data_diabetes_scaled, X_fake)), vstack((np.ones((data_diabetes_scaled.shape[0],1)), np.zeros((128,1))))\n",
        "history = model_disc.fit(x, y, epochs = 30) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 256)               2304      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 134,401\n",
            "Trainable params: 134,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 512)               4608      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 8)                 2056      \n",
            "=================================================================\n",
            "Total params: 137,992\n",
            "Trainable params: 137,992\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5578 - accuracy: 0.8594\n",
            "Epoch 2/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3513 - accuracy: 0.8839\n",
            "Epoch 3/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.2324 - accuracy: 0.8929\n",
            "Epoch 4/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1708 - accuracy: 0.9185\n",
            "Epoch 5/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1370 - accuracy: 0.9408\n",
            "Epoch 6/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1117 - accuracy: 0.9688\n",
            "Epoch 7/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0918 - accuracy: 0.9833\n",
            "Epoch 8/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0738 - accuracy: 0.9900\n",
            "Epoch 9/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0595 - accuracy: 0.9967\n",
            "Epoch 10/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0474 - accuracy: 0.9989\n",
            "Epoch 11/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0376 - accuracy: 1.0000\n",
            "Epoch 12/30\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 1.0000\n",
            "Epoch 13/30\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 1.0000\n",
            "Epoch 14/30\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 1.0000\n",
            "Epoch 15/30\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 1.0000\n",
            "Epoch 16/30\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0151 - accuracy: 1.0000\n",
            "Epoch 17/30\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.9989\n",
            "Epoch 18/30\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 1.0000\n",
            "Epoch 19/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9989\n",
            "Epoch 20/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0091 - accuracy: 1.0000\n",
            "Epoch 21/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 22/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 23/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 24/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 25/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 26/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 27/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 28/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 29/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 30/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g532T6N_rUve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a402a3-386f-4fab-9141-b5a7a835ad5b"
      },
      "source": [
        "# evaluate on real data\n",
        "model_disc.evaluate(data_diabetes_scaled, np.ones((data_diabetes_scaled.shape[0],1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0014943027636036277, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WIXEVxLrfKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0a17318-ffdb-453f-fcdf-001dd0e17c55"
      },
      "source": [
        "# evaluate on noise\n",
        "noise = tf.random.normal([128, latent_dim])\n",
        "X_fake = model_gen.predict(noise)\n",
        "model_disc.evaluate(X_fake, np.zeros((128,1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.012430811300873756, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrBcxS3a_KgS"
      },
      "source": [
        "### Run Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj8gm7T86NFD"
      },
      "source": [
        "##### plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rgkyjhWEt4W"
      },
      "source": [
        "# create a line plot of loss for the gan and save to file\n",
        "def plot_history(d_hist, g_hist, d_acc_hist, adv_acc_hist):\n",
        "  # plot loss\n",
        "  pyplot.subplot(2, 1, 1)\n",
        "  pyplot.plot(d_hist, label='discriminator loss')\n",
        "  pyplot.plot(g_hist, label='generetor loss')\n",
        "  pyplot.legend()\n",
        "  pyplot.title(\"Discriminator and Generator Loss\")\n",
        "  pyplot.xlabel(\"steps\")\n",
        "  pyplot.ylabel(\"loss\")\n",
        "  # plot discriminator accuracy\n",
        "  pyplot.subplot(2, 1, 2)\n",
        "  pyplot.plot(d_acc_hist, label='disc_acc')\n",
        "  pyplot.plot(adv_acc_hist, label='adv_acc')\n",
        "  pyplot.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOqeSsJ-6Oxb"
      },
      "source": [
        "#### losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_rq_c6W_KEd"
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "\n",
        "# calcuate the discriminator loss based on https://www.tensorflow.org/tutorials/generative/dcgan\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss, real_loss, fake_loss\n",
        "\n",
        "# calcuate the generator loss based on https://www.tensorflow.org/tutorials/generative/dcgan\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS31jnL0rorM"
      },
      "source": [
        "# define optimizers and metrics\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2*1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2*1e-4)\n",
        "\n",
        "disc_accuracy_tracker = tf.keras.metrics.Accuracy(name=\"disc_accuracy\")\n",
        "adv_accuracy_tracker = tf.keras.metrics.Accuracy(name=\"adv_accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncEybhgTuiWW"
      },
      "source": [
        "# calculate discriminator loss on real and fake samples separately - while interval is close to 1 and 0\n",
        "def discriminator_real_loss(real_output):\n",
        "    real_loss = cross_entropy(tf.random.uniform((real_output.shape[0],1),minval=0.7, maxval=1.2), real_output)\n",
        "    return real_loss\n",
        "\n",
        "def discriminator_fake_loss(fake_output):\n",
        "    fake_loss = cross_entropy(tf.random.uniform((fake_output.shape[0],1),minval=0.0, maxval=0.3), fake_output)\n",
        "    return fake_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQIgw2O96QTs"
      },
      "source": [
        "#### train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBCSRdyP_g_r"
      },
      "source": [
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(records):\n",
        "    num_sampels = len(records)\n",
        "    \n",
        "    noise = tf.random.normal([num_sampels, latent_dim]) # random noise\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape, tf.GradientTape() as disc_fake_tape:  \n",
        "      generated_records = generator(noise, training=True) # generate samples from noise\n",
        "\n",
        "      real_output = discriminator(records, training=True) # feed disc with real data\n",
        "      fake_output = discriminator(generated_records, training=True) # feed disc with fake data\n",
        "      \n",
        "      gen_loss = generator_loss(fake_output)  # calculate gen loss on fake data\n",
        "      total_disc_loss, real_loss, fake_loss  = discriminator_loss(real_output, fake_output) # calculate disc loss on both fake and real data\n",
        "      real_new_loss = discriminator_real_loss(real_output)  # calculate disc loss on real data\n",
        "      fake_new_loss = discriminator_fake_loss(fake_output)  # calculate disc loss on fake data\n",
        "\n",
        "    # caluclate gradients of gen and disc\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator_real = disc_tape.gradient(real_new_loss, discriminator.trainable_variables)\n",
        "    gradients_of_discriminator_fake = disc_fake_tape.gradient(fake_new_loss, discriminator.trainable_variables)\n",
        "\n",
        "    # apply gradients to both of the models\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator_real, discriminator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator_fake, discriminator.trainable_variables))\n",
        "\n",
        "    # round prediction\n",
        "    rounded_real = tf.math.round(real_output)\n",
        "    rounded_fake = tf.math.round(fake_output)\n",
        "    \n",
        "    # cocatenate disc prediction\n",
        "    disc_output = tf.concat([rounded_real,rounded_fake], 0)\n",
        "    disc_real = tf.concat([np.ones((num_sampels, 1)), np.zeros((num_sampels, 1))], axis =0)\n",
        "    \n",
        "    # calculate disc accuracy\n",
        "    disc_accuracy_tracker.update_state(disc_real, disc_output)\n",
        "    adv_accuracy_tracker.update_state(np.ones((num_sampels, 1)), rounded_fake)\n",
        "\n",
        "    return total_disc_loss, real_loss, fake_loss, gen_loss, disc_accuracy_tracker.result(), adv_accuracy_tracker.result()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vqLQpDa_i0l"
      },
      "source": [
        "d_hist, d_r_hist, d_f_hist, g_hist, d_acc_hist, adv_acc_hist = list(), list(), list(), list(), list(), list()\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    for image_batch in dataset:\n",
        "      d_loss, real_loss, fake_loss, g_loss, d_accuracy, g_accuracy =  train_step(image_batch)\n",
        "      # record history\n",
        "      d_hist.append(d_loss.numpy())\n",
        "      d_r_hist.append(real_loss.numpy())\n",
        "      d_f_hist.append(fake_loss.numpy())\n",
        "      g_hist.append(g_loss.numpy())\n",
        "      d_acc_hist.append(d_accuracy.numpy())\n",
        "      adv_acc_hist.append(g_accuracy.numpy())\n",
        "    print('>%d, d_loss=%.3f,d_R_loss=%.3f,d_f_loss=%.3f, g_loss=%.3f, d_acc=%d, aadv_acc=%d' %\n",
        "\t\t\t(epoch, d_hist[-1], d_r_hist[-1], d_f_hist[-1],  g_hist[-1], int(100*d_acc_hist[-1]), int(100*adv_acc_hist[-1])))\n",
        "  \n",
        "  plot_history(d_r_hist, d_f_hist, d_acc_hist, adv_acc_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CfXkJUJLBZhc",
        "outputId": "ac3280fb-3ebf-48f7-8458-6a2455a06e32"
      },
      "source": [
        "discriminator = build_disc(8)\n",
        "generator = build_gen(8, latent_dim)\n",
        "train(train_data_diabetes, 200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_42 (Dense)             (None, 256)               2304      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_28 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_29 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 134,401\n",
            "Trainable params: 134,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_45 (Dense)             (None, 512)               4608      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_30 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_31 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 8)                 2056      \n",
            "=================================================================\n",
            "Total params: 137,992\n",
            "Trainable params: 137,992\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            ">0, d_loss=1.357,d_R_loss=0.259,d_f_loss=1.099, g_loss=0.414, d_acc=50, aadv_acc=98\n",
            ">1, d_loss=1.630,d_R_loss=0.642,d_f_loss=0.988, g_loss=0.469, d_acc=48, aadv_acc=99\n",
            ">2, d_loss=1.042,d_R_loss=0.576,d_f_loss=0.466, g_loss=0.993, d_acc=54, aadv_acc=79\n",
            ">3, d_loss=1.262,d_R_loss=0.510,d_f_loss=0.752, g_loss=0.667, d_acc=60, aadv_acc=68\n",
            ">4, d_loss=1.370,d_R_loss=0.794,d_f_loss=0.577, g_loss=0.830, d_acc=56, aadv_acc=70\n",
            ">5, d_loss=0.821,d_R_loss=0.469,d_f_loss=0.352, g_loss=1.222, d_acc=61, aadv_acc=58\n",
            ">6, d_loss=0.649,d_R_loss=0.250,d_f_loss=0.398, g_loss=1.150, d_acc=66, aadv_acc=50\n",
            ">7, d_loss=1.555,d_R_loss=0.525,d_f_loss=1.030, g_loss=0.561, d_acc=68, aadv_acc=47\n",
            ">8, d_loss=1.518,d_R_loss=0.592,d_f_loss=0.926, g_loss=0.568, d_acc=67, aadv_acc=48\n",
            ">9, d_loss=0.954,d_R_loss=0.418,d_f_loss=0.536, g_loss=1.139, d_acc=67, aadv_acc=45\n",
            ">10, d_loss=0.711,d_R_loss=0.400,d_f_loss=0.311, g_loss=1.383, d_acc=69, aadv_acc=42\n",
            ">11, d_loss=0.839,d_R_loss=0.559,d_f_loss=0.280, g_loss=1.466, d_acc=71, aadv_acc=39\n",
            ">12, d_loss=0.513,d_R_loss=0.287,d_f_loss=0.226, g_loss=1.762, d_acc=73, aadv_acc=36\n",
            ">13, d_loss=0.965,d_R_loss=0.558,d_f_loss=0.406, g_loss=1.333, d_acc=73, aadv_acc=35\n",
            ">14, d_loss=1.158,d_R_loss=0.403,d_f_loss=0.755, g_loss=1.007, d_acc=73, aadv_acc=35\n",
            ">15, d_loss=1.465,d_R_loss=0.785,d_f_loss=0.681, g_loss=0.815, d_acc=72, aadv_acc=36\n",
            ">16, d_loss=0.768,d_R_loss=0.456,d_f_loss=0.311, g_loss=1.359, d_acc=72, aadv_acc=35\n",
            ">17, d_loss=0.711,d_R_loss=0.310,d_f_loss=0.401, g_loss=1.345, d_acc=73, aadv_acc=33\n",
            ">18, d_loss=1.118,d_R_loss=0.399,d_f_loss=0.719, g_loss=0.787, d_acc=74, aadv_acc=32\n",
            ">19, d_loss=2.048,d_R_loss=0.905,d_f_loss=1.143, g_loss=0.403, d_acc=73, aadv_acc=34\n",
            ">20, d_loss=1.408,d_R_loss=0.724,d_f_loss=0.685, g_loss=0.713, d_acc=70, aadv_acc=36\n",
            ">21, d_loss=1.168,d_R_loss=0.585,d_f_loss=0.582, g_loss=0.852, d_acc=71, aadv_acc=35\n",
            ">22, d_loss=1.233,d_R_loss=0.500,d_f_loss=0.733, g_loss=0.679, d_acc=71, aadv_acc=35\n",
            ">23, d_loss=1.488,d_R_loss=0.631,d_f_loss=0.858, g_loss=0.572, d_acc=70, aadv_acc=37\n",
            ">24, d_loss=1.389,d_R_loss=0.619,d_f_loss=0.770, g_loss=0.643, d_acc=69, aadv_acc=39\n",
            ">25, d_loss=1.277,d_R_loss=0.500,d_f_loss=0.777, g_loss=0.627, d_acc=69, aadv_acc=40\n",
            ">26, d_loss=1.338,d_R_loss=0.512,d_f_loss=0.826, g_loss=0.599, d_acc=68, aadv_acc=41\n",
            ">27, d_loss=1.287,d_R_loss=0.452,d_f_loss=0.835, g_loss=0.594, d_acc=68, aadv_acc=42\n",
            ">28, d_loss=1.469,d_R_loss=0.581,d_f_loss=0.889, g_loss=0.543, d_acc=67, aadv_acc=43\n",
            ">29, d_loss=1.490,d_R_loss=0.542,d_f_loss=0.947, g_loss=0.509, d_acc=66, aadv_acc=45\n",
            ">30, d_loss=1.585,d_R_loss=0.620,d_f_loss=0.965, g_loss=0.503, d_acc=65, aadv_acc=47\n",
            ">31, d_loss=1.444,d_R_loss=0.519,d_f_loss=0.926, g_loss=0.584, d_acc=65, aadv_acc=48\n",
            ">32, d_loss=1.381,d_R_loss=0.541,d_f_loss=0.840, g_loss=0.583, d_acc=65, aadv_acc=48\n",
            ">33, d_loss=1.327,d_R_loss=0.523,d_f_loss=0.805, g_loss=0.605, d_acc=64, aadv_acc=50\n",
            ">34, d_loss=1.483,d_R_loss=0.636,d_f_loss=0.847, g_loss=0.580, d_acc=64, aadv_acc=50\n",
            ">35, d_loss=1.478,d_R_loss=0.626,d_f_loss=0.852, g_loss=0.565, d_acc=63, aadv_acc=51\n",
            ">36, d_loss=1.415,d_R_loss=0.603,d_f_loss=0.812, g_loss=0.590, d_acc=63, aadv_acc=53\n",
            ">37, d_loss=1.392,d_R_loss=0.561,d_f_loss=0.832, g_loss=0.580, d_acc=62, aadv_acc=54\n",
            ">38, d_loss=1.386,d_R_loss=0.569,d_f_loss=0.817, g_loss=0.591, d_acc=62, aadv_acc=55\n",
            ">39, d_loss=1.311,d_R_loss=0.537,d_f_loss=0.774, g_loss=0.630, d_acc=61, aadv_acc=55\n",
            ">40, d_loss=1.277,d_R_loss=0.546,d_f_loss=0.730, g_loss=0.667, d_acc=61, aadv_acc=56\n",
            ">41, d_loss=1.374,d_R_loss=0.605,d_f_loss=0.769, g_loss=0.631, d_acc=61, aadv_acc=56\n",
            ">42, d_loss=1.475,d_R_loss=0.655,d_f_loss=0.820, g_loss=0.596, d_acc=61, aadv_acc=56\n",
            ">43, d_loss=1.396,d_R_loss=0.638,d_f_loss=0.758, g_loss=0.648, d_acc=60, aadv_acc=57\n",
            ">44, d_loss=1.132,d_R_loss=0.541,d_f_loss=0.590, g_loss=0.813, d_acc=61, aadv_acc=56\n",
            ">45, d_loss=1.271,d_R_loss=0.546,d_f_loss=0.725, g_loss=0.680, d_acc=61, aadv_acc=55\n",
            ">46, d_loss=1.345,d_R_loss=0.569,d_f_loss=0.776, g_loss=0.632, d_acc=61, aadv_acc=56\n",
            ">47, d_loss=1.539,d_R_loss=0.720,d_f_loss=0.819, g_loss=0.617, d_acc=60, aadv_acc=56\n",
            ">48, d_loss=1.296,d_R_loss=0.608,d_f_loss=0.688, g_loss=0.765, d_acc=60, aadv_acc=56\n",
            ">49, d_loss=1.299,d_R_loss=0.489,d_f_loss=0.810, g_loss=0.650, d_acc=60, aadv_acc=55\n",
            ">50, d_loss=1.365,d_R_loss=0.552,d_f_loss=0.813, g_loss=0.625, d_acc=60, aadv_acc=55\n",
            ">51, d_loss=1.485,d_R_loss=0.569,d_f_loss=0.916, g_loss=0.540, d_acc=60, aadv_acc=55\n",
            ">52, d_loss=1.339,d_R_loss=0.611,d_f_loss=0.728, g_loss=0.693, d_acc=60, aadv_acc=56\n",
            ">53, d_loss=1.430,d_R_loss=0.593,d_f_loss=0.837, g_loss=0.611, d_acc=60, aadv_acc=56\n",
            ">54, d_loss=1.358,d_R_loss=0.512,d_f_loss=0.846, g_loss=0.587, d_acc=60, aadv_acc=56\n",
            ">55, d_loss=1.480,d_R_loss=0.595,d_f_loss=0.885, g_loss=0.556, d_acc=60, aadv_acc=56\n",
            ">56, d_loss=1.417,d_R_loss=0.628,d_f_loss=0.788, g_loss=0.617, d_acc=60, aadv_acc=57\n",
            ">57, d_loss=1.331,d_R_loss=0.578,d_f_loss=0.753, g_loss=0.661, d_acc=59, aadv_acc=57\n",
            ">58, d_loss=1.370,d_R_loss=0.577,d_f_loss=0.793, g_loss=0.616, d_acc=59, aadv_acc=57\n",
            ">59, d_loss=1.372,d_R_loss=0.638,d_f_loss=0.733, g_loss=0.668, d_acc=59, aadv_acc=58\n",
            ">60, d_loss=1.372,d_R_loss=0.611,d_f_loss=0.761, g_loss=0.638, d_acc=59, aadv_acc=58\n",
            ">61, d_loss=1.328,d_R_loss=0.605,d_f_loss=0.723, g_loss=0.669, d_acc=59, aadv_acc=58\n",
            ">62, d_loss=1.312,d_R_loss=0.574,d_f_loss=0.738, g_loss=0.658, d_acc=59, aadv_acc=58\n",
            ">63, d_loss=1.252,d_R_loss=0.503,d_f_loss=0.749, g_loss=0.650, d_acc=59, aadv_acc=58\n",
            ">64, d_loss=1.287,d_R_loss=0.536,d_f_loss=0.751, g_loss=0.654, d_acc=59, aadv_acc=58\n",
            ">65, d_loss=1.490,d_R_loss=0.627,d_f_loss=0.864, g_loss=0.570, d_acc=59, aadv_acc=58\n",
            ">66, d_loss=1.527,d_R_loss=0.661,d_f_loss=0.866, g_loss=0.555, d_acc=58, aadv_acc=59\n",
            ">67, d_loss=1.354,d_R_loss=0.629,d_f_loss=0.725, g_loss=0.673, d_acc=58, aadv_acc=59\n",
            ">68, d_loss=1.137,d_R_loss=0.480,d_f_loss=0.657, g_loss=0.750, d_acc=58, aadv_acc=59\n",
            ">69, d_loss=1.398,d_R_loss=0.522,d_f_loss=0.875, g_loss=0.552, d_acc=58, aadv_acc=59\n",
            ">70, d_loss=1.429,d_R_loss=0.593,d_f_loss=0.836, g_loss=0.583, d_acc=58, aadv_acc=60\n",
            ">71, d_loss=1.479,d_R_loss=0.665,d_f_loss=0.814, g_loss=0.601, d_acc=58, aadv_acc=60\n",
            ">72, d_loss=1.385,d_R_loss=0.573,d_f_loss=0.812, g_loss=0.614, d_acc=58, aadv_acc=60\n",
            ">73, d_loss=1.407,d_R_loss=0.566,d_f_loss=0.840, g_loss=0.585, d_acc=58, aadv_acc=60\n",
            ">74, d_loss=1.349,d_R_loss=0.637,d_f_loss=0.712, g_loss=0.684, d_acc=57, aadv_acc=61\n",
            ">75, d_loss=1.281,d_R_loss=0.598,d_f_loss=0.684, g_loss=0.713, d_acc=57, aadv_acc=61\n",
            ">76, d_loss=1.348,d_R_loss=0.590,d_f_loss=0.758, g_loss=0.635, d_acc=57, aadv_acc=61\n",
            ">77, d_loss=1.354,d_R_loss=0.583,d_f_loss=0.771, g_loss=0.624, d_acc=57, aadv_acc=61\n",
            ">78, d_loss=1.379,d_R_loss=0.626,d_f_loss=0.753, g_loss=0.648, d_acc=57, aadv_acc=61\n",
            ">79, d_loss=1.429,d_R_loss=0.606,d_f_loss=0.823, g_loss=0.596, d_acc=57, aadv_acc=61\n",
            ">80, d_loss=1.312,d_R_loss=0.583,d_f_loss=0.729, g_loss=0.666, d_acc=57, aadv_acc=62\n",
            ">81, d_loss=1.191,d_R_loss=0.479,d_f_loss=0.712, g_loss=0.696, d_acc=57, aadv_acc=61\n",
            ">82, d_loss=1.402,d_R_loss=0.519,d_f_loss=0.883, g_loss=0.560, d_acc=57, aadv_acc=61\n",
            ">83, d_loss=1.424,d_R_loss=0.648,d_f_loss=0.776, g_loss=0.631, d_acc=57, aadv_acc=62\n",
            ">84, d_loss=1.392,d_R_loss=0.664,d_f_loss=0.729, g_loss=0.670, d_acc=57, aadv_acc=62\n",
            ">85, d_loss=1.366,d_R_loss=0.641,d_f_loss=0.725, g_loss=0.680, d_acc=57, aadv_acc=62\n",
            ">86, d_loss=1.440,d_R_loss=0.652,d_f_loss=0.788, g_loss=0.624, d_acc=57, aadv_acc=62\n",
            ">87, d_loss=1.369,d_R_loss=0.608,d_f_loss=0.761, g_loss=0.669, d_acc=57, aadv_acc=62\n",
            ">88, d_loss=1.364,d_R_loss=0.572,d_f_loss=0.792, g_loss=0.632, d_acc=57, aadv_acc=62\n",
            ">89, d_loss=1.381,d_R_loss=0.630,d_f_loss=0.751, g_loss=0.647, d_acc=57, aadv_acc=62\n",
            ">90, d_loss=1.323,d_R_loss=0.647,d_f_loss=0.677, g_loss=0.717, d_acc=57, aadv_acc=62\n",
            ">91, d_loss=1.353,d_R_loss=0.530,d_f_loss=0.823, g_loss=0.594, d_acc=57, aadv_acc=62\n",
            ">92, d_loss=1.376,d_R_loss=0.527,d_f_loss=0.849, g_loss=0.578, d_acc=57, aadv_acc=62\n",
            ">93, d_loss=1.486,d_R_loss=0.592,d_f_loss=0.895, g_loss=0.551, d_acc=56, aadv_acc=62\n",
            ">94, d_loss=1.356,d_R_loss=0.545,d_f_loss=0.812, g_loss=0.605, d_acc=57, aadv_acc=62\n",
            ">95, d_loss=1.304,d_R_loss=0.498,d_f_loss=0.805, g_loss=0.602, d_acc=57, aadv_acc=62\n",
            ">96, d_loss=1.379,d_R_loss=0.527,d_f_loss=0.852, g_loss=0.569, d_acc=56, aadv_acc=62\n",
            ">97, d_loss=1.437,d_R_loss=0.659,d_f_loss=0.779, g_loss=0.643, d_acc=56, aadv_acc=63\n",
            ">98, d_loss=1.449,d_R_loss=0.593,d_f_loss=0.856, g_loss=0.576, d_acc=56, aadv_acc=63\n",
            ">99, d_loss=1.571,d_R_loss=0.658,d_f_loss=0.913, g_loss=0.522, d_acc=56, aadv_acc=63\n",
            ">100, d_loss=1.358,d_R_loss=0.609,d_f_loss=0.749, g_loss=0.645, d_acc=56, aadv_acc=63\n",
            ">101, d_loss=1.339,d_R_loss=0.625,d_f_loss=0.714, g_loss=0.679, d_acc=56, aadv_acc=63\n",
            ">102, d_loss=1.378,d_R_loss=0.557,d_f_loss=0.821, g_loss=0.583, d_acc=56, aadv_acc=63\n",
            ">103, d_loss=1.449,d_R_loss=0.667,d_f_loss=0.782, g_loss=0.616, d_acc=56, aadv_acc=64\n",
            ">104, d_loss=1.355,d_R_loss=0.591,d_f_loss=0.764, g_loss=0.633, d_acc=56, aadv_acc=64\n",
            ">105, d_loss=1.400,d_R_loss=0.623,d_f_loss=0.777, g_loss=0.623, d_acc=56, aadv_acc=64\n",
            ">106, d_loss=1.376,d_R_loss=0.587,d_f_loss=0.790, g_loss=0.620, d_acc=56, aadv_acc=64\n",
            ">107, d_loss=1.370,d_R_loss=0.529,d_f_loss=0.840, g_loss=0.573, d_acc=56, aadv_acc=64\n",
            ">108, d_loss=1.442,d_R_loss=0.579,d_f_loss=0.863, g_loss=0.558, d_acc=56, aadv_acc=64\n",
            ">109, d_loss=1.382,d_R_loss=0.580,d_f_loss=0.802, g_loss=0.603, d_acc=56, aadv_acc=65\n",
            ">110, d_loss=1.311,d_R_loss=0.579,d_f_loss=0.732, g_loss=0.661, d_acc=56, aadv_acc=65\n",
            ">111, d_loss=1.293,d_R_loss=0.511,d_f_loss=0.783, g_loss=0.622, d_acc=56, aadv_acc=65\n",
            ">112, d_loss=1.505,d_R_loss=0.642,d_f_loss=0.862, g_loss=0.557, d_acc=56, aadv_acc=65\n",
            ">113, d_loss=1.438,d_R_loss=0.645,d_f_loss=0.793, g_loss=0.609, d_acc=55, aadv_acc=65\n",
            ">114, d_loss=1.307,d_R_loss=0.644,d_f_loss=0.663, g_loss=0.728, d_acc=55, aadv_acc=65\n",
            ">115, d_loss=1.260,d_R_loss=0.579,d_f_loss=0.681, g_loss=0.713, d_acc=55, aadv_acc=65\n",
            ">116, d_loss=1.336,d_R_loss=0.558,d_f_loss=0.777, g_loss=0.634, d_acc=55, aadv_acc=65\n",
            ">117, d_loss=1.398,d_R_loss=0.598,d_f_loss=0.801, g_loss=0.602, d_acc=55, aadv_acc=65\n",
            ">118, d_loss=1.340,d_R_loss=0.603,d_f_loss=0.737, g_loss=0.658, d_acc=55, aadv_acc=65\n",
            ">119, d_loss=1.307,d_R_loss=0.559,d_f_loss=0.748, g_loss=0.657, d_acc=55, aadv_acc=65\n",
            ">120, d_loss=1.455,d_R_loss=0.582,d_f_loss=0.873, g_loss=0.554, d_acc=55, aadv_acc=65\n",
            ">121, d_loss=1.392,d_R_loss=0.570,d_f_loss=0.822, g_loss=0.598, d_acc=55, aadv_acc=65\n",
            ">122, d_loss=1.406,d_R_loss=0.611,d_f_loss=0.795, g_loss=0.623, d_acc=55, aadv_acc=65\n",
            ">123, d_loss=1.309,d_R_loss=0.575,d_f_loss=0.734, g_loss=0.690, d_acc=55, aadv_acc=65\n",
            ">124, d_loss=1.219,d_R_loss=0.583,d_f_loss=0.636, g_loss=0.775, d_acc=55, aadv_acc=65\n",
            ">125, d_loss=1.533,d_R_loss=0.649,d_f_loss=0.884, g_loss=0.552, d_acc=55, aadv_acc=65\n",
            ">126, d_loss=1.553,d_R_loss=0.710,d_f_loss=0.844, g_loss=0.584, d_acc=55, aadv_acc=65\n",
            ">127, d_loss=1.310,d_R_loss=0.624,d_f_loss=0.685, g_loss=0.717, d_acc=55, aadv_acc=65\n",
            ">128, d_loss=1.378,d_R_loss=0.600,d_f_loss=0.778, g_loss=0.632, d_acc=55, aadv_acc=65\n",
            ">129, d_loss=1.327,d_R_loss=0.599,d_f_loss=0.728, g_loss=0.666, d_acc=55, aadv_acc=65\n",
            ">130, d_loss=1.277,d_R_loss=0.560,d_f_loss=0.717, g_loss=0.711, d_acc=55, aadv_acc=65\n",
            ">131, d_loss=1.412,d_R_loss=0.614,d_f_loss=0.798, g_loss=0.619, d_acc=55, aadv_acc=65\n",
            ">132, d_loss=1.497,d_R_loss=0.649,d_f_loss=0.848, g_loss=0.567, d_acc=55, aadv_acc=65\n",
            ">133, d_loss=1.411,d_R_loss=0.605,d_f_loss=0.806, g_loss=0.605, d_acc=55, aadv_acc=66\n",
            ">134, d_loss=1.367,d_R_loss=0.564,d_f_loss=0.804, g_loss=0.606, d_acc=55, aadv_acc=66\n",
            ">135, d_loss=1.336,d_R_loss=0.536,d_f_loss=0.799, g_loss=0.601, d_acc=55, aadv_acc=66\n",
            ">136, d_loss=1.446,d_R_loss=0.608,d_f_loss=0.838, g_loss=0.580, d_acc=55, aadv_acc=66\n",
            ">137, d_loss=1.484,d_R_loss=0.625,d_f_loss=0.860, g_loss=0.561, d_acc=55, aadv_acc=66\n",
            ">138, d_loss=1.397,d_R_loss=0.619,d_f_loss=0.778, g_loss=0.621, d_acc=55, aadv_acc=66\n",
            ">139, d_loss=1.345,d_R_loss=0.595,d_f_loss=0.751, g_loss=0.642, d_acc=55, aadv_acc=66\n",
            ">140, d_loss=1.362,d_R_loss=0.593,d_f_loss=0.768, g_loss=0.627, d_acc=55, aadv_acc=67\n",
            ">141, d_loss=1.416,d_R_loss=0.644,d_f_loss=0.772, g_loss=0.631, d_acc=55, aadv_acc=67\n",
            ">142, d_loss=1.330,d_R_loss=0.623,d_f_loss=0.707, g_loss=0.705, d_acc=55, aadv_acc=66\n",
            ">143, d_loss=1.302,d_R_loss=0.580,d_f_loss=0.722, g_loss=0.687, d_acc=55, aadv_acc=66\n",
            ">144, d_loss=1.210,d_R_loss=0.538,d_f_loss=0.672, g_loss=0.726, d_acc=55, aadv_acc=66\n",
            ">145, d_loss=1.384,d_R_loss=0.547,d_f_loss=0.837, g_loss=0.587, d_acc=55, aadv_acc=66\n",
            ">146, d_loss=1.359,d_R_loss=0.592,d_f_loss=0.767, g_loss=0.631, d_acc=55, aadv_acc=66\n",
            ">147, d_loss=1.327,d_R_loss=0.577,d_f_loss=0.751, g_loss=0.666, d_acc=55, aadv_acc=66\n",
            ">148, d_loss=1.305,d_R_loss=0.552,d_f_loss=0.753, g_loss=0.641, d_acc=55, aadv_acc=66\n",
            ">149, d_loss=1.412,d_R_loss=0.615,d_f_loss=0.797, g_loss=0.619, d_acc=55, aadv_acc=66\n",
            ">150, d_loss=1.291,d_R_loss=0.592,d_f_loss=0.699, g_loss=0.694, d_acc=55, aadv_acc=66\n",
            ">151, d_loss=1.211,d_R_loss=0.501,d_f_loss=0.709, g_loss=0.685, d_acc=55, aadv_acc=66\n",
            ">152, d_loss=1.326,d_R_loss=0.507,d_f_loss=0.820, g_loss=0.627, d_acc=55, aadv_acc=66\n",
            ">153, d_loss=1.483,d_R_loss=0.581,d_f_loss=0.902, g_loss=0.540, d_acc=55, aadv_acc=66\n",
            ">154, d_loss=1.487,d_R_loss=0.629,d_f_loss=0.858, g_loss=0.568, d_acc=55, aadv_acc=67\n",
            ">155, d_loss=1.236,d_R_loss=0.549,d_f_loss=0.687, g_loss=0.709, d_acc=55, aadv_acc=67\n",
            ">156, d_loss=1.336,d_R_loss=0.588,d_f_loss=0.748, g_loss=0.652, d_acc=55, aadv_acc=67\n",
            ">157, d_loss=1.463,d_R_loss=0.599,d_f_loss=0.864, g_loss=0.556, d_acc=55, aadv_acc=67\n",
            ">158, d_loss=1.365,d_R_loss=0.612,d_f_loss=0.752, g_loss=0.645, d_acc=55, aadv_acc=67\n",
            ">159, d_loss=1.244,d_R_loss=0.552,d_f_loss=0.693, g_loss=0.706, d_acc=55, aadv_acc=67\n",
            ">160, d_loss=1.323,d_R_loss=0.514,d_f_loss=0.809, g_loss=0.614, d_acc=55, aadv_acc=67\n",
            ">161, d_loss=1.497,d_R_loss=0.540,d_f_loss=0.957, g_loss=0.501, d_acc=55, aadv_acc=67\n",
            ">162, d_loss=1.433,d_R_loss=0.560,d_f_loss=0.873, g_loss=0.544, d_acc=55, aadv_acc=67\n",
            ">163, d_loss=1.345,d_R_loss=0.550,d_f_loss=0.795, g_loss=0.605, d_acc=55, aadv_acc=67\n",
            ">164, d_loss=1.258,d_R_loss=0.536,d_f_loss=0.722, g_loss=0.675, d_acc=55, aadv_acc=67\n",
            ">165, d_loss=1.297,d_R_loss=0.507,d_f_loss=0.790, g_loss=0.617, d_acc=55, aadv_acc=67\n",
            ">166, d_loss=1.366,d_R_loss=0.556,d_f_loss=0.811, g_loss=0.597, d_acc=55, aadv_acc=67\n",
            ">167, d_loss=1.406,d_R_loss=0.609,d_f_loss=0.797, g_loss=0.606, d_acc=54, aadv_acc=67\n",
            ">168, d_loss=1.349,d_R_loss=0.608,d_f_loss=0.740, g_loss=0.654, d_acc=54, aadv_acc=67\n",
            ">169, d_loss=1.428,d_R_loss=0.652,d_f_loss=0.776, g_loss=0.622, d_acc=54, aadv_acc=67\n",
            ">170, d_loss=1.354,d_R_loss=0.626,d_f_loss=0.729, g_loss=0.670, d_acc=54, aadv_acc=68\n",
            ">171, d_loss=1.276,d_R_loss=0.559,d_f_loss=0.717, g_loss=0.679, d_acc=54, aadv_acc=68\n",
            ">172, d_loss=1.357,d_R_loss=0.649,d_f_loss=0.709, g_loss=0.690, d_acc=54, aadv_acc=67\n",
            ">173, d_loss=1.349,d_R_loss=0.579,d_f_loss=0.771, g_loss=0.658, d_acc=54, aadv_acc=67\n",
            ">174, d_loss=1.512,d_R_loss=0.618,d_f_loss=0.894, g_loss=0.550, d_acc=54, aadv_acc=67\n",
            ">175, d_loss=1.512,d_R_loss=0.622,d_f_loss=0.889, g_loss=0.548, d_acc=54, aadv_acc=67\n",
            ">176, d_loss=1.241,d_R_loss=0.516,d_f_loss=0.725, g_loss=0.733, d_acc=54, aadv_acc=67\n",
            ">177, d_loss=1.304,d_R_loss=0.525,d_f_loss=0.779, g_loss=0.648, d_acc=54, aadv_acc=68\n",
            ">178, d_loss=1.473,d_R_loss=0.620,d_f_loss=0.854, g_loss=0.569, d_acc=54, aadv_acc=68\n",
            ">179, d_loss=1.447,d_R_loss=0.642,d_f_loss=0.805, g_loss=0.603, d_acc=54, aadv_acc=68\n",
            ">180, d_loss=1.507,d_R_loss=0.698,d_f_loss=0.809, g_loss=0.599, d_acc=54, aadv_acc=68\n",
            ">181, d_loss=1.415,d_R_loss=0.625,d_f_loss=0.790, g_loss=0.610, d_acc=54, aadv_acc=68\n",
            ">182, d_loss=1.301,d_R_loss=0.583,d_f_loss=0.717, g_loss=0.676, d_acc=54, aadv_acc=68\n",
            ">183, d_loss=1.347,d_R_loss=0.571,d_f_loss=0.776, g_loss=0.628, d_acc=54, aadv_acc=68\n",
            ">184, d_loss=1.490,d_R_loss=0.657,d_f_loss=0.834, g_loss=0.573, d_acc=54, aadv_acc=68\n",
            ">185, d_loss=1.387,d_R_loss=0.624,d_f_loss=0.763, g_loss=0.631, d_acc=54, aadv_acc=68\n",
            ">186, d_loss=1.380,d_R_loss=0.641,d_f_loss=0.739, g_loss=0.654, d_acc=54, aadv_acc=68\n",
            ">187, d_loss=1.356,d_R_loss=0.642,d_f_loss=0.714, g_loss=0.675, d_acc=54, aadv_acc=68\n",
            ">188, d_loss=1.358,d_R_loss=0.620,d_f_loss=0.738, g_loss=0.656, d_acc=54, aadv_acc=68\n",
            ">189, d_loss=1.390,d_R_loss=0.621,d_f_loss=0.769, g_loss=0.632, d_acc=54, aadv_acc=68\n",
            ">190, d_loss=1.408,d_R_loss=0.581,d_f_loss=0.827, g_loss=0.589, d_acc=54, aadv_acc=68\n",
            ">191, d_loss=1.452,d_R_loss=0.587,d_f_loss=0.865, g_loss=0.555, d_acc=54, aadv_acc=69\n",
            ">192, d_loss=1.303,d_R_loss=0.552,d_f_loss=0.750, g_loss=0.647, d_acc=54, aadv_acc=69\n",
            ">193, d_loss=1.361,d_R_loss=0.569,d_f_loss=0.792, g_loss=0.613, d_acc=54, aadv_acc=69\n",
            ">194, d_loss=1.331,d_R_loss=0.561,d_f_loss=0.770, g_loss=0.629, d_acc=54, aadv_acc=69\n",
            ">195, d_loss=1.400,d_R_loss=0.590,d_f_loss=0.810, g_loss=0.594, d_acc=54, aadv_acc=69\n",
            ">196, d_loss=1.375,d_R_loss=0.633,d_f_loss=0.742, g_loss=0.652, d_acc=54, aadv_acc=69\n",
            ">197, d_loss=1.370,d_R_loss=0.592,d_f_loss=0.778, g_loss=0.627, d_acc=54, aadv_acc=69\n",
            ">198, d_loss=1.310,d_R_loss=0.535,d_f_loss=0.775, g_loss=0.627, d_acc=54, aadv_acc=69\n",
            ">199, d_loss=1.362,d_R_loss=0.512,d_f_loss=0.849, g_loss=0.569, d_acc=54, aadv_acc=69\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2ReCgpCkoGs"
      },
      "source": [
        "### Validate Performance Diabetes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7gWemwcwQGr"
      },
      "source": [
        "#### Evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1bzinAOd92w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b67da9-f322-4499-f77e-bd2a5dcbdafe"
      },
      "source": [
        "# evaluate real data accuracy\n",
        "prediction = tf.math.round(discriminator.predict(train_data_diabetes))\n",
        "real_acc = tf.keras.metrics.Accuracy()\n",
        "real_acc.update_state(np.ones(data_diabetes.shape[0]), prediction)\n",
        "result_real = real_acc.result()\n",
        "print(f\"Real data accuracy: {result_real.numpy()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Real data accuracy: 0.8463541865348816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyL3H16TWX04",
        "outputId": "ec42fe4a-dc83-4cad-f10b-8d5f26624a16"
      },
      "source": [
        "# evaluate fake data accuracy - 100 samples\n",
        "num_examples_to_generate = 100\n",
        "seed = tf.random.normal([num_examples_to_generate, latent_dim])\n",
        "prediction_gen = generator(seed, training=False)\n",
        "prediction_disc = discriminator.predict(prediction_gen)\n",
        "rounded_prediction = tf.math.round(prediction_disc)\n",
        "fake_acc = tf.keras.metrics.Accuracy()\n",
        "fake_acc.update_state(np.zeros((num_examples_to_generate,1)), rounded_prediction)\n",
        "result_fake = fake_acc.result()\n",
        "print(f\"Fake data accuracy: {result_fake.numpy()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fake data accuracy: 0.15000000596046448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTdPKrM2GAPT"
      },
      "source": [
        "# inverse to real values\n",
        "predictions_inverse = mms_diabetes.inverse_transform(prediction_gen)\n",
        "real_data = mms_diabetes.inverse_transform(data_diabetes_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f4AKogaEiWF",
        "outputId": "22bdf668-5e47-4380-9b24-4bf7e1a1237a"
      },
      "source": [
        "# samples which fooled the detector\n",
        "indices_fooled = [i for i in range(100) if rounded_prediction[i] == 1]\n",
        "chosen_samples = random.sample(indices_fooled, 5)\n",
        "for i in chosen_samples:\n",
        "  distances_scale = []\n",
        "  for item in data_diabetes_scaled:\n",
        "    distances_scale.append(np.linalg.norm(prediction_gen[i] - item))\n",
        "  closest_index = np.argmin(distances_scale)\n",
        "  print(f\"sample number {i}:\")\n",
        "  print(f\"Fake sample : {predictions_inverse[i].astype(int)}\")\n",
        "  print(f\"Closest real : {real_data[closest_index].astype(int)}\")\n",
        "  print(f\"Euclidean Distance: {distances_scale[closest_index]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample number 88:\n",
            "Fake sample : [10 93 86 15  4 33  0 53]\n",
            "Closest real : [ 8 99 84  0  0 35  0 50]\n",
            "Euclidean Distance: 0.4593472182750702\n",
            "sample number 85:\n",
            "Fake sample : [  0 140  90  37 231  39   0  21]\n",
            "Closest real : [  0 127  80  37 210  36   0  23]\n",
            "Euclidean Distance: 0.30372029542922974\n",
            "sample number 75:\n",
            "Fake sample : [ 7 89 42 20 22 27  0 30]\n",
            "Closest real : [  7 106  60  24   0  26   0  29]\n",
            "Euclidean Distance: 0.3633643388748169\n",
            "sample number 57:\n",
            "Fake sample : [  3 121  74  45 145  37   0  31]\n",
            "Closest real : [  2 104  80  45 191  33   0  29]\n",
            "Euclidean Distance: 0.3165197968482971\n",
            "sample number 66:\n",
            "Fake sample : [  1 118  79  50 209  39   0  26]\n",
            "Closest real : [  2 104  80  45 191  33   0  29]\n",
            "Euclidean Distance: 0.2642476260662079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NFsgqmMGykl",
        "outputId": "92f51d78-e614-42ed-aa26-9ed7e3e642c6"
      },
      "source": [
        "# samples which did not fool the detector\n",
        "indices_not_fooled = [i for i in range(100) if rounded_prediction[i] == 0]\n",
        "chosen_samples = random.sample(indices_not_fooled, 5)\n",
        "for i in chosen_samples:\n",
        "  distances_scale = []\n",
        "  for item in data_diabetes_scaled:\n",
        "    distances_scale.append(np.linalg.norm(prediction_gen[i] - item))\n",
        "  closest_index = np.argmin(distances_scale)\n",
        "  print(f\"sample number {i}:\")\n",
        "  print(f\"Fake sample : {predictions_inverse[i].astype(int)}\")\n",
        "  print(f\"Closest real : {real_data[closest_index].astype(int)}\")\n",
        "  print(f\"Euclidean Distance: {distances_scale[closest_index]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample number 87:\n",
            "Fake sample : [  3 191 114   5  48  36   0  31]\n",
            "Closest real : [  4 189 110  31   0  28   0  37]\n",
            "Euclidean Distance: 0.664002001285553\n",
            "sample number 60:\n",
            "Fake sample : [  3 193  99  30 143  32   0  32]\n",
            "Closest real : [  4 189 110  31   0  28   0  37]\n",
            "Euclidean Distance: 0.44953539967536926\n",
            "sample number 4:\n",
            "Fake sample : [  1 187  90  42 268  35   0  28]\n",
            "Closest real : [  4 184  78  39 277  37   0  30]\n",
            "Euclidean Distance: 0.4064830541610718\n",
            "sample number 16:\n",
            "Fake sample : [  3 183  84  24 350  29   0  31]\n",
            "Closest real : [  2 158  70  30 328  35   0  35]\n",
            "Euclidean Distance: 0.44706934690475464\n",
            "sample number 3:\n",
            "Fake sample : [12 96 50  1  0 22  0 41]\n",
            "Closest real : [13 76 60  0  0 32  0 41]\n",
            "Euclidean Distance: 0.4179590940475464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvTF_9GsIaUM",
        "outputId": "0eea3ae7-198c-4ebf-8891-030fcc3e7255"
      },
      "source": [
        "# fooled successfully samples - mean distance\n",
        "indices_fooled = [i for i in range(100) if rounded_prediction[i] == 1]\n",
        "closest_distances = []\n",
        "for i in indices_fooled:\n",
        "  distances_scale = []\n",
        "  for item in data_diabetes_scaled:\n",
        "    distances_scale.append(np.linalg.norm(prediction_gen[i] - item))\n",
        "  closest_distances.append(min(distances_scale))\n",
        "print(f\"The mean distance of samples which fooled the detector: {np.mean(closest_distances)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean distance of samples which fooled the detector: 0.3419110178947449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoEYVzOmJVpg",
        "outputId": "d5a37155-04f7-4a8b-d331-dcf9cf88507f"
      },
      "source": [
        "# Samples which did not fooled - mean distance\n",
        "indices_not_fooled = [i for i in range(100) if rounded_prediction[i] == 0]\n",
        "closest_distances = []\n",
        "for i in indices_not_fooled:\n",
        "  distances_scale = []\n",
        "  for item in data_diabetes_scaled:\n",
        "    distances_scale.append(np.linalg.norm(prediction_gen[i] - item))\n",
        "  closest_distances.append(min(distances_scale))\n",
        "print(f\"The mean distance of samples which did not fooled the detector: {np.mean(closest_distances)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean distance of samples which did not fooled the detector: 0.492653489112854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2U8YOBcW8EN"
      },
      "source": [
        "#### Dimensionality Reudction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lub92p7W-dQ"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(real_data)\n",
        "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
        "principalDf['target'] = 'real'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiqGTmRrdEdk"
      },
      "source": [
        "# all fake data\n",
        "a = pca.transform(predictions_inverse)\n",
        "b = pd.DataFrame(a, columns=['principal component 1', 'principal component 2'])\n",
        "b['target'] = 'Fooled'\n",
        "principalDf = principalDf.append(b, ignore_index=True)\n",
        "principalDf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSMxZz2K12ed"
      },
      "source": [
        "# fake data which did not fool the detector\n",
        "indices_not_fooled = [i for i in range(100) if rounded_prediction[i] == 0]\n",
        "index_0 = len(real_data)\n",
        "for i in indices_not_fooled:\n",
        "  principalDf[\"target\"].iloc[i+index_0] = 'NotFooled'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "EJ5Kkyf8a5_U",
        "outputId": "8a1b2aa1-0a9e-422e-a029-226632717ab3"
      },
      "source": [
        "# plot real and fake data\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = ['real','NotFooled', 'Fooled']\n",
        "colors = ['r', 'black', 'b']\n",
        "for target, color in zip(targets, colors):\n",
        "    if target ==\"NotFooled\":\n",
        "      s = 60\n",
        "    else:\n",
        "      s=50\n",
        "    indicesToKeep = principalDf[\"target\"] == target\n",
        "    ax.scatter(principalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , principalDf.loc[indicesToKeep, 'principal component 2']\n",
        "               , c = color\n",
        "               , s = s)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "vzKCqaRSi1n5",
        "outputId": "789d39ce-9760-4bde-c0a3-1a6f183f6dd4"
      },
      "source": [
        "# plot real data only\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = ['real']\n",
        "colors = ['r']\n",
        "for target, color in zip(targets, colors):\n",
        "    indicesToKeep = principalDf[\"target\"] == target\n",
        "    ax.scatter(principalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , principalDf.loc[indicesToKeep, 'principal component 2']\n",
        "               , c = color\n",
        "               , s = s)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAH6CAYAAABxmfQYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde5gcZZk3/u89M52EYZIFA2QRooERkIMIJKvw7oskoGLCQYwBlASRoMEQFTH7W2F15eC7Ki5ZFOIiMUQDI0QIoKATQCEBj4sJYAjnGQwnXTARSCaHyUzm/v3xVDk1PdXVT1VXdVVXfT/XVVfPVFdXPV2Z1HM/Z1FVEBERUbE0pZ0AIiIiqj8GAERERAXEAICIiKiAGAAQEREVEAMAIiKiAmIAQEREVEAMAIiIiAqIAQAVjoiMFZFPicidItIlIttE5E0R+bWInCci/H+RMyIyWURURC6L8Nn1zmfdbUBE3hCR34rIPBFpqfC58SLyTRFZIyKvi0ifiLwmIr8UkQtF5B8CrjnTc70Phk0zkQ3fP1yinDsdwHUA/gJgJYAXAYwDMB3AYgBTReR05SxZNNR3ALwBoBnAfgA+CuAYACfA/O38nYh8CsBCACMB/BHALQBeBzAWwP8F8G0A/w5gjwrXmgNAAYjz833xfhUiBgBUTM8COBXAz1V1wN0pIv8G4GGYB/t0ALenkzzKqG+r6nr3FxH5BoA/APiIiBynqg86+2cC+D5Mhv9RVf15+YlE5J8BfNfvIiJyEID3AfglgN0BnCoi41T11Zi/DxUcqzqpcFT1AVW925v5O/v/F8D3nF8nhzmniLxTRJY41cW9TlXvr0Rkrs+xJ4jIPSLyN+fYZ52q4mFVwiKyyqkGLonIV0WkW0S2i8gzIvJpz3GfEZHHneaMl0Xk8vKmDBGZ4Jzrh056f+KkYYvT/OFb1SwiI0XkYuf8W0Vkk/PdzvA51nuNCSKyTEQ2OGleLSInB9zDj4vISqd6fbuIPCUiXxGRkT7HqnNv9hCRRSLyF+dePiEi55Yd+0OYmh4AuLSsOn9ypfRUo6pPAFjl/Poe51qjAVzj7PuYX+bvfPY3AN5b4dTuv+sPAPwQQAnAJ6Omk6gS1gAQDdXnvPbbfkBETgJwG0x17z0w1b27AXg3gH+FaW5wjz3f+X2L85nXYIKNLwE4RUT+WVXf8LnMMpgMo9NJ4wwAi0SkD8DhAM4B8DMA98PUbnwVwFYAV/qcaz8AvwPwOIDrAewN4EwAK0TkLFX9sSe9IwDcC+A4AE/DlFpbnev/WESOUNV/87nG22FqU54HcBOAtzjX+KmIvF9VV3oPFpElAM4F8DJMzcsbAI4G8DUAJ4jIB1S1/N9kNwC/AbADwHKY+386gCUiMqCqS53jfuK8ngPgQQxm2gCw3iftYYjz6jYXzYD5rr9X1cBqe1XtHXYyc7/PAfAmgDsB7AJgAYBPici32CxFsVJVbty4qQImIH4c5mF+ouVn9oB5WO8AcJzP+/t6fn47gF4AmwC8s+y4/3auu6hs/ypn/x8A7ObZv79zzdcB/AnAPp73dgOwAcBfAbR49k9wzqUA/rPsOpNgAovXAYzx7L/EOb6z7Fx7wWSeCuD/VLjGpWXXONE9V9n+Tzr77wCwS9l7lznvXVi2373GYgDNnv2HwARvT5YdP9k5/rIIfxfu95xQtv9QmCBLARzr7LvB+f3/Rfwb/Jjz+es9+5Y7+05I+/8It3xtqSeAG7esbACuch60Pw/xmfnOZ75jceyXnWO/7vPe7k5gsA3ASM/+VZUe/gAecN6b7fPeD5z33u7Z52bObwAY7fOZHzrvn+PZ9xyAAZQFLM575znHL/G5xnpvxux5/wUAG8r2PQoTfOzmc3wzTDDzcNl+halFGePzmQed99s8++IIAL7tBCRfA9Dhyfzv8Bzb6ez7TMS/wfudzx/j2Xeys+/Haf7/4Ja/jU0ARABE5PMwmfnTAM4O8dGjndcVFsce5bw+UP6Gqr4uIo/CdP56J0zPca/VPuf7s/O6xue9V5zXfWEyXa9HVHWzz2dWwVQ/HwlgqdOe/Q4Ar6jq0z7Hu9/jSJ/3HlPVnT77X4LpOQ8AEJFWmKaSDQC+ICI+H0EvgIN99j+nqpsqXAMwQVWP3wkjutB5Vee8a2ECge9V/EQIIvIOAFMAPKOqv/O8dQ+A/wVwmojsoaob4rgeEQMAKjwR+SzMEK8nYUrafwvx8d2c11cCjzLcTn5/qfC+u3+38jdU9U2f49028aD3Sj7vVepN/r/O6z+UvYZOL0wtg59+DO18vDtMO/qeAC6t8JlKgq4BmNqDOO2nnlEAFbj3ZJ8I5/80zL34oXenqvaLyI9gAtRPwtRUEdWMowCo0ETkCwCuBbAOwBQ1IwHCcDMhmwe+m1H/Y4X39y47LinjKux30/Vm2WuS6XU/+6iqStBWwzXq6dfO6wlhPiQi3p7+3ygbqaAwmT8wOEKAqGYMAKiwRORLAK4G8BhM5v9ahNP83nmdanHso87rZJ+07AbgCADbATwVIR1hHOVU75dz0/UoADjNBN0A9hGRA3yOn+K8PhI1IaraA+AJAIeKyFuinseC2xwRd61AueUA/gbgGBF5f9CBZcMbPwzTsfIZmI6EftvzAA4UkeMSSDcVEAMAKiQR+XcA34RpPz+hhnbVpTCd9+aKyPt8rrOv59cOmM5un3Pae72+BmAMgA71GR4Ws3+AGSb4dyIyCcBMDA4/cy2BqZb+TxFp9hy/B8xMdu4xtfgvACNghu8Na04Qkd1F5KjhHwtlo/P6thrPE8gJmj7v/PpjETnR7zgRORpmKKZrjvP6VVX9lN8G4OtlxxLVhH0AqHBE5BwAV8CUCn8F4PM+nc/Wq+oPq51LVTeIyFkwJb+VIrICpnPYGJjx+eNhxt1DVdc7TQ7fBfCIiNwKM1TvOJiOcU/DzAeQtIdgxpW/F2YcvTsPQBOA88s61l0FU7vxYQB/FJFOmHkATocpsX5LVX+NGqjqEhGZCOACAN0ici/M9Mxvgbl374MZ1fCZGi7zDEw/jY85cye8ANOZ7yZVLe8kWRNV/ZGI7AIzFfA9IvIYgN9icCrgYzDY8REish+A9zu//8T3pMaPYUYifFREPheyrwrRMAwAqIj2c16bAXyhwjEPoqwzViWq+nOnBP0lmLbfD8I87J8G8I2yY/9bRLoA/AvMlMOtML3W/xNmeGCljm1x+hNMZvpN53UkTDX+Fap6b1l6d4jIBwB8EcBZAD4H08nujwC+oKq3xJEgVZ3nBE+fgckMd4OpSn8R5t501Hj+nSLyEZjvfDqA0TA1G7/G8FESNVPVxU4g81kAH4CpXdkVps/IOgAXYbDm5FNOWm5S1R0B5+wRkVtg+gGcA9N8RRSZqHJiKaIiEJEJMJn/UlX9ZKqJIaLUsQ8AERFRATEAICIiKiAGAERERAXEPgBEREQFxBoAIiKiAsrtMMA99thDJ0yYkHYyUrNlyxbsuuuuaSejcHjf08N7nw7e9/T43fs1a9ZsUNU9bT6f2wBgwoQJWL3abwG1Yli1ahUmT56cdjIKh/c9Pbz36eB9T4/fvRcR63kt2ARARERUQAwAiIiICogBABERUQHltg8AERFRX18fXn75ZWzfvj3tpMRq1KhR8FnELBQGAERElFsvv/wyRo8ejQkTJtScYWaFqmLjxo01j75gEwAREeXW9u3bMXbs2Nxk/gAgIhg7diyam5trOg8DACIiyrU8Zf6uOL4TAwAiIiIA6O4GLrgAGDMGaGoyrxdcYPanaMKECdiwYUPs52UAQEREtGIFcPjhwOLFwObNgKp5XbzY7F+xIpbLqCoGBgZiOVetGAAQEVGxdXcDM2YAW7cCfX1D3+vrM/tnzIhcE7B+/XocdNBB+MQnPoHDDjsMX/va1/BP//RPOPzww3HppZf+/bjTTjsNEydOxKGHHopFixbV8o2sMAAgIqJiW7BgeMZfrq8PuPrqyJd47rnncMEFF+Dqq6/GK6+8gocffhiPPfYY1qxZg4ceeggAsGTJEqxZswarV6/GNddcg40bN0a+ng0GAEREVGwdHXYBwE03Rb7E29/+dhx99NG47777cN999+HII4/EUUcdhaeffhrPPfccAOCaa67Bu9/9bhx99NF46aWX/r4/KZwHgIiIiq2nJ97jfLhj9lUVl1xyCc4///wh769atQq//OUv8bvf/Q6tra2YPHly4pMXsQagmoz2CiUiopi0tcV7XIATTzwRS5YsQY8TTLzyyit47bXX8Oabb2L33XdHa2srnn76afz+97+v+VrVMAAIUqdeoURElKJZs4BSKfiYUgk4++yaL/XBD34QZ511Fo455hi8613vwowZM7B582Z86EMfQn9/Pw4++GBcfPHFOProo2u+VjVsAqjE2yu0XF+f2WbMANauBdrb658+IiKKx/z5wNKlwf0ASiXgoosinX7ChAlYt27d33+/8MILceGFFw47bkWFQuX69esjXbca1gBUEqVXKJsLiIgaT3s7sHw50No6vCagVDL7ly/PXWGPAUAlYXuFsrmAiKhxTZ1qanTnzBlaiJszx+yfOjXtFMaOTQCVhOkVyuYCIqLG194OLFxotgJgDUAlra32x9VhEgkiIopGVdNOQuzi+E4MACqZMMH+uDpMIkFEROGNGjUKGzduzFUQoKrYuHEjdu7cWdN52ARQiW2vy/XrgS1b7I6tYRIJIiIKb99998XLL7+Mv/71r2knJVajRo3CFtu8pwIGAJX4tedXOq6tzXT4qyaGSSSIiMheqVTCfvvtl3YyEvHCCy/U9Hk2AVQSZmaoOk4iQUREFAcGAJXMmgU0Nwcf09xsMvX58+0CgIiTSBAREcWNAUAlM2YA1TpY7NwJTJ9e2EkkiIiocTEAqGT5crsagDvuMD8XcBIJIiJqXAwAKunosKsB8A7tcyeRePNN896bb5rfWfInIqKMYQBQie2QPZve/0RERBnDAKCSFssRktU6/xEREWUQA4Ba5Wh2KSIiKg4GAJVUm9rX1d+fbDqIiIgSwACgktGj4z2OiIgoQxgAVMLZ/YiIKMcYAFTC2f2IiCjHGABUwtn9iIgoxxgABOHsfkRElFNcDrgad3a/hQvTTgkREVFsWANARERUQAwAiIiICogBABERUQExACAiIiogBgBEREQFlFoAICJLROQ1EVnn2fcWEfmFiDznvO7u7BcRuUZEukRkrYgclVa6iYiI8iDNGoAfAvhQ2b6LAdyvqgcAuN/5HQCmAjjA2eYAuK5OaSQiIsql1AIAVX0IwN/Kdn8YwFLn56UATvPsv1GN3wPYTUT2rk9KiYiI8idrfQDGqepfnJ//F8A45+d9ALzkOe5lZx8RERFFkNmZAFVVRUTDfEZE5sA0EWDcuHFYtWpVEklrCD09PYX+/mnhfU8P7306eN/TU+u9z1oA8KqI7K2qf3Gq+F9z9r8CYLznuH2dfUOo6iIAiwBg0qRJOnny5ISTm12rVq1Ckb9/Wnjf08N7nw7e9/TUeu+z1gRwF4BznJ/PAfBTz/5POKMBjgbwpqepgIiIiEJKrQZARG4BMBnAHiLyMoBLAXwTwK0ich6AFwCc4RzeCWAagC4AWwGcW/cEExER5UhqAYCqfrzCWyf4HKsA5iWbIiIiouLIWhMAERER1QEDACIiogJiAEBERFRADACIiIgKiAEAERFRATEAICIiKiAGAERERAXEAICIiKiAGAAQEREVEAMAIiKiAmIAQEREVEAMAIiIiAqIAQAREVEBMQAgIiIqIAYAREREBcQAgIiIqIAYABARERUQAwAiIqICYgBARERUQAwAiIiICogBABERUQExACAiIiogBgBEREQFxACAiIiogBgAEBERFRADACIiogJiAEBERFRADACIiIgKiAEAERFRATEAICIiKiAGAERERAXEAICIiKiAGAAQEREVEAMAIiKiAmIAQEREVEAMAIiIiAqIAQAREVEBMQAgIiIqIAYAREREBcQAgIiIqIBa0k6Al4gcBODHnl37A/gqgN0AfBrAX539/6aqnXVOHhERUW5kKgBQ1WcAHAEAItIM4BUAdwI4F8DVqnpViskjIiLKjSw3AZwAoFtVX0g7IURERHmT5QDgYwBu8fz+WRFZKyJLRGT3tBJFRESUB6KqaadhGBEZAeDPAA5V1VdFZByADQAUwNcA7K2qs30+NwfAHAAYN27cxGXLltUx1dnS09ODtra2tJNROLzv6eG9Twfve3r87v2UKVPWqOokm89nNQD4MIB5qvpBn/cmAPiZqh4WdI5Jkybp6tWrk0lgA1i1ahUmT56cdjIKh/c9Pbz36eB9T4/fvRcR6wAgq00AH4en+l9E9va89xEA6+qeIiIiohzJ1CgAABCRXQF8AMD5nt3fEpEjYJoA1pe9R0RERCFlLgBQ1S0AxpbtOzul5BAREeVSVpsAiIiIKEEMAIiIiAqIAQAREVEBMQAgIiIqIAYAREREBcQAgIiIqIAYANjo7gYuuAAYMwZoajKvF1xg9hMRETUgBgDVrFgBHH44sHgxsHkzoGpeFy82+1esSDuFREREoTEACNLdDcyYAWzdCvT1DX2vr8/snzGDNQFERNRwGAAEWbBgeMZfrq8PuPrq+qSHiIgoJgwAgnR02AUAN91Un/QQERHFhAFAkJ6eeI8jIiLKCAYAQdra4j3OD0cYEBFRChgABJk1CyiVgo8plYCzIy5WyBEGRESUEgYAQebPtwsALroo/Lk5woCIiFLEACBIezuwfDnQ2jo8ECiVzP7ly81xYXGEARERpYgBQDVTpwJr1wJz5gxtp58zx+yfOjXaeTnCgIiIUtSSdgIaQns7sHCh2eLCEQZERJQi1gCkpR4jDIiIiCpgAJCWpEcYEBERBWAAkJYkRxgQERFVwQAgLUmOMCAiIqqCAUCakhphQEREVAVHAaQtiREGREREVbAGgIiIqIAYABARERUQAwAiIqICYgBARERUQAwAiIiICogBABERUQExACAiIiqgwABARPYRkX8XketE5AsisrvPMQeLyAPJJZGIiIjiVjEAEJEDADwO4F8BHAvgmwCeFZFTyw4dA+C4xFJIREREsQuqAbgSwDMA3qaqhwEYD2AFgDtE5Iv1SBwF6O4GLrhg6BTCF1xg9hMREVURFAAcA+Drqvo6AKjqX1X1EwA+B+BKEflOPRKYKVnJdFesAA4/HFi8GNi8GVA1r4sXm/0rVtQ3PURE1HCCAoBdAGwt36mq1wH4KIBPichtAEYllLZsSTvTdYOPtjZg2jRg61agr2/oMX19Zv+MGUBvb7LpISKihha0GNAzMG3/95e/oap3icgHAdwF4J8SSlt2dHebTHXrsHjIZLp9feb9tWuTWb53xQpzfvda1fT1Aa+9Fn86iIgoN4JqAO6BKeWP9HtTVX8D4H0AmpNIWKYsWFA94+3rA66+Ov5re4MPm8zfTcvGjfGnhYiIciMoALgKwIlBx6jqEwCOAnB8zOnKlo4OuwDgppviv7ZN8OFn587400JERLlRsQlAVTcDeKLaCVT1rwAejDNRmbN5s91xPT3xX9sm+PDTnP+KGSIiio4zAVYTpod/W1v8148SVJRKwNix8aeFiIhyI5MBgIisF5HHReQxEVnt7HuLiPxCRJ5zXofNSpiIBQvsjhMBzj47/utHCSpKJWCvveJPCxER5UYmAwDHFFU9QlUnOb9fDOB+VT0AZmTCxXVJxQ9/aHecKnDRRfFff9Ysk6HbKJWA1lZg+XJgpG/fTSIiIgDZDgDKfRjAUufnpQBOq8tVt22zP3bBgvgnCZo/3y4AaGsD5swxQxGnTq3tmkRElHuiqtUPEvkqgMWq+mef9/YG8GlVvSK2RIn8CcDrABTA9aq6SETeUNXdnPcFwOvu757PzQEwBwDGjRs3cdmyZbUnZs2aMAk3NQHe30XM3ABjxlT+XG8v8OqrZujewIAJIMaOBcaNMyX5TZtMIKFqff6enh60JdEngQLxvqeH9z4dvO/p8bv3U6ZMWeOpOQ+mqlU3ADsBvKfCexMB7LQ5j+0GYB/ndS8Af4SZb+CNsmNeDzrHxIkTNRaD2W70rbVVtavL//ydneb9UmnoZ0ols7+z0xzX1aU6b57qmDGqTU3mdd48//N2denKpUtVR49WFTGvc+dWTgPFZuXKlWknobB479PB+54ev3sPYLVa5rW2TQACUxr3sy9MaT02qvqK8/oagDsBvAfAq05tg1vr0DhT3VWaJChokh/vtL7d3aaUv3Ah8OabZoz/m2+a38tnHnSnLN6wgesEEBFRRUHLAZ8jIg+IyAMwmf917u+e7bcAOhDjPAAisquIjHZ/BvBBAOtgph0+xznsHAA/jeuagQ49tPZzVJokKO4ZBr0BRXnTTnlAQUREhRZUA7AVwEZnEwBven53tz8B+BacdveYjAPwaxH5I4CHAfxcVe8B8E0AHxCR5wC83/k9eddcE895/Mbzxz3DYJpTFhMRUUMJmgnwNgC3AYCI/ADA11T1+aQT5Fzj3T77NwI4IenrD/P2t8dzHrejRne3yag7OuKfYTBMQLFw4eA+b5p6ekxaZ80yIxCSWNyIiIhSF7Qa4N+p6rlJJySzbCcCClIqmUmCwq7q57LtYWsbKHiP80uT22dg6VIzpwCHFRIR5Y5VAAAAIjIJwHSYTn+jyt9X1TNiTFd2dHTUfo5SCZg+HTjlFP8lhavZtMkM86tWKm9rs6tV8NZGpLnMMRERpcZqFICIzIVpj/8UgHYAe/ps+RRmLn6Rob97Z+Zbvjzaoj6uzZuB738/uCe/zayBbm0EwD4DlXR3m0mc4p7UiYgoQ2yHAf4LgCUA3qqq/6yqU8q3BNOYrtZW+2NLpaGZhndmvqir+nn19wf35LeZNbBUGpyyOM1ljuOQREbtDqNcvJjDKCldDEQpYbYBwF4AblHV/iQTk0kTJtgf29dXeZx+nEsF79jhXypvbzc1Da2twbURYdOUxDLHtUoiow4zLwNRkhiIUh3YBgArALw3yYRk1vr19se2BHSpiHOqzP7+yqXyqVNNrcOee1aujQibpqxN85lURs0mEcoCBqJUJ7YBwHcBnCMil4rI/xGRQ8q3JBOZqjCd9nburPxemFX9bASVytvbgfHjTS3Es88CM2cCN94IHHDA0GrEsH0GsiKpjLrRm0QoHxiIUp3YBgArARwA4FIAvwLwuGdb57zmU1CpvtzAQOX3bFf1s2VTKq9WjThpUrg+A1mRVEbdyE0ilB8MRKlObHO3/Hbyqxe3fT7KPAB+qpXKe3urD/E77zzTRFBJSwtw7bXZGwKYVEYddhglURIYiFKdWNUAqOqD1bakE5qaMBl1tdK02z4/Z85g+/yoYVMqVNfSYkrlQb2EX33VLu1BtRaqwPnnm86DWeqFnFTfBdtmmm3bsnEfKJ8atW8ONRzbJgAAgIhMFZF/F5FFIvI2Z9/7ROStySQvA3bd1f7YM8+sfkz5qn7nRphk8frrTdt+UPX+xo211zLs3Gk6HG7blq1eyEn1XbBtpunrS/Y+cPhXsTVq3xxqOLYTAY0Tkf8BcDfMSnznAdjDeftcAP+eTPIy4K0hYpsomXnYmQZvuAE47rjqvYSDSva1yEIv5LDzHdjyDqOsdv6k7gOHf1FSf99EZWxrAK4F0Abgnc7mHWT+S6SxSE+9/OlP9sfecUf484dtx1u9GvjiF6NNKRynNHshB2XUfvMdhOFtprGtDYjrPnD4FwHJ/n0TedgGAB8C8BVV7QJQttA8XgawT6ypypL+EHMfRemVG2amQQBYtAi4667w14lb2r2Q/fpTVJrvICy3mcamf0ac94HDv8iV5N83kSNMH4BKOeEeALbFkJbGZ7u8r9vGO2oUsGVLuGsEzTVQbza1F0m2Z5f3pyiffbFW9e6NzeFf5JX03zcVnm0A8CsAnxeRZs8+tyZgNoAHYk1VloQZu28zZ4Dbxnv99WaoXtL8pgWOS7VeyI3enl3v3tgc/kVEdWQbAHwJwD/BTPrzNZjM/9Mi8iCAYwB8JZnkZcDpp9sfWy2j9bbxJtVJr9zatcBZZ8V/3mq9kPPQnl3v3tgc/kVEdWQ7D8A6ABMBrAbwSQA7AUyHaf9/r6o+m1QCU3fFFfbHVqu+tWnjjVNbm6ku7OgAOjvterfbqtYLOQ/t2fXujc3hX0RUR9Z9AFS1W1XPVtW3quoIVf1HVZ2pqs8lmcDUxdneFseSwGGcc87gz1OnAnffDRx00NBj3JkA29qAww4zr01NJlhobh7erGHbCzkP7dn17o3N4V9EVEehJgKiKrR8gESZerfdTp8+2AmvtRU44QRg3bqhxzQ3m/duvRV4/HHTRr9zp+mc+MwzZibAKL2Q89KeXc/e2Bz+RUR1ZL3SjYjMgKn23xfAsPFRqvqeGNPVuLq7TfV3R4fJ3NraTNXu/Pn2c83H5ZJLTIa/Y0fl4YzuugAzZpgMzZu5uL2QFy4Mf+08zatfy30Iyw04rr7a1I64f0Nnn21K/sz8iSgmtjMBXgbgVgAHA3gJwBM+GwH+vd6vuw54xzvMlLr19PDDprOdzVwGcbfHF6E922+I44sv1t6xkcO/iKgObJsAzgPwTVU90mn3P7d8SzKRDcWv17srzKRC9RZ3e3yt7dlZnw+/0hDHDRsaY4gjERWebQAwGsD9SSaEMiDO9vha2rOzPn9A0BBH1cYY4khEhWcbACyDmQ64mMKsCNjIytvjay2FR+lA1wjzB+RhiCMRFZ5tAHA/gI+KyA9E5CwRmVa+JZnI1E2YkHYKklfeHh9XKdymPdsbaLzjHdUXOvLLXOvZZJCHIY5EVHi2AcCPAUyAWQq4A8DPyra7k0hcZqxfn3YKkudtj7cphX/0o8DMmcEZrk2mXB5o2OjrM4siueepd5NBXoY4ElGh2Q4D3C/RVGRd2AV7bI0YYcbh13t0gFdLi0nHtdcODl+0yYi3bQNuuWVw7gM3w1261LTtAyaIcIcZ+h1z4IGDgUZYfX0mc7/2WuBzn/M/R9AQx1rkaYgjERWWVQCgqi8knZBME6k+yU9YpZJZZ+DOO+M9bxgiZtu6FTjvvPDfs/xYN8OdPt2cyy+w8WbKp+/65/0AACAASURBVJ1W28yIW7ea/gTV1mBwmwziGsc/a5YJZILS3uhDHIko96xnAhSRFhE5U0SuFZEfOa9niIj1ZEINK+7MHzAZhGp9pwYuV379uL5nby+wfXvwMX19wG231f79d+6sPrwya0MciYgywHYioL1gFgK6BcBJAPZ3XpcB+IOI7JlYCvPGLa1u3QrcfHO6AUBSVKsHE96mgXqo1xBHEU7ZS0QNwbYG4L8AjAVwtKrur6rHqOr+AN7r7P+vpBJIFIu42+MrDXHcc8/41wggIkqAbQAwDcCXVPVh705V/QOAS2BqA8hGEs0JjapUim954mrXSaI93m+I4/jxLPkTUUOwDQBGAqjU7XkzgBHxJIdywe1cGKRUAs44o/YAoKVl+JLFftdie3ztsj49MxGFYhsA/B7Al0RkyJR4zu9fct6nIhAxQxeDjBwJjBq2YORQpRJw+eWV29JtjRgBXH89l9BNWtanZyai0GwDgPkADgXwkogsE5HviMgtMCsDHuK8T0WgajL4oAz3jjuA228PzthVzbwDBx442JYetp2+pcVk7rNnh59ymOw1wvTMRBSaVQCgqo8BOADAIgB7AvgAgL0AfA/AAar6x8RSmAVvf3vaKciW7durZ7jeTnKtrcPPsW3bYOnx2WeBk04CBgbsawKam4F77x3M3LmEbnK49gFRLlmP4VfVDQAuTjAt2XX44cALxZ4LaYi2tsEMt3xyHbeduKPDDL1rbQV27PA/j3fioB07TABg66tfBY4/Pvp3SEJ3N/DiiyYY6ukx92nWLDNvQCMHImHWPohrsiUiSpz1REAAICK7icj/FZHTReSfRWS3pBKWKQ88kHYKsiOoR71fO/GWLdUn6tm+PVzmDwBXXpmtKmf3u2/YkL82cq59QJRLthMBtYjIlQBeBvAQzOJAvwLwsoh8S0TqMJYrRUmtBdCIynvUuyX+tjZg2jT/duIkZKnK2dtG7jc9cqO3kdv2zeDaB0QNJcxEQBcC+DpMp789nNdvAPgcgAVxJEZExovIShF5UkSeEJELnf2XicgrIvKYs2V7+eG5c+szvr2e/HrUe0v89Q6SsrTcbt7byGfNsvt73rSJQwOJGohtAHA2gH9T1a+r6tOq+jfn9T8AfMV5Pw79AOar6iEAjgYwT0QOcd67WlWPcLbOmK6XjBkz8hEABPWoD+oZXi9ZqXIO00beiGzWPnDVu9nDOzfBmjUMQIhCsA0ABgA8UeG9dQBimd5OVf+iqo84P28G8BSAfeI4d13dcUfl8e2lkhkjn/UAYcyY4B71NqXepGWlyrlebeRpTcQTtPaBn3o1e5T3OQHy0++CqA5ELaamFZFvA9hHVU/3eW85gD+r6udjTZjIBJj+BocB+CKATwLYBLMo0XxVfd3nM3MAzAGAcePGTVy2bFk8iVmzJtzxzc3AEUeYVfFeew3YuNFkpM3NwNix5kH6wguJTgvcs+++aHv55dpO0tRk0jtunBn77/Xoo+E77sVtr73M1Ltp89yLwPvu/l1EsWmTyUzLF1pyZ11sbzcBQZLK/56rETFrIyTxb9TbCzz55JC/wWH3vqkJOOSQ4X+7FKuenh60ZSUYLxi/ez9lypQ1qjrJ6gSqWnUDcBHMpD9PwLT7X+S8Puns/wKAC5xtrs05q1yvDcAaANOd38cBaIapsfgPAEuqnWPixIkam8HHrt3W1FT5XF1dqq2t4c8Zclt51VXxnKu52aS3s3Po9xBJ/DtU3bq64vs3rsXcuaqlUvB9L5VU582Ldn6bv5nW1vrej9Gj7f6NxoxJ5vqee17xb76We07WVq5cmXYSCsvv3gNYrVXyR3ezbQJYAFMVfzDM1L8LnNd3Ovv/C8BCzxaZM6LgdgA/UtU7AEBVX1XVnao6AOD7AN5TyzUSFxQNZ6HqPIydO0117rRppjPYAw+YameNofbiwAOjf3aXXbIztt6mjbyW9Qiy2Mkw7aGBee93QVQHtjMBNoXYqkwUX5mICIAbADylqv/l2b+357CPwPQ7yK6gledsHlxZ9aMfASecAFx3Xe3nam0FOmvoy7ltW3Y6fHnbyMsXQYpjPYIsZnZpDw1MOwAhyoFQEwHVwT/DjCg4vmzI37dE5HERWQtgCkwTRHYFlfT4QDKuvdZkiLvuWv3YSrLU4cud+njPPeNfjyCLmZ3N0MCklmEG0g9AiHLAeipgABCRg2Cq/Ict9aYxDM1T1V8D8FtHNtvD/soFlfTa2gZ7LBdVSwvwyCNmEZ8JE4AnKg0wseBOJzxjhslo02wWaG8HXnrJjJqIk+3fTD0zu/nzgaVLg2smklyGedYsE/xVu35SAQhRDtjOBPguEVkH0+nvlwB+VrbdnVgK8+bYY9NOQfr6+werq9evj+ecjTzRTjVpl7b9BA0NrMcyzEn3uyAqANsmgCUA+gCcDOAgAPuVbfsnkrq86e4GVq5MOxXZ4FZXb90az/n6+oDvfjcbfQLiltXMzrviY72XYU47ACHKAdsA4GAAF6vqClV9TlVfKN+STGTDqZQBLVhQeWW8omlrM+32cYwm8MpKn4A4ZTmzS3MZ5vIABKhfAEKUA7YBwMMA3pZkQnKlUlX0jTfaTaCSd6UScOqppt0+bnlYfMdPmqXtLPMGIBMn1jcAIWpwtgHAHABzRGSmiLxVRFrLtyQT2XCWLvXfX4RVBVtbzRakVDIlf9vhkM3NpuNgGHnsE5BmaZuIcsc2ANgAYD2AG2Fm/tvss+VX+djuanp6/Odsz7vmZuDcc+2qq++6yz4AWLQIGDEiXFo4CQwRUSDbYlUHgGMAXAWgC0CxGrKjtFN7hyi549XzbuRI0xGtvd1US199tcmEe3pMm//ZZw++H2bM+qc+ZRZQcmsB+vvtPsc5F4iIKrINAKYA+LSq3pxkYjJr113DV9+Xl24bdfY/W83NQzuiudXVCyvMDB1mPgRVM/NfS0u4YMw7Lr6723TC7OgYDEhmzTI97FmFTkQFZNsEsB5ATOO1GtCECWmnINtETOn/pJP8p+f1W8b2bW8zP4fR32/avltaqvcJ8I6LL182VjWeWQT9vteLL+ar8yER5ZbtE/j/A/BlZ4ne4olrspq8cTPwpibT894vY62U+T79dG3LCVerCXDHxXd3mxEBW7f618pEHTFQ6Xtt2JC/YYhElEu2TQCXwwwDfFZE1gN4o/wAVc32Cn21iGuyGq9SqfGbBdwMvHxoozs97/TppnZg27bhn61lOGR//+BIA/darlLJbG5zxAUX2K+kV6m5opw3qCinOhhUpD01MRFRANsagHUw8/H/CMBvADzhs+VX3HOst7UNnbwkr3p7ge3bkzn39u2Vx8XffbfZxowxKxfGvZJeFpfnpfrwa/bJ4+yTVAi2ywGfW21LOqGpmjUrvnOVSsA55wyO5547t/o0r41KNf6Z/lxtbf7j4k86CTjllMGqeVthRgxkcXleSl5SfUmIUhJ6OWARGSsiB4jI2CQSlEnz58d3rvI5223meaehKi18E9TeX02YWp4sLs9bVN4S+Zo1yZXIk+pLQpQi6wBARM4UkacAvAbgaQCvichTInJ6YqlrZLZztrvzvI8cWd/0NbJKC9/YVM37aW4Ot5Ie16LPhvISOZBciZzNPsO5wdejj7I5pEHZLgf8cQC3AHgewLkApjmvzwNYJiIfSyyFWbBgQfjPhJmz/cADw8822Cji/F7VFr6xqZr3s3On6bBoK4vL8xZNvUvkbPYZyht8DQywOaRB2dYAfBnAIlU9SVVvVNV7ndeTAHwfwFeSS2IGdHSE/0yYOdsXLMjvIkGjRsV3rmoL30Stcm9uBu64w/74rC7PayMvndjqXSJns88gNofkhm0A8A4At1d473bn/fxK+j911JJrEuIssYsAt9/uvy5AGKUSMG/e4DC9ShlY1Cr3nTvDldyClucVMfuvvdZkUlnKaPPUia3eJXI2+wxic0hu2AYArwKYVOG9Sc77+ZX0f+oslRpKpaGZ1syZpso7yj0YPdo0b5x2Wu1puugik0Eddhjwve8NzcC+9z2z/9hjowcaYf8NKi3Pu+eeJvP/3OeyldHmrdRW7xI5m30GsTkkN2wDgB8AuExEviIi7xSR3UXkIBH5CoBLASxJLokZEOcwQD9ZKjX095smi2efNZn/XXcBP/qRKdlOmwbssotdLUGpBLzvfSbDu+226DUcu+xiStsA8JGPmPH/5UMLVc3+e+6J3pQyMBC+lO43DHGvvUzmn7WMNm+ltnqXyBu52SdubA7JDdsA4AqYlQAvhpn0ZwOAJ53fr3Lez6+wwwDDrl1/7LH2x5ZKpl395JOTGT7Y1la5qvgXvzA/T55c/TzNzcD990cbkuc6+WTg8cdNLcLUqWZioSADA7VNLxxHKf3VV7OZ0eat1FbvEnm1Zh/A/K0feWT6TT1JY3NIbthOBDSgql8GMB7AZAAfd17Hq+pXVJOa7SUj2tvDZephSqHd3cDKlXbHujMIrltnZrp76qnBKXHjsmmTKelXKsFu3w78z/8AN9zg/zB0e+qfcIL9sr3lTj4Z6Ooy3/HZZ02G/Nxz0c4Vlk0pPagj3caN2cxo81ZqS6NEXt7s41cTlnZTTz2wOSQ3Qk0EpKqvq+qvVPVW5/X1pBKWOWEyszDx0IIFduc++WTzcPGOJvCWSuqprw945JHKU/GuXQs89FD0kv8DD5jXoDn3k1aplF6tI51tDUS1jDbu3vp5K7UFlcirDRet9boLF5q//112MfvK/783Yp+KMNgckhsVAwARmSQiG0VkWsAx00Rkg4i8O5nkFYDtCICHHvLf75ZKDjss3nQFcUuwfm3gboBSS0nSzXyjTuwTB79Suk1HOltBGW0SvfXzWGorL5ED1efciEve+lSEkVbwRbELqgH4AoDfqmpnpQOc934NIMa5cgsmjqrZ9nbgJz+pb01AtXTXMv7fzXzTHh5Z/h1tA5JqnSSDMtqkeuvntdTmDUIPO8x0XL3xRuCAA5Idepm3PhVheYOv5ma7Cc8oc4ICgCkAbGbAuQXA8fEkp4DiqJrt7jaZUz27YlRLz44dtZ2/pyf99ujy72gbkFT7dwjKaJMqWea91LZiBfDkk/Ubepm3PhVRuMHXEUfYTXhGmRMUAOwB4BWLc7wCYM94klNAYapm/dqFTz7ZlHwWLwa2batPmltagPHj/ecLGDMGeMc7ap/ZsK0t3fZov1K67cPcnQwoSkabZMmy0twFjV5qc2tNBgbqN/Qyb30qqJCCAoC/AdjH4hz7OMdSFLZVs0cd5d8u/POfm5759awq7+8HnnlmaDpuvtnMFxBmCd5K3MzXJjhKil8p3fZhPnp09Iw26ZJlUL+NRpVGe3we+1RQ4QQFAA8COM/iHLOdY8krzGQy1apm3ZnlahlTHwfvUMioQ/xsuJlvWkslt7T4l9JtHvoi5qEfNaNlyTI821qT7343vj4Bee1TQYUSFAB8E8BxIrJERN5S/qaI7CYiiwEcB+AbSSWwYYUpbVSrml29Ot2MX8Sk56CDwk9yFIU70VBQcNTcnMy1m5vNPAhnnjl8+J3NQ1+ktoc+S5bhhakNiatPQN77VFAhVAwAVPUxmAl/ZgB4RUR+JSI/EpEOEXkIwJ8BnAHgLFX9Y32S20DCttEGlRjT6g0/ahTQ2WnaVt98E3jxxWRL/q5f/MI8pJcsMZMBiQz9/qNGxb98sggwcqQJcFas8O9I9uyz1R/67e21PfRZsgwvTG1InH0C8tqngpKVoRU5AycCUtU7ABwEU8LvBXAUgIkAdgD4OoCDnGOoXJy9f9PoSXz44WbGQe9DrF7pcB/S550HLFoEbNky+F5Li+nzEHcgcuqp5j9jb29wR7IDDwx+6Lvj0aOKu2SZoYdNYqL0FYmrT0Ae+1RQcjK2ImfVmQBV9S+qeoWqvl9VD3a296vq/1PVv9QjkQ0pzjbaNNp7164FLr10aEYRZzqmTbN7aJePJqgl4y9vvnAz1M5O4K1vrX5uN9NI+qEfV8kyYw+bxETpK5LnMfqUTRlckTPUVMAUQpxttGn1hr/55qEZRVzpmDULePDB+jVrjBpl1i44//zKGWrWJnapNcjI4MMmMW6tSVNTuL/PPI/Rp+zJ4OyRDACSErWN1q/K9s0369P5rpzq0Iwirl75U6YMrdZP0sknm6aM2bODM9S8TeySwYdNoqZOBQ45xAR1tjiSguopa4UMMABITpTq4EpVtrfdZjrijRxZeSnSJHkziilTajuXiBnSmLQRIwZXFLT5t8j68LuwbfkZfNgkbuRIE9TNncuRFJQ9GSxkMADIimpVtr29ZpswwWRC9cj4vdf/wQ9McHLffbWdq6Ul+ar/Ugn49KfDBWFZHn4XpS0/gw+buuFICsqiDBYyGABkhe1CM11dprPaiBHm93rN/791a+0TEbkP5XoEAGEf7lnNNKK25WfwYVM3HKNPWZTBQkbQcsCtYba6pTivwiw0s327qQ1oNKVSvJl/pV79UR7uWc00Lr20+hoPfm35GXzY1BXH6FPWZLCQEVQD0ANgc4iNapHHqliXNwMdPbr287W2Vu/VH0XWMo0VK8z6CtVqefza8jP4sKm7RhijX4R5GsjIYCEjKACYHXJLnIh8SESeEZEuEbm4Htesm6SqYpOaMteGO4WwNwM95ZTa+i+ImHPNnm0yr5kzgV13NW3iN95omlJqeXjWO9OolAE88ICp2rdVHkBm8GEzBDO+4szTQIOyVshQ1YbYADQD6AawP4ARAP4I4JBKx0+cOFFjZf572m9dXeHOP3euaqkU/joVtpVXXWV+HjVKdcSI2M5bdSuVVFtbVW+4wXyn0aNVRczrSSepjhxZ2/lnzTL3q7PTXKf8nrnX7+yM99/f0sqVK+0PrvQdRMLflzFj/K/R1aU6b555v6nJvM6bF/7vM04J/duFuvdp6+oy3zXo37S1Nd1/J0sNdd9zxu/eA1itlvlqI3UCfA+ALlV9XlV3AFgG4MMpp6mySy8Nd3xSK9/Vc6ngUslEsu7qhX5LF9fSd2HkSOCyy/IxyU3Qd9CQHTuD2vKzVg2eh3+7OBRtngbKJOsAQETOFJFfisiLIvJa+ZZkIh37AHjJ8/vLzr5suvXWcMd7q2zjHuIXNkOJorUVeOopUy0fdeliEVMlVv79Rcxsfnfeae6TzcNz2zYTLGSV7agPG43Uls+MzyjiPA2UOaIWmYOInAVgCYAfApjj/NwE4FQAbwC4UVWvSC6ZgIjMAPAhVf2U8/vZAN6rqp/1HDPHSR/GjRs3cdmyZfElYM2a8J+ZODH8Z3p7gT//Gfjb38J/1qNn333R9vLLNZ3DiojZ2ttNW9aLLwIbNkQPOpqagD32ADZuNCXW5mZg7Fhgr71MDQAAPPqomRjJxgEH1L5ATwg9PT1os+nPEeY7VFPn71gT2+/d3AwccUSoU1vf+ywI8zyJ8hypo4a67znjd++nTJmyRlUnWZ3App0AwKMAvgzTDj8A4Chn/2gAvwfwL7ZtDlE3AMcAuNfz+yUALql0fOp9AIDarue2k7a0RLr23/sAJLmNGDG8PXn06NrOKVL93oRpI69zO6p1e2iUdn6/e3XqqcP7Wsydm922Y9vv3dQU+tQN1RZt+/+kUt+ODGmo+54z9eoDcACA36jqTgA7AYxxgofNAK4E8NmAz8blDwAOEJH9RGQEgI8BuKsO103H1KmmLT3LRowY3p5c63BG1ertv2HXf89idXIcJaYRI8zMjI3Ui7zIExR5FX2eBsoE2wBgEwCn/hWvADjY854AGBtnovyoaj9MoHEvgKcA3KqqTyR93chqXbynu9u0pce97n2cenqGZzK1PrhFqmfYYVYlzGo7ai0rK5ZKwC67mJ/9OnlmuTMdMz6D8zRQBtgGAH8AcLjz810AvioinxaRcwD8J0wzQOJUtVNVD1TVdlX9j3pcM7Jp02r7fJydxKoZNSr6Z8szmVqXDFatnmGHHTGRxUmWoo76GDHCjLT4yEeqt6VnsfaDGZ+R9XkaqBBsA4BvAHjR+fmrAB4GcB2AHwDYAOD8+JPW4GrtlGU7NXCpNNg5rpKRI00mX/6gaWkxna1qGXVQnsnEMZyxWobtPjxtpVmdXGnCG6ByBlBJayvw5JOm2eXuu5PpRZ70BD3M+AZlbVIYKh7bzgLlG0yTwJion096S70TYK2dd8J0lrrhhmGdBVdedZXZ506sUj4hzC67mPcjdjIM/K6dnbWdt63N7h7NnFn9PpVK5nvXyZBOOTYT3nj/XdzvUv6d/CbISaIzXT0nV0pggiJ2RksH73t66j4RkBh7Atihqptijkfyo9ZqZ9tS68iRpq+AH1XTkXDq1KETwjz7rCn19/fH08eg/LtOnQrce2/0aYgnTLA77vLLB9vCK0mrOtl2whtg8N9lYMCs9lheAvcrEcbdma7eE/RkbYIiogIKMxHQNBH5LYDtAP4XwHYR+a2InJRY6hpZa40LJNq0pbe0DD6c/TLynTtNcFD+0I67f4FfJnP88aaaOkwVt2v9ervjslydHHXCG9uMMe7OdJygh6hwrAIAETkfwN0wKwReCOB057UHwF3O+/m2++7hjrctxVZi05auWv08fg9t2/4FNkTMAj/A8PbjM88ETjsNOOOMcH0ienrs25+z2o6a9ExvcXem48x0RIVjWwPwbwCuV9UPqur3VPUO5/WDAL4PM0lQvm3fHu7455+v7Xo2pdsRI6pX4fs9tOPsFa8K3HKLyWj8Vja77TYzhe+yZeGWAnY/bzOmPUvVyb29JmjZbLlCdtR/i7hrP2zTkcURFUQUiW0AMBbAnRXeux3AW+JJToaFDQC2bq39mtVKt7ZpKn9ox90rfmAA+Pa3g9uPp00z8/OHHXGQ5THt5VasML30Fy+2/0wt/xZx1n5wgh6iwrENAFYCOK7Ce8cBeCie5GRYWg++oNJt1Id2rWP1o+rvt2u28JP19me3E93AgH3zShwT3sRV+8EJeogKxzYAuAbA2SJynYicKCJHOq/fA3A2gKtF5BB3Sy65KZo1K+0UDBf1oZ3U0sNJqrX9Oenx7VE6VmZpwhtO0ENUOLYBwL0AxsNM+LMCwGrndY6z/x4AjwNY57zmz/z5aadgOJuH9sAAMH360H1hJ9LJiqjtzytW+PdPiHPO/DAdK9MeoeAnyyMqiPwkHdQXgG0AMKVsO97n9+M9P+dPFh983od20Jj7U04ZnslNnQrMnJls+uIWpRmmXuPbwwQnaY9QqCSrIyqIytUjqC8AqxVrVPXBpBOSO7UuBmRr6lQz3v7EE/3f37lzMJNbu3ZoIHP55aaHfrUOi5MnA6tWxZXi4dyOgUH9A6K2P4cZ375wYfjzu1pbgS1bqh/X1lbbdZLm9inIchqp2LxBfbm+PrP5Pe9omNAzAZKl/fev37WWL6/eu77SpDNuDYJfwNLcDNxwg4mqmxL8U3EnnQ0Stf05yfHt3ipIm8wfqH1+CKKi46RVsan4VBeR10TkSOfnvzq/V9zql+QG8cIL5jWudqqg89SSybnVvuefP/Tc8+YBzzwDzJ5tjku6RkPEv/25pcVsqsABB4S/f0mNby+vgrRlO8shEfnjpFWxCXqqfxfAq56fI47fKqjeXpNJzJgxWC0FDLZTLV1qSt827arVzrNtm12aKmVU1ap9FywwTQlJGj0aeOQRE7XfdJPJkEeNMvdRZPA7hr1/bW12GXSY/gVBVZDVRPlMd7f5N+joMPelrc2MAJk/n1WcVDyctCo2FWsAVPVyVf2z8/Nlzu8Vt/oluYHE0fnMphOb7dh61cqdY4JqGG68MdkAwG3fL1+wCDDXLZ/tMMz9S2J8+4IFJjCJImxHRnZ2IhqKk1bFxnYtgPEiclSF944SkfHxJisn4minsmnvErGfYc8v06yWydi2b0fl175v8723bQMuuyz4mCTGt0cNiMIGGvVeoS/ryoPUtjbgwAPNfRUB1qwx02PPnFmce1JEnLQqNrY9u64DUGkmnLMA/Hc8ycmZONqpbNq7bDrRea/pDTpsMplafeEL4ceX237vjo7q6wTEPb49akAUNtBgZ6dBfkHqli3Ac88NrSHq6wNuvhk49FDWjuQVJ62KjW0AcDSAByq8t9J5n6Ko1k5l245lWwNQHnTEvTSwn0WLzFDFMOPLw7TfVSsFpz2+PWqgwc5ORlCQWklvr5kAizUB+cNJq2JjGwC0IrgT4K4xpCVfgibm8arWTmXbjhVmpT1v5hrn0sCV9PUBd9wRbs76MO13NqVgvznzL7rIBEBJziRWS6DBzk5G1CC1t7cYtSNFlHZQnxO2AcDjAD5e4b2PA3ginuTkyOTJ8bRThWnvsg0CmpsHM7l6ZB5hSqluO2+Y1RejlIJr6Vy3q2W829ZW29LE7OxkRA1SVfNfO1JkWVoGvEHZBgDfBHCWiNwmIic5Hf9OEpFbYQKA/0guiQ2qqyuedqow7V22q/z19w9mcvXKPGwCDW+mHPaBHyaQqbVz3Sc+Ub2Gp7kZOOcc+zT5YWcno5YgNe+1I0Q1sAoAVPVOAOcAOAbA3QD+4LweA2CWqv4ksRQ2qhdeiKedKkx7l+0qf6qDmdwpp9hlMlOmVD9vkGqBRpR23jDn96q1c938+cDIkcGfHzmy9k5I7Oxk1BKk5r12hKgG1vO7qupNMCv/HQLgfc7r21T1loTS1vjiaqeyPY83WLDR12c6D9pkMt//vpkWOAqbUmotnRHDloJr7VwXFJS5MxrG0QmJnZ0M25qtciL5rx0hqoWq5nKbOHGixm5wwJ3dlpauLl25YIFdGseMUe3sVG1tVS2Vhr5XKpn9nZ2D5z7rLFWRcPehtVW1qys4zaNHh7+/Yc7vZZv+pqaq91nnzTP3sKlJdcwYXXnjjeHSYsPnOjpvXvzXyaquLvNvXOXfa+VVVw3dN2pUce5RilauXJl2EgrL794DWK2W+aR1DYCIvFVE5ojIZaA44gAAIABJREFUFSLyrbLtygRjlMaU5OI51bS3288L0NMTrqbiiiuAXXYJlx6bUmqUttqopeC4Otf5dUIaPz7+EnnROzsF1YRUMnKkGXlSlHtEFIHtTIAfAfA8zJoA5wE43WfLP9ux9oBpW0+TbQDiZnK2mYz7MLZZHEjEVN/aNHWEaav1C1DCLLrEznWNxy9I9c4E6CqVzL/vE0/Y/d3FtVgXUSOyqSYA8BSAuwC8xbZqIe0t9SaAlKseVy5dOrxKv3wrlUxVchT336/a3Bxf1fzcudHTG6YJQ9WuSjlss4KD1aHpCX3vw/7dkC/+zaenXk0A4wFco6p/SygOyZ+0qx7HjUu2B/nxx5vZ/eLqoBa1x3uUIX3VOvEB5nNHHsnSYF5xnQUi6wDgtwAOSjIhFLORI5PpQe6tMj3pJJNhHnSQqY61GeVQqcoViJbeqEP6yquU/Zp3uOpefnGdBSLrAOCLAOaIyDlOZ8DW8i3JRFJEcU+XWWlBlmeeAQYGgJ/9LLiDWrXZ94Dw6a1lSJ/b7+GRRwY7NpZ3nmRpMJ+4zgKRdQCwFsC7APwAwEsANvts5BV3iTFqZ6W4epDbVJl+9KNmKVa/NNpWuQLh0hvHfPksDRYP11kgsg4AZgM419lmV9jIK84So1ty/v73h5acr7sOeOc7gSVL4rlOEJtMcts24JZb/Ev3F16YTCYbx5A+lgbzzS94thnFAnAmQco1q/8FqvrDhNORP25mtnBhbefxlpz99PcD550HvPYacPHFtV0riO2CLH5V6H19wM9/Xv2zfX3Ad79rmhPmzx9a6u/uNkFIR4cplbW1meFep5wC3HZbcNqqDeljaTC/Vqww/3/cv0PABKY2Q3o5FJRyLsXZanIurhLjggXAjh3Vj7vkkmRrAuqZ+ZV3vAvqO3DnndXnPKg22oGr7uVTULOTzURZRVhngQqt4pNTRB4WkUOcn//g/F5xq1+SG0gcmWZHhynl25gzx77ZwaZPgfcY25kF4+DtE/DAA8F9B7ZtMz+PGhV9tAMnBson2/UlymsDirTOAhVaUBPAEwC2eX6uYw6QE3GUGMMEETt32jU7VKoWXbwYWLrUPPiA4cfUW1+fXd+BgQHgjDOA3XYztS5uE8HZZ5sSXLWH+Pz55ntXa0ZgabCx2DZbtbSYUSBh/26IGlzFAEBVz/X8/Mm6pCZv4igxtrWZzNnWTTcFBwBBfQrczH76dFMqckvXaenrA9atszvu7rsHRwyE5U4M5BfwlEpmY2mw8dgGz+5oE6KCqdoHQERGiUiviJxWjwTlShwlxlmzwh1f7aFnUy3a2wts3x7uutWUSsDJJ4db0CWMzZtrm9M97jkTKH3s20EUqGoAoKrbAbwGwLIhmv4ujhLj/Pn2Q5aA6g8zm2pRd1b0OJVKwLe/PZjJxk218gRDtnMyFH3Vvbxh3w6iQLajAK4H8HkRSaDoZojIf4rI0yKyVkTuFJHdnP0TRGSbiDzmbN9LKg2Z1N4OXH+93bEtLdUfZkn15hexm8bXzWRnzqw+FKtUAg47zL7GIIk53dNcLY4r1dUm6voSRAVhGwDsBuAwAOtF5EYns/6WZ7syhrT8AsBhqno4gGcBXOJ5r1tVj3C2z8RwrcYyezbwjW9UP27EiOoPs6SqO1taTIbuzYTb2vyr0FesAG6/vXotQ6kEfOc7tTcZRJ3Fr9rUxUmuD5DmtfMiaNEn9vQnsg4APgqgF8AOAMcCmAHg9LKtJqp6n6q6zQy/B7BvredMXZwltYsvBm64AWhuHv5eS4v9w8ymWjSK/n6zLoCrVDK98086afiEPjNmVO9jsMsu5vscf3zwQ9xGlDkZ0lwtjivVxYd9O4gqsgoAVHW/Ktv+MadrNgBvEWc/EXlURB4UkWNjvlZy4p47fvZss/DOvHlDH2bnn2//MLOpFo3CdhEd27HZbs/+7u7gh7itsE0faa4PwLUJ4sW+HUS+RAOqYUVkFwDTAEwA8BcA96vqq5EvJvJLAP/o89aXVfWnzjFfBjAJwHRVVREZCaBNVTeKyEQAPwFwqKpu8jn/HABzAGDcuHETly1bFjWp/tasCXd8czNwxBHxpsFST08P2ipV92/aZDLWJDr7lRMB9twTGD/e/P7oo6ZmwPazIuZBPWaM/zG25wv7bxHxvIH3PeFrF10s955C431Pj9+9nzJlyhpVnWR1AlX13QDsD+B5AAOe7Q0AH6z0mVo3AJ8E8DsArQHHrAIwqdq5Jk6cqLEbzDLtNpH402Bp5cqVwQd0danOmxf+O0XZxowZvK5I+M+3tpr0+pk7V7VUCv58qWS+axi26WxqCnffE7x20cVy7yk03vf0+N17AKvVMs8NagL4lpPpHwugFcChAB6FGREQOxH5EIB/BXCqqm717N9TRJqdn/cHcABMYJJ9SVS1x8WtFh09Ovlreavfo5QUgqq7k+rpneYYco5fJ6I6CAoAjgHwFVX9japuV9WnAJwP4G0isncCaVkIYDSAX5QN93sfgLUi8hiA5QA+o6p/S+D61YUZjw/Ud/78qJLqFOjlzaiiXC+oE19SPb3THEPO8etEVAdBAcDeGF7S7gYg8G/Hr4mqvkNVx2vZcD9VvV1VD3X2HaWqd8d9bWvTpoU73nYRnzQl1SnQa9OmwTHsM2ZEu15QJ74kenqnOYac49eJqA6qjQJogCJsHYWtLt9112TSEadqJei4bN4MXHcdcMIJwCGHmGF+Yc5frbo77p7eaY4h5/h1IqqDagHAvSLymrvBjAQAgPu9+5338u+uu8IdP2FCIsmIXRzD7MJYvdpk0ieeaBcEpFXdneYYco5fJ6KEBTVqX163VDSKsGPJn89gX8XubjPOvKNjcPnTWbNMtfPChcNX01u0KJnlgHfsAB54ALjnHuCUU/xXJ3SlWd3t1ixEWWWwka9NdoL+P7GGhjIuaDlgBgDlwi7NG5SppWHFiuFL3rrTyy5daqqVy0uWp58O3HxzMunp6wPuuKM+S/HyQR0d752/KP+fiDLEdipgAsIvzZslUaeXveIKYOTIZNLk9u5Purqb8+pHx3vnj9M1Uw4wAAhj/vy0UxCdzfSy27YBl102dF97O3DnncCoUdVX74vCbVZJarpWPqij472rjNM1Uw4wAAgjSmaUlYdjR0f1B5aqOa68VDd1KrBu3fClaefNMwsU+fVWt5X0ZDZ8UEfHe1eZzf+nKItQEdURA4CkZeXhGKYDo1+prlIJffbsodX3YWoJ6tG7nw/q6HjvKrP9/xS24zBRHTEASFpWHo5hStphS3Xe4GBgAOjsNOP8q6lH734+qKPjvauM0zVTDjAASFpWHo5hpuCttVQ3dSrw+OPAySf7v1/PyWz4oI6O964yTtdMOcAAIGlZeTiGnfI3KHDp7h7eH+CCC4Y2G7S3A3ffDXR1mb4CaU1mwwd1dLx3lXG6ZsoBBgBJamrKzsPRnV7WVqXAJeywsKR699uyeVD39QFvvJGdDptZwUyuMk7XTDnAACBJAwPA9Olpp2LQ1KnAzJnVO+pVKtXFOSzMphYhDkEPaq9bby32uHY/zOSCcbpmanAMAJLU3GxmusuSyy+v3kGvpcW/VBfXsLB6Ty7jPqjPPDM43UmOa69XwBM3ZnLB0q7hIqoBA4Ak7dyZnVEALrdUFzS738AA8Oyzw/fHMSzMphZh2jTTBBFnBtneblZztGkOiHvoZqPPpsdMjiiXGAAkLczaAfVy4IHBzQC9vWaBngceGLo/jmFhNrUIALBlS/wZZFzj2sOU5rM+m16j1kwQUc0YAIQVtle/X4kz7YfuggWmJBfEXa7Xm/nGMSzMJhN2xZ1BxhHAVCvNb9o09HibgGfrVuDDH65/ptvoNRNEVBMGAGGFHdevOvT3LDx0bTPh/v6hma/NsDARs6ZApcAmyrwIcVXL1xrA2JTmu7uHfl/be/3EE/XNdLNeM0FEiWMAkLT+/sGfs/LQDZMJb90KHHywychnzKgeAKia71IpsIkyL4JbLV9rzUmt49ptSvOqQ4OVsPe6Xpku5/knKjwGAEkbPXrw56w8dMNmwn19JiM/5RTgS18Kt/hPeWATZkZCr82ba685qXVcu+2CSt4+BFHudT0yXc7zT1R4DACSJDK0NJmVh26UTNjNyK+80szw5x0WVipVn1vAzdjCzkjoUq295qTWce1R+hCEvdf1ynQ5zz9R4TEASJLq0NJkVh66UTNhwGRQd9wxdFjYqFHD+zr4fe6mm+wn5omSLpuScy3j2qP0IYhyr8v//ZPoNMp5/okKjwFAPWXloetmws3N4T/rV0ING9h4M+G4vmuYknPUce22nSC9tT7egMeW954k1WmU8/wTFR4DgKR5S6VZeuhOnQrcd5+Z9S8sNyN3S6bVSv8ub8bmZsKbN5vlg+OoEchCzYnI8D4EbsBz6KHVr+H990+y0yjn+ScqPAYASfOWSrP20D3+eOCuu8Jnvm1tQ0umNsoDG2+19kknmYzzoINqqxGoV81JUB+C9nb/moT2duCnP61eE+D990+y0yjn+ScqPAYASfOWSrP40PVWx9sGAZs2mel6/UqmlXgzNr9q7S1bgGeeMdMQV+tQWOn8UWtOwrSxB/UhuPtus6pgpfOE/fdPutMo5/knKjZVzeU2ceJETURLi6rJsuy2MWOGn6OrS3XePPNeU5N5nTfP7I/JypUrw3+oq0u1tTXc96u2lUrmnJ2dyV0DMOeMcv86O81nS6XgdFueZ+WCBdXP4/33FzHHlErm59GjVefONceI2H33pqbw3zuHIv3NU81439Pjd+8BrFbLfJI1AGGFLRWdeurwfVldXCWJHvre0mR3t5nyduvW4M+I2NcC1FJzElcbu/c8WtYfwu887r//smWDKzP6TZ40apTd92BPfSKKgAFAWGPGhDu+PEPIuvJq4Vo0NQ0GNm61/xNPVP+cW7atpq2tturquNrYo5zHJvjYsaN6J0321CeiiBgAhHXXXeGOv/vuZNKRJG8NhXcmw7Dckqk3s7MlEtxW3tlpSsu11JzE1cYe5Tw2QYNNLQh76hNRRAwAwgo71KzRZ1KLmn5vydR2CWCv0aOT76AW18RMUc5jEzT09w8GPFnpNEpEucEAIKyw7a1Jt89W6sHe2xvP+aOm31syDbMEsPvZs89Ovq9EXBMzRTmPbdDQ28ue+kSUCAYAYc2aFe74JNtng2aJe/LJeJaWDTuXvV/JNGwtQr2qteOamCnKecIEDVntNEpEDY0BQFgzZoQ7PqmMrFonsoGBeJaWtZ3LXqRyyTRMLUI9q7XjmpgpynmyNCskERUSA4Cwli8Pd3xSGVm9lha2mbyms9MEHJVKpra1CIcdVt9q7bgmZvKep7zjXqXzZG1WSCIqHAYAYXV0pJ0Co55LC9c6Y5xNZtfaCvzkJ/Wv1o5rNjz3PHvuaXeeLM4KSUSFwgAgrLDt2bVWwdeajjhGIXR3mxqHG280fQx23RWYOdOUTm0yqLCZXRLL31ZLXxxt7O3twPjx9ufhVLxElCIGAGGFWdYVqL0KvpJ6LS0c13K0tpldUsvfZhU7+BFRShgAhDVhQrjj46iC91OPTmRxL0dbLbNLcvlbIiIaggFAWGEzn6QmAqpHJ7J6dTRM63pERAWWmQBARC4TkVdE5DFnm+Z57xIR6RKRZ0TkxDTTie3bwx2f1ERA1drVm5pq70RWz46GaVyPiKjAMhMAOK5W1SOcrRMAROQQAB8DcCiADwH4bxFpTjOR1kSSHccd1K5+yCG1dyKLq6Ohbae+enZsJCIquKwFAH4+DGCZqvaq6p8AdAF4T8ppsqOa/DjuSu3qI0fWfm7b2ouBgcqZephOffXq2EhERJkLAD4rImtFZImI7O7s2wfAS55jXnb2NYZG7s0dZhpgv0w9bKc+zo5HRFQ3onVcr15EfgngH33e+jKA3wPYAEABfA3A3qo6W0QWAvi9qnY457gBwApVHTYln4jMATAHAMaNGzdx2bJl8X+JJ58Etm2zP/6ww+IpjYfU09ODtlpLyr295vsODIT7XFOTaYJ49VVgwwZT6q9ExEyeM358uOs1NQFjxwLjxtnf395ek6aNG801opyjiljuO0XCe58O3vf0+N37KVOmrFHVSVYnUNXMbQAmAFjn/HwJgEs8790L4Jhq55g4caIm4v77VU2WZrfNm5dMOqpYuXJlPCfq7FRtbVUtley/c6lkvvfo0XbHjxkT7Xqlkjm2szP69whzDgux3XcKjfc+Hbzv6fG79wBWq2Vem5kmABHZ2/PrRwCsc36+C8DHRGSkiOwH4AAAD9c7fX93/PHAiBH2xzd6j/XyjoY23J76UTr1lV+vfG798uts3QpMm2b6BVSaLZDzCxARDZOZAADAt0TkcRFZC2AKgIsAQFWfAHArgCcB3ANgnqruTC+ZAHbssD82Dz3WvR0NgzJkr56e6J36vNf7zGfs+iFs2VJ5tkDOL0BENExmAgBVPVtV36Wqh6vqqar6F897/6Gq7ap6kKo21lyweWsbC5Opx9Gpz2ZuAFel0jznFyAiGiYzAUBu5a3HephMPY7ZCqPUoJSX5usxv4A718Gjj9ZnASMiohoxAEha3tZzD5Op17rkbXc30NISPo3lpfmk5xfwznUwMJD/BYyIKBcYACStkecB8BM2U4+65K2bqfb3R0vnpk2DJfAk5xdgB0MialAMAPLAb6rdF19MLtMJm6mHXfLWm6nWMk+FWwKfNCl8U4Tt9MXsYEhEDYoBQKOrNNXuhg3JVj8nuY69TaZqwy2Bf+5zwLXX2tdahJm+mB0MiahBMQBoZEHVz6qNW/0cpue/jb4+4JFH7GotwlbpcwEjImpQDACSlmTmm9fq57gzS7cEblNrEfae2nYcbG5uvECMiHKNAUDSksx881r9bJup2i5UBNgHFWHvqe2CSf39HBFARJnCACBpS5cmd+68Vj/b9tqfMwcYPdrunLZBRdh7ajMsEmjsJhkiyiUGAElLMvMNO77dtmd72sLMNRD3EL+w99Q7LNJmmuRGbJIholxiANDIwmR+YXq2py3MXANxzDboFSWgcIdF2kxa1IhNMkSUSwwAGplt5jd9euNNVmM710Ctsw2WixpQtLfbT1rUaE0yRJRLDACisF0WFwjXUS2soMxPZDDzW768MUcL2M41EHW2wUrX/P/bu/touaryjuPfHwkkCEYIAiJEowEtqC1IRGh9CWirARShkPASMRQWC0MFaViCohCxriUuI5DGpiIg8mJiSaC8iK0UCavtEjQ0VnknIMaEt0AgEASSkKd/7H1zJ8Pk3pmTebtzfp+1zrr3nLPnzDN7zr3zzD777F00oWj1kMNmZk3kBKCIRqYDbrVNffjtuGP/h1+v3i1QqZkDExVNKKZMGbwfQNEhh83MmswJQBGvvFJ/2bVrW9/xrtaH35gx/R9+vXq3QCsVSSimT68vAei1CaLMbEhyAtAOne5456bp9hg3Li3N6o9gZtZCTgDaodMd71o5G55tbNSo5vVHMDNrIScAndSujnfNvlVuqIwn0CmtnCjJzKxJnAB0Urs63jXzVrmhNJ6AmZltkhOAIrbZpnnHalfHu2bcKtfoTHlmZta1nAAUcfzxaXa3Zmhnx7vNbZru1dkHzcxKyAlAEdOnw4gRm3+codbxrgzjCZiZlYQTgCL6rqnXM/b7QIbaPeEeT8DMrGc4AShq4kQ4/fT6ylZfLhiq94R7PAEzs57hBKCoRx6Biy8evJwEe+7ZG/eEezwBM7Oe4QSgqJkz65v9LQKWLu2Ne8IbmX3Q4wSYmXU1JwBFXX11/WV75Zp4PeMJnHUWfOpTHifAzKzLOQEo6sUX6y/bS9fEBxpP4Kab4IILPE6AmdkQ4ASgHXrtmvimxhOYP9/jBJiZDRFOANphKN3qtzk8ToCZ2ZDhBKAdhmKHvyI8ToCZ2ZDhBMCax+MEmJkNGU4Aitp++/rKlenDzuMEmJkNGU4Aipo1q75yr7xSnl7v9Y4TUJY+EWZmXcwJQFFTpsDYsYOXW7cOzjuv5eF0hXrGCRhqwx+bmfUoJwCbY/ny+spde21r4+gmA40TMBSHPzYz61GbOZ1dyQ12y1ufNWtaG0e36RsnYPbsTkdiZmab4BYAMzOzEnICUFQjHfsG6xhnZmbWZk4Aipo5s/6ykye3Lg4zM7MCuqYPgKSfAO/Oq9sBz0fE3pLGAvcDD+Z9d0bEKe2PsMqVV9ZfdsaMloVhZmZWRNckABGx4WuypJnAqordj0TE3u2PagAvvVR/Wd/2ZmZmXaZrEoA+kgRMAg7qdCxmZma9qhv7AHwYeCoiHq7Y9g5JiyXdIenDnQqssLKMBGhmZkOGIqJ9Tyb9J/CWGrvOiYgbcpk5wJKImJnXRwDbRsSzkvYF/g14T0S8UOP4JwMnA+y88877zps3r0WvBFi8GNavr6/sTjvBmDGti6WG1atXs22Z5iHoEq73znHdd4brvXNq1f2BBx54d0SMr+fxbU0ABiNpOLAc2Dcilm2izELgzIhYNNCxxo8fH4sWDVhk80ybBnPm1Fd21ChYtWrwck20cOFCJkyY0NbnNNd7J7nuO8P13jm16l5S3QlAt10C+DjwQOWHv6QdJQ3Lv78T2AN4tEPx9TvyyPrLrl7dujjMzMwK6LZOgEcDc6u2fQQ4X9JaYD1wSkSsbHtk1ebPr7+sm8fMzKzLdFUCEBFTa2xbACxofzSDuPrq+st+9rOti8PMzKyAbrsEMHQ00qx/xBGti8PMzKwAJwBFNdKsP2tW6+IwMzMrwAlAUVOm1F/2xhtbF4eZmVkBTgCKauQugAj42c9aF4uZmVmDnAAU1chdAJASBo8IaGZmXcIJQFGN3AUAsHYtXHhha2IxMzNrkBOAohod3GftWrjqqtbEYmZm1iAnAEUVGdzHIwKamVmXcAJQ1JQpsOWWjT3GIwKamVmXcAJQ1PTp9c8G2McjApqZWZdwAlDUL38Jr73W2GPOOKM1sZiZmTXICUBRp53WWPlDD4Vx41oTi5mZWYOcABT13HP1l916a7jootbFYmZm1iAnAO2wYIG//ZuZWVdxAtAOEyd2OgIzM7ONOAEwMzMrIScAZmZmJeQEwMzMrIScABRV7yiAjY4WaGZm1gZOAIo66qj6yk2e3No4zMzMCnACUNT558OIEQOXGTECZsxoSzhmZmaNcAJQ1LhxcP31MHIkSBvvk9L266/3/f9mZtaVnABsjokT4Z57YNo0GDUKttgi/Zw2LW33/f9mZtalhnc6gCFv3DiYPTstZmZmQ4RbAMzMzErICYCZmVkJOQEwMzMrIScAZmZmJeQEwMzMrIScAJiZmZWQEwAzM7MScgJgZmZWQk4AzMzMSsgJgJmZWQk5ATAzMyshJwBmZmYl5ATAzMyshBQRnY6hJSStAP7Q6Tg66M3AM50OooRc753juu8M13vn1Kr7t0fEjvU8uGcTgLKTtCgixnc6jrJxvXeO674zXO+ds7l170sAZmZmJeQEwMzMrIScAPSuSzodQEm53jvHdd8ZrvfO2ay6dx8AMzOzEnILgJmZWQk5Aegxkj4p6UFJSySd3el4eo2kMZJul3SfpHslnZ63j5Z0q6SH88/t83ZJmpXfj99Ken9nX8HQJmmYpMWSbs7r75B0V67fn0jaKm8fkdeX5P1jOxn3UCdpO0nzJT0g6X5JB/icbz1JZ+T/M/dImitpZDPPeScAPUTSMOB7wERgL+AYSXt1Nqqesw6YHhF7AfsDp+Y6Phu4LSL2AG7L65Deiz3ycjIwp/0h95TTgfsr1i8ALoyI3YHngBPz9hOB5/L2C3M5K+5i4N8j4s+AvyC9Bz7nW0jSrsBpwPiIeC8wDDiaJp7zTgB6y37Akoh4NCLWAPOAwzocU0+JiCci4n/z7y+S/hHuSqrnH+ViPwI+k38/DLgykjuB7STt0uawe4Kk3YBDgEvzuoCDgPm5SHW9970f84GP5fLWIElvAj4CXAYQEWsi4nl8zrfDcGBrScOBNwBP0MRz3glAb9kV+GPF+rK8zVogN7HtA9wF7BwRT+RdTwI759/9njTPRcCXgPV5fQfg+YhYl9cr63ZDvef9q3J5a9w7gBXAD/Pll0slbYPP+ZaKiOXAd4ClpA/+VcDdNPGcdwJgVoCkbYEFwBcj4oXKfZFurfHtNU0k6VDg6Yi4u9OxlNBw4P3AnIjYB3iJ/uZ+wOd8K+Q+FYeRErC3AtsAn2zmczgB6C3LgTEV67vlbdZEkrYkffhfExHX5c1P9TVz5p9P5+1+T5rjr4BPS3qMdGnrINJ16e1y8yhsXLcb6j3vfxPwbDsD7iHLgGURcVden09KCHzOt9bHgd9HxIqIWAtcR/o7aNo57wSgt/wa2CP3Et2K1GHkxg7H1FPyNbXLgPsj4rsVu24EPpd//xxwQ8X243PP6P2BVRXNplaniPhyROwWEWNJ5/UvIuI44HbgyFysut773o8jc3l/Qy0gIp4E/ijp3XnTx4D78DnfakuB/SW9If/f6av3pp3zHgiox0g6mHStdBhweUR8s8Mh9RRJHwL+C/gd/deiv0LqB/CvwNtIs1BOioiV+Q93Nqnp7k/ACRGxqO2B9xBJE4AzI+JQSe8ktQiMBhYDUyLiVUkjgatIfTRWAkdHxKOdinmok7Q3qfPlVsCjwAmkL5A+51tI0teByaS7jxYDJ5Gu9TflnHcCYGZmVkK+BGBmZlZCTgDMzMxKyAmAmZlZCTkBMDMzKyEnAGZmZiXkBMBKT9IMSVGxPC5pgaRxdTz2CklNv8Upx/RMs4+bjz01v85t6yi7d55h7ElJa3LdXCPpA62IrddImiRpap1lJ0u6TtIT+f2p63FmRTkBMEtWAQfk5Uxgb+C2POb5QL4BTG1BPJcCn2jBcesm6QjgV6TxxM8gjUw2nTTC2M87GNpQMon6z48jgbHAza0KxqzS8MGLmJXCujxzGcCdkpbwwX9GAAAGjElEQVSSBvw5GLi2urCkrSPi5Yh4pBXBRMQy0hCsHSHpraSZxeYCU6tGFJubx+a35pocEetzy8xJnQ7Gep9bAMxq65t0ZiyApMckzZT0NUnLgBfy9o0uAVQ0r79P0q2SXpL0QP42vRFJh0v6laSXJT0r6RZJb8/7NroEIGlCPu7fSLo5H3eppFOqjnmApBtzM/JLkn4j6bgCr/8k0qhv02sNJxoRG76lShqW410q6VVJ90o6tiquKyQtknSIpPsk/UnSTyWNlrS7pNtzvIsk/XnVY0PSP0i6WNJKSc9L+qc83HVlub0l3ZaP/Vy+VLFzxf6x+ViTJH1f0ipJyyR9XdIWVcd6b47vxbxcK+ktFfv73o8Jed9qSY9Kmlb5moG/BT5acXlpxqYqPCLWb2qfWSs4ATCrbWz++WTFtmOBjwLTSMNzDuTHpLG5DwceBuYpzWcPgKTPkib3eITUTHwC8BCw4yDHvQz4LXAEcAswp+rb+NuB/wFOBD5FmrToh5KOGeS41T4KLIqIevohnA+cA1wCfDo//zU1nvNtuexXgZOBv8yPmZeXI0mtkvOk181jPp008clxwD/mx28Y5lrSjsBC0pzpxwJfyK/h1upEAfg2sDo/39XAufSPrY6k3fNrGAlMITXhvwe4qUZcPwD+j/Q+LwS+J2m/vO8bpHHbF9N/eelSzLpFRHjxUuoFmAE8Q/rwGQ68i/SP+wVgl1zmMdKc3COrHnsF6YOyb30qaVrUv6vYtgNpLO9T8voWpJm7rhsspor1Cfm4l1SVuxW4cxPHUH493ydNDFId47YDPP8DwNw66m40aXrY86q23wI8WFVP64BxFdu+neM4vmLbwXnbnhXbIsezRcW2c0jjzI/O698CngdGVZT5YH7sMXl9bF6/sirW3wDzKtavAh4EtqrYtgfwGnBI1ftxfkWZLYEVwLcqts0HFjZ4Pm6bjz21038bXnp7cQuAWbIDsDYvDwLvJF2TrZzF7LaIeKXO423oJBcRz5KmSu1rAXg3aX7vHxaI8/qq9euAfSUNgzSHuKRZkv5A/+s5mZTUNKqeiULeS/rWXd1P4ifAu/I38z6PxcZ9Jpbkn7+osW3XquPdEBs3kV8HbJ2fH2A/4OcR8cKG4NP0tY8BH6o6VnUHxvvof28gdXa8HlgvabjS1Kq/z8cav6ljRZqy9eGqY5l1LXcCNEtWkf7xB6nZ//GIqP4AfKqB4z1ftb6G1KQMKdmA1KLQqKdrrA8H3kyK7wpgf1Lz832kVozPA4c1+DzLSU32g9kl/6yum7710aRvxVC7Tqq3920bWVW21uuufP5dgHtrxPdUjqHSQO8NpLo8Ky/VxlStD3Yss67lBMAsWReDT1narKkzn80/dxmwVG071VhfBzyjNB3oocCpEfEvfQWqO7jVaSFwjqTREbFygHJ9ScxO9L8ugL7OdwM9thG1Xnfl8z9Ro0xfHHfX2D6QlaQWgFrX61syNoNZJ/gSgFn7PUj6hv25Ao89vMb63RHxGjCC9Df9at9OSW8kdcxr1GWkywffqbVT0iH513tI1+KPqioyCXgoIlbQHIdVJTJHAC/n5we4C/hEfr19MX6AdN3/vxt8rttInf7ujohFVctjDR7LLQLWtdwCYNZmke71/hKpp/w1pHvtAziI1PFuoJaIiZK+CdxB+hD8a3LzfkSskvRr4FxJLwDrgbNJlzdGNRjj40oj0c3Ndy9cTkpadgWOBj5C6oC3UtJFwFclrQMW5bgOBhq982AgbwSulfQD0ofz14DvVbROfJd0qeM/JF1A6kj3LeB3pDshGjGDNADSTyVdTvrWvyuprq+IiIUNHOsBUvLyGdK4Do9HxOO1CkraC9iL/oRhvKTVwIqIuKPB12A2KCcAZh0QET+W9AqpN/t8Uk/6O+m/Xr4pJwFfJI3Mt5LU3H9jxf5jSb3+ryQ1yc8mddL7+wIxLpD0QeDLwMX0X8//Bam/RJ9zSZchPk9qcl8CTImIeY0+5wBmkjpmziW1clwGfKUi1hWSDszl5pK+ed8CnBERa15/uE2LiIck7U+63fASUmfD5aSWgSUDPbaGfwb2ISVQ2wNfJyUYtUwCzqtYPzUvd5DuOjBrKr2+n5OZdRtJE0i3Jr4vIu4ZpHhPkRTAFyJidqdjMesl7gNgZmZWQk4AzMzMSsiXAMzMzErILQBmZmYl5ATAzMyshJwAmJmZlZATADMzsxJyAmBmZlZCTgDMzMxK6P8Bnn3tiPseQWcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoUIWOz2RcWD"
      },
      "source": [
        "## German Credit Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpHgVHGxxWrB"
      },
      "source": [
        "#### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoL4HUl9xYiC"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "latent_dim = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hCsEr8iR1OJ"
      },
      "source": [
        "### GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSmU-XmQR1OJ"
      },
      "source": [
        "https://arxiv.org/pdf/1904.09135.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IISNamFPR1OJ"
      },
      "source": [
        "#### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4rRLQt8R1OJ"
      },
      "source": [
        "# build discriminator model\n",
        "def build_disc(input_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(256, input_shape=(input_shape,)))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])  \n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onJWEui3R1OJ"
      },
      "source": [
        "#### Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6FB4lHQR1OK"
      },
      "source": [
        "# build generator model\n",
        "def build_gen(output_shape, noise_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(512, input_shape=(noise_shape,)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dense(256))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dense(output_shape, activation='tanh')) \n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdv9QRIHR8B2"
      },
      "source": [
        "### Run Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLYvcsxlRcWE"
      },
      "source": [
        "###### plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUW_YSkvRcWE"
      },
      "source": [
        "# create a line plot of loss for the gan and save to file\n",
        "def plot_history(d_hist, g_hist, d_acc_hist, adv_acc_hist):\n",
        "  # plot loss\n",
        "  pyplot.subplot(2, 1, 1)\n",
        "  pyplot.plot(d_hist, label='discriminator loss')\n",
        "  pyplot.plot(g_hist, label='generetor loss')\n",
        "  pyplot.legend()\n",
        "  pyplot.title(\"Discriminator and Generator Loss\")\n",
        "  pyplot.xlabel(\"steps\")\n",
        "  pyplot.ylabel(\"loss\")\n",
        "  # plot discriminator accuracy\n",
        "  pyplot.subplot(2, 1, 2)\n",
        "  pyplot.plot(d_acc_hist, label='disc_acc')\n",
        "  pyplot.plot(adv_acc_hist, label='adv_acc')\n",
        "  pyplot.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4uZCUndRcWE"
      },
      "source": [
        "##### losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shWIJvuX8nUC"
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8z2uc69RcWF"
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss, real_loss, fake_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asX4DsQ-RcWF"
      },
      "source": [
        "def discriminator_real_loss(real_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    return real_loss\n",
        "    \n",
        "def discriminator_fake_loss(fake_output):\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    return fake_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFlHR7Bl8l-f"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5W853PX8k86"
      },
      "source": [
        "disc_accuracy_tracker = tf.keras.metrics.Accuracy(name=\"disc_accuracy\")\n",
        "disc_accuracy_real_tracker = tf.keras.metrics.Accuracy(name=\"disc_accuracy_real\")\n",
        "disc_accuracy_fake_tracker = tf.keras.metrics.Accuracy(name=\"disc_accuracy_fake\")\n",
        "adv_accuracy_tracker = tf.keras.metrics.Accuracy(name=\"adv_accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4BoduqrRcWF"
      },
      "source": [
        "##### train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOjwwEbZRcWG"
      },
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(records):\n",
        "    num_sampels = len(records)\n",
        "\n",
        "    noise = tf.random.normal([num_sampels, latent_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape, tf.GradientTape() as disc_fake_tape, tf.GradientTape() as gen_tape_2:  \n",
        "      \n",
        "      generated_records = generator(noise, training=True) # generate samples from noise \n",
        "     \n",
        "      real_output = discriminator(records, training=True) # feed dsic with real data\n",
        "      fake_output = discriminator(generated_records, training=True)# feed disc with fake data\n",
        "      \n",
        "      # Generator train\n",
        "      gen_loss = generator_loss(fake_output)  # calc gen loss\n",
        "\n",
        "      #discriminator train\n",
        "      total_disc_loss, real_loss, fake_loss  = discriminator_loss(real_output, fake_output)\n",
        "      real_new_loss = discriminator_real_loss(real_output)\n",
        "      fake_new_loss = discriminator_fake_loss(fake_output)\n",
        "\n",
        "    # calculate grads\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator_real = disc_tape.gradient(real_new_loss, discriminator.trainable_variables)\n",
        "    gradients_of_discriminator_fake = disc_fake_tape.gradient(fake_new_loss, discriminator.trainable_variables)\n",
        "   \n",
        "    # apply grads\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator_real, discriminator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator_fake, discriminator.trainable_variables))\n",
        "    \n",
        "    # evaluate accuracy\n",
        "    rounded_real = tf.math.round(real_output)\n",
        "    rounded_fake = tf.math.round(fake_output)\n",
        "\n",
        "    disc_output = tf.concat([rounded_real, rounded_fake], axis=0)\n",
        "    disc_real = tf.concat([tf.ones_like(rounded_real), tf.zeros_like(rounded_fake)], axis =0)\n",
        "\n",
        "    disc_accuracy_tracker.update_state(disc_real, disc_output)\n",
        "    adv_accuracy_tracker.update_state(tf.ones_like(rounded_fake), rounded_fake)\n",
        "\n",
        "    disc_accuracy_real_tracker.update_state(tf.ones_like(rounded_real), rounded_real)\n",
        "    disc_accuracy_fake_tracker.update_state(tf.zeros_like(rounded_fake), rounded_fake)\n",
        "\n",
        "    return total_disc_loss, real_loss, fake_loss, disc_accuracy_real_tracker.result(), disc_accuracy_fake_tracker.result(), \\\n",
        "    gen_loss, disc_accuracy_tracker.result(), adv_accuracy_tracker.result()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbPyveq9RcWG"
      },
      "source": [
        "d_hist, d_r_hist, d_f_hist, g_hist, d_acc_hist, adv_acc_hist, real_acc_hist, fake_acc_hist= list(), list(), list(), list(), list(), list(), list(), list()\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    for image_batch in dataset:\n",
        "      d_loss, real_loss, fake_loss, real_acc, fake_acc, g_loss, d_accuracy, g_accuracy, =  train_step(image_batch)\n",
        "      \n",
        "      # record history\n",
        "      d_hist.append(d_loss.numpy())\n",
        "      d_r_hist.append(real_loss.numpy())\n",
        "      d_f_hist.append(fake_loss.numpy())\n",
        "      g_hist.append(g_loss.numpy())\n",
        "      d_acc_hist.append(d_accuracy.numpy())\n",
        "      adv_acc_hist.append(g_accuracy.numpy())\n",
        "      real_acc_hist.append(real_acc.numpy())\n",
        "      fake_acc_hist.append(fake_acc.numpy())\n",
        "\n",
        "    print('>%d, d_loss=%.3f, d_R_loss=%.3f, d_f_loss=%.3f, real_acc=%d, fake_acc=%d, g_loss=%.3f, d_acc=%d, aadv_acc=%d' %\n",
        "\t\t\t(epoch, d_hist[-1], d_r_hist[-1], d_f_hist[-1], int(100*real_acc_hist[-1]), int(100*fake_acc_hist[-1]),\\\n",
        "     g_hist[-1], int(100*d_acc_hist[-1]), int(100*adv_acc_hist[-1])))\n",
        "  \n",
        "  plot_history(d_r_hist, d_f_hist, d_acc_hist, adv_acc_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k2fQkvu7RcWH",
        "outputId": "5fb64fdb-c6ce-4998-a4b9-9417c7e6b1b1"
      },
      "source": [
        "discriminator = build_disc(50)\n",
        "generator = build_gen(50, latent_dim)\n",
        "train(train_data_german, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_48 (Dense)             (None, 256)               13056     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_32 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_33 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 145,153\n",
            "Trainable params: 145,153\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_51 (Dense)             (None, 512)               10752     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_34 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_35 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 50)                12850     \n",
            "=================================================================\n",
            "Total params: 158,002\n",
            "Trainable params: 156,466\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n",
            ">0, d_loss=0.641, d_R_loss=0.101, d_f_loss=0.540, real_acc=98, fake_acc=65, g_loss=0.925, d_acc=81, aadv_acc=34\n",
            ">1, d_loss=0.603, d_R_loss=0.193, d_f_loss=0.410, real_acc=99, fake_acc=76, g_loss=1.157, d_acc=87, aadv_acc=23\n",
            ">2, d_loss=1.124, d_R_loss=0.318, d_f_loss=0.806, real_acc=98, fake_acc=74, g_loss=0.660, d_acc=86, aadv_acc=25\n",
            ">3, d_loss=1.134, d_R_loss=0.243, d_f_loss=0.891, real_acc=95, fake_acc=66, g_loss=0.565, d_acc=81, aadv_acc=33\n",
            ">4, d_loss=1.657, d_R_loss=0.814, d_f_loss=0.843, real_acc=92, fake_acc=59, g_loss=0.616, d_acc=75, aadv_acc=40\n",
            ">5, d_loss=1.522, d_R_loss=0.545, d_f_loss=0.976, real_acc=88, fake_acc=51, g_loss=0.490, d_acc=69, aadv_acc=48\n",
            ">6, d_loss=1.552, d_R_loss=0.669, d_f_loss=0.883, real_acc=84, fake_acc=45, g_loss=0.544, d_acc=65, aadv_acc=54\n",
            ">7, d_loss=1.483, d_R_loss=0.659, d_f_loss=0.824, real_acc=82, fake_acc=42, g_loss=0.598, d_acc=62, aadv_acc=57\n",
            ">8, d_loss=1.162, d_R_loss=0.437, d_f_loss=0.725, real_acc=81, fake_acc=40, g_loss=0.668, d_acc=60, aadv_acc=59\n",
            ">9, d_loss=1.169, d_R_loss=0.452, d_f_loss=0.717, real_acc=80, fake_acc=39, g_loss=0.679, d_acc=60, aadv_acc=60\n",
            ">10, d_loss=1.428, d_R_loss=0.609, d_f_loss=0.819, real_acc=80, fake_acc=39, g_loss=0.625, d_acc=59, aadv_acc=60\n",
            ">11, d_loss=1.360, d_R_loss=0.499, d_f_loss=0.861, real_acc=80, fake_acc=38, g_loss=0.579, d_acc=59, aadv_acc=61\n",
            ">12, d_loss=1.283, d_R_loss=0.465, d_f_loss=0.818, real_acc=80, fake_acc=38, g_loss=0.615, d_acc=59, aadv_acc=61\n",
            ">13, d_loss=1.142, d_R_loss=0.357, d_f_loss=0.785, real_acc=79, fake_acc=39, g_loss=0.660, d_acc=59, aadv_acc=60\n",
            ">14, d_loss=1.300, d_R_loss=0.613, d_f_loss=0.687, real_acc=79, fake_acc=39, g_loss=0.758, d_acc=59, aadv_acc=60\n",
            ">15, d_loss=1.193, d_R_loss=0.446, d_f_loss=0.747, real_acc=79, fake_acc=39, g_loss=0.679, d_acc=59, aadv_acc=60\n",
            ">16, d_loss=1.262, d_R_loss=0.566, d_f_loss=0.697, real_acc=79, fake_acc=40, g_loss=0.714, d_acc=59, aadv_acc=59\n",
            ">17, d_loss=1.490, d_R_loss=0.543, d_f_loss=0.948, real_acc=78, fake_acc=40, g_loss=0.518, d_acc=59, aadv_acc=59\n",
            ">18, d_loss=1.364, d_R_loss=0.591, d_f_loss=0.773, real_acc=78, fake_acc=40, g_loss=0.699, d_acc=59, aadv_acc=59\n",
            ">19, d_loss=1.301, d_R_loss=0.527, d_f_loss=0.774, real_acc=78, fake_acc=40, g_loss=0.664, d_acc=59, aadv_acc=59\n",
            ">20, d_loss=1.174, d_R_loss=0.575, d_f_loss=0.599, real_acc=77, fake_acc=41, g_loss=0.829, d_acc=59, aadv_acc=58\n",
            ">21, d_loss=1.257, d_R_loss=0.539, d_f_loss=0.718, real_acc=77, fake_acc=41, g_loss=0.702, d_acc=59, aadv_acc=58\n",
            ">22, d_loss=1.431, d_R_loss=0.596, d_f_loss=0.835, real_acc=77, fake_acc=41, g_loss=0.605, d_acc=59, aadv_acc=58\n",
            ">23, d_loss=1.263, d_R_loss=0.464, d_f_loss=0.799, real_acc=77, fake_acc=41, g_loss=0.676, d_acc=59, aadv_acc=58\n",
            ">24, d_loss=1.228, d_R_loss=0.600, d_f_loss=0.627, real_acc=77, fake_acc=42, g_loss=0.809, d_acc=59, aadv_acc=57\n",
            ">25, d_loss=1.387, d_R_loss=0.714, d_f_loss=0.673, real_acc=76, fake_acc=42, g_loss=0.774, d_acc=59, aadv_acc=57\n",
            ">26, d_loss=1.224, d_R_loss=0.547, d_f_loss=0.677, real_acc=76, fake_acc=43, g_loss=0.791, d_acc=59, aadv_acc=56\n",
            ">27, d_loss=1.449, d_R_loss=0.782, d_f_loss=0.667, real_acc=76, fake_acc=43, g_loss=0.728, d_acc=59, aadv_acc=56\n",
            ">28, d_loss=1.282, d_R_loss=0.604, d_f_loss=0.677, real_acc=76, fake_acc=43, g_loss=0.758, d_acc=59, aadv_acc=56\n",
            ">29, d_loss=1.238, d_R_loss=0.489, d_f_loss=0.749, real_acc=76, fake_acc=43, g_loss=0.680, d_acc=59, aadv_acc=56\n",
            ">30, d_loss=1.231, d_R_loss=0.560, d_f_loss=0.671, real_acc=76, fake_acc=44, g_loss=0.747, d_acc=60, aadv_acc=55\n",
            ">31, d_loss=1.179, d_R_loss=0.605, d_f_loss=0.573, real_acc=76, fake_acc=44, g_loss=0.865, d_acc=60, aadv_acc=55\n",
            ">32, d_loss=1.390, d_R_loss=0.592, d_f_loss=0.798, real_acc=75, fake_acc=45, g_loss=0.717, d_acc=60, aadv_acc=54\n",
            ">33, d_loss=1.595, d_R_loss=0.814, d_f_loss=0.781, real_acc=75, fake_acc=45, g_loss=0.637, d_acc=60, aadv_acc=54\n",
            ">34, d_loss=1.175, d_R_loss=0.528, d_f_loss=0.647, real_acc=75, fake_acc=45, g_loss=0.768, d_acc=60, aadv_acc=54\n",
            ">35, d_loss=1.268, d_R_loss=0.634, d_f_loss=0.634, real_acc=75, fake_acc=46, g_loss=0.777, d_acc=60, aadv_acc=54\n",
            ">36, d_loss=1.256, d_R_loss=0.597, d_f_loss=0.659, real_acc=75, fake_acc=46, g_loss=0.782, d_acc=60, aadv_acc=53\n",
            ">37, d_loss=1.315, d_R_loss=0.720, d_f_loss=0.595, real_acc=75, fake_acc=46, g_loss=0.838, d_acc=60, aadv_acc=53\n",
            ">38, d_loss=1.171, d_R_loss=0.484, d_f_loss=0.687, real_acc=75, fake_acc=46, g_loss=0.780, d_acc=60, aadv_acc=53\n",
            ">39, d_loss=1.213, d_R_loss=0.563, d_f_loss=0.650, real_acc=74, fake_acc=47, g_loss=0.800, d_acc=61, aadv_acc=52\n",
            ">40, d_loss=1.379, d_R_loss=0.744, d_f_loss=0.634, real_acc=74, fake_acc=47, g_loss=0.773, d_acc=61, aadv_acc=52\n",
            ">41, d_loss=1.291, d_R_loss=0.657, d_f_loss=0.634, real_acc=74, fake_acc=47, g_loss=0.857, d_acc=61, aadv_acc=52\n",
            ">42, d_loss=1.217, d_R_loss=0.582, d_f_loss=0.634, real_acc=74, fake_acc=47, g_loss=0.844, d_acc=61, aadv_acc=52\n",
            ">43, d_loss=1.410, d_R_loss=0.755, d_f_loss=0.655, real_acc=74, fake_acc=47, g_loss=0.767, d_acc=61, aadv_acc=52\n",
            ">44, d_loss=1.466, d_R_loss=0.662, d_f_loss=0.803, real_acc=74, fake_acc=48, g_loss=0.604, d_acc=61, aadv_acc=51\n",
            ">45, d_loss=1.306, d_R_loss=0.683, d_f_loss=0.624, real_acc=74, fake_acc=48, g_loss=0.790, d_acc=61, aadv_acc=51\n",
            ">46, d_loss=1.344, d_R_loss=0.652, d_f_loss=0.692, real_acc=74, fake_acc=48, g_loss=0.802, d_acc=61, aadv_acc=51\n",
            ">47, d_loss=1.219, d_R_loss=0.489, d_f_loss=0.730, real_acc=74, fake_acc=48, g_loss=0.716, d_acc=61, aadv_acc=51\n",
            ">48, d_loss=1.310, d_R_loss=0.564, d_f_loss=0.746, real_acc=74, fake_acc=49, g_loss=0.685, d_acc=61, aadv_acc=50\n",
            ">49, d_loss=1.585, d_R_loss=0.849, d_f_loss=0.736, real_acc=74, fake_acc=49, g_loss=0.701, d_acc=61, aadv_acc=50\n",
            ">50, d_loss=1.395, d_R_loss=0.708, d_f_loss=0.687, real_acc=73, fake_acc=49, g_loss=0.725, d_acc=61, aadv_acc=50\n",
            ">51, d_loss=1.117, d_R_loss=0.507, d_f_loss=0.610, real_acc=73, fake_acc=49, g_loss=0.844, d_acc=61, aadv_acc=50\n",
            ">52, d_loss=1.269, d_R_loss=0.582, d_f_loss=0.687, real_acc=73, fake_acc=49, g_loss=0.779, d_acc=61, aadv_acc=50\n",
            ">53, d_loss=0.958, d_R_loss=0.347, d_f_loss=0.611, real_acc=73, fake_acc=49, g_loss=0.814, d_acc=61, aadv_acc=50\n",
            ">54, d_loss=1.340, d_R_loss=0.744, d_f_loss=0.595, real_acc=73, fake_acc=50, g_loss=0.829, d_acc=61, aadv_acc=49\n",
            ">55, d_loss=1.360, d_R_loss=0.685, d_f_loss=0.675, real_acc=73, fake_acc=50, g_loss=0.764, d_acc=61, aadv_acc=49\n",
            ">56, d_loss=1.142, d_R_loss=0.465, d_f_loss=0.677, real_acc=73, fake_acc=50, g_loss=0.762, d_acc=61, aadv_acc=49\n",
            ">57, d_loss=1.348, d_R_loss=0.683, d_f_loss=0.665, real_acc=73, fake_acc=50, g_loss=0.742, d_acc=61, aadv_acc=49\n",
            ">58, d_loss=1.075, d_R_loss=0.531, d_f_loss=0.545, real_acc=73, fake_acc=50, g_loss=0.955, d_acc=61, aadv_acc=49\n",
            ">59, d_loss=1.258, d_R_loss=0.535, d_f_loss=0.723, real_acc=73, fake_acc=50, g_loss=0.716, d_acc=62, aadv_acc=49\n",
            ">60, d_loss=1.349, d_R_loss=0.593, d_f_loss=0.756, real_acc=73, fake_acc=50, g_loss=0.704, d_acc=62, aadv_acc=49\n",
            ">61, d_loss=1.383, d_R_loss=0.624, d_f_loss=0.759, real_acc=73, fake_acc=50, g_loss=0.720, d_acc=62, aadv_acc=49\n",
            ">62, d_loss=1.385, d_R_loss=0.701, d_f_loss=0.685, real_acc=73, fake_acc=50, g_loss=0.770, d_acc=61, aadv_acc=49\n",
            ">63, d_loss=1.173, d_R_loss=0.649, d_f_loss=0.524, real_acc=73, fake_acc=51, g_loss=0.958, d_acc=62, aadv_acc=48\n",
            ">64, d_loss=1.192, d_R_loss=0.587, d_f_loss=0.604, real_acc=73, fake_acc=51, g_loss=0.848, d_acc=62, aadv_acc=48\n",
            ">65, d_loss=1.183, d_R_loss=0.507, d_f_loss=0.676, real_acc=73, fake_acc=51, g_loss=0.757, d_acc=62, aadv_acc=48\n",
            ">66, d_loss=1.381, d_R_loss=0.653, d_f_loss=0.728, real_acc=73, fake_acc=51, g_loss=0.705, d_acc=62, aadv_acc=48\n",
            ">67, d_loss=1.132, d_R_loss=0.524, d_f_loss=0.607, real_acc=72, fake_acc=51, g_loss=0.859, d_acc=62, aadv_acc=48\n",
            ">68, d_loss=1.307, d_R_loss=0.529, d_f_loss=0.778, real_acc=72, fake_acc=51, g_loss=0.655, d_acc=62, aadv_acc=48\n",
            ">69, d_loss=1.211, d_R_loss=0.585, d_f_loss=0.626, real_acc=72, fake_acc=51, g_loss=0.777, d_acc=62, aadv_acc=48\n",
            ">70, d_loss=1.087, d_R_loss=0.473, d_f_loss=0.614, real_acc=72, fake_acc=52, g_loss=0.862, d_acc=62, aadv_acc=47\n",
            ">71, d_loss=1.182, d_R_loss=0.587, d_f_loss=0.596, real_acc=72, fake_acc=52, g_loss=0.882, d_acc=62, aadv_acc=47\n",
            ">72, d_loss=1.278, d_R_loss=0.510, d_f_loss=0.767, real_acc=72, fake_acc=52, g_loss=0.676, d_acc=62, aadv_acc=47\n",
            ">73, d_loss=1.314, d_R_loss=0.641, d_f_loss=0.674, real_acc=72, fake_acc=52, g_loss=0.775, d_acc=62, aadv_acc=47\n",
            ">74, d_loss=1.192, d_R_loss=0.498, d_f_loss=0.694, real_acc=72, fake_acc=52, g_loss=0.741, d_acc=62, aadv_acc=47\n",
            ">75, d_loss=1.078, d_R_loss=0.540, d_f_loss=0.538, real_acc=72, fake_acc=52, g_loss=0.912, d_acc=62, aadv_acc=47\n",
            ">76, d_loss=1.286, d_R_loss=0.536, d_f_loss=0.750, real_acc=72, fake_acc=52, g_loss=0.694, d_acc=62, aadv_acc=47\n",
            ">77, d_loss=1.327, d_R_loss=0.707, d_f_loss=0.620, real_acc=72, fake_acc=52, g_loss=0.788, d_acc=62, aadv_acc=47\n",
            ">78, d_loss=1.352, d_R_loss=0.489, d_f_loss=0.863, real_acc=72, fake_acc=52, g_loss=0.576, d_acc=62, aadv_acc=47\n",
            ">79, d_loss=1.155, d_R_loss=0.541, d_f_loss=0.614, real_acc=72, fake_acc=52, g_loss=0.824, d_acc=62, aadv_acc=47\n",
            ">80, d_loss=1.284, d_R_loss=0.638, d_f_loss=0.645, real_acc=72, fake_acc=52, g_loss=0.786, d_acc=62, aadv_acc=47\n",
            ">81, d_loss=1.040, d_R_loss=0.468, d_f_loss=0.573, real_acc=72, fake_acc=52, g_loss=0.869, d_acc=62, aadv_acc=47\n",
            ">82, d_loss=1.270, d_R_loss=0.601, d_f_loss=0.668, real_acc=72, fake_acc=53, g_loss=0.762, d_acc=62, aadv_acc=46\n",
            ">83, d_loss=1.424, d_R_loss=0.667, d_f_loss=0.757, real_acc=72, fake_acc=53, g_loss=0.712, d_acc=62, aadv_acc=46\n",
            ">84, d_loss=1.198, d_R_loss=0.579, d_f_loss=0.619, real_acc=72, fake_acc=53, g_loss=0.866, d_acc=62, aadv_acc=46\n",
            ">85, d_loss=1.130, d_R_loss=0.514, d_f_loss=0.616, real_acc=72, fake_acc=53, g_loss=0.818, d_acc=62, aadv_acc=46\n",
            ">86, d_loss=1.176, d_R_loss=0.510, d_f_loss=0.666, real_acc=72, fake_acc=53, g_loss=0.797, d_acc=62, aadv_acc=46\n",
            ">87, d_loss=1.415, d_R_loss=0.699, d_f_loss=0.716, real_acc=72, fake_acc=53, g_loss=0.712, d_acc=62, aadv_acc=46\n",
            ">88, d_loss=1.238, d_R_loss=0.637, d_f_loss=0.602, real_acc=72, fake_acc=53, g_loss=0.886, d_acc=62, aadv_acc=46\n",
            ">89, d_loss=1.368, d_R_loss=0.582, d_f_loss=0.786, real_acc=72, fake_acc=53, g_loss=0.672, d_acc=62, aadv_acc=46\n",
            ">90, d_loss=1.167, d_R_loss=0.583, d_f_loss=0.585, real_acc=72, fake_acc=53, g_loss=0.843, d_acc=62, aadv_acc=46\n",
            ">91, d_loss=1.562, d_R_loss=0.805, d_f_loss=0.757, real_acc=72, fake_acc=53, g_loss=0.659, d_acc=63, aadv_acc=46\n",
            ">92, d_loss=1.265, d_R_loss=0.691, d_f_loss=0.574, real_acc=72, fake_acc=53, g_loss=0.895, d_acc=63, aadv_acc=46\n",
            ">93, d_loss=1.518, d_R_loss=0.804, d_f_loss=0.714, real_acc=72, fake_acc=53, g_loss=0.755, d_acc=63, aadv_acc=46\n",
            ">94, d_loss=1.211, d_R_loss=0.673, d_f_loss=0.537, real_acc=72, fake_acc=54, g_loss=1.011, d_acc=63, aadv_acc=45\n",
            ">95, d_loss=1.258, d_R_loss=0.505, d_f_loss=0.753, real_acc=72, fake_acc=54, g_loss=0.659, d_acc=63, aadv_acc=45\n",
            ">96, d_loss=1.389, d_R_loss=0.727, d_f_loss=0.662, real_acc=72, fake_acc=54, g_loss=0.804, d_acc=63, aadv_acc=45\n",
            ">97, d_loss=1.164, d_R_loss=0.524, d_f_loss=0.640, real_acc=72, fake_acc=54, g_loss=0.826, d_acc=63, aadv_acc=45\n",
            ">98, d_loss=1.048, d_R_loss=0.523, d_f_loss=0.525, real_acc=72, fake_acc=54, g_loss=0.935, d_acc=63, aadv_acc=45\n",
            ">99, d_loss=1.176, d_R_loss=0.564, d_f_loss=0.611, real_acc=72, fake_acc=54, g_loss=0.828, d_acc=63, aadv_acc=45\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMP_WYDJSFai"
      },
      "source": [
        "### Validate Performance German Credit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yko1Dmppyivq"
      },
      "source": [
        "#### Evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sggq7RpSFai",
        "outputId": "36679545-296f-482a-c5ea-55462f4ab616"
      },
      "source": [
        "# evaluate real data accuracy\n",
        "prediction = tf.math.round(discriminator.predict(train_data_german))\n",
        "real_acc = tf.keras.metrics.Accuracy()\n",
        "real_acc.update_state(tf.ones_like(prediction), prediction)\n",
        "result_real = real_acc.result()\n",
        "print(f\"Real data accuracy: {result_real.numpy()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Real data accuracy: 0.7979999780654907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_IGXZetSFaj",
        "outputId": "ae7f53e6-bac5-43f5-fc4a-90613ba8ae1f"
      },
      "source": [
        "# evaluate fake data accuracy\n",
        "num_examples_to_generate = 100\n",
        "seed = tf.random.normal([num_examples_to_generate, latent_dim])\n",
        "prediction_gen = generator(seed, training=False)\n",
        "prediction_disc_1 = discriminator.predict(prediction_gen)\n",
        "prediction_disc = tf.math.round(prediction_disc_1)\n",
        "fake_acc = tf.keras.metrics.Accuracy()\n",
        "fake_acc.update_state(tf.zeros_like(prediction_disc), prediction_disc)\n",
        "result_fake = fake_acc.result()\n",
        "print(f\"Fake data accuracy: {result_fake.numpy()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fake data accuracy: 0.7099999785423279\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qyKa1msSFak"
      },
      "source": [
        "# inverse to real values\n",
        "predictions_inverse = mms_german.inverse_transform(prediction_gen)\n",
        "real_data = mms_german.inverse_transform(data_german_credit_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NETsKLzmy44T",
        "outputId": "2bfeb6b5-bcff-4033-d1eb-58792a5d4e7d"
      },
      "source": [
        "# samples which fooled the detector\n",
        "indices_fooled = [i for i in range(100) if prediction_disc[i] == 1]\n",
        "chosen_samples = random.sample(indices_fooled, 5)\n",
        "for i in chosen_samples:\n",
        "  distances_scale = []\n",
        "  for item in data_german_credit_encoded:\n",
        "    distances_scale.append(np.linalg.norm(prediction_gen[i] - item))\n",
        "  closest_index = np.argmin(distances_scale)\n",
        "  print(f\"sample number {i}:\")\n",
        "  print(f\"Fake sample : {predictions_inverse[i].astype(int)}\")\n",
        "  print(f\"Closest real : {real_data[closest_index].astype(int)}\")\n",
        "  print(f\"Euclidean Distance: {distances_scale[closest_index]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample number 12:\n",
            "Fake sample : [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   3   3  10 530   3   3  30   1   1]\n",
            "Closest real : [  0   0   1   0   0   0   0   0   0   1   0   0   0   0   0   0   0   1\n",
            "   0   1   0   0   1   0   0   0   0   0   1   0   1   0   0   1   0   0\n",
            "   1   0   1   0   1   2   2  11 766   4   3  66   1   1]\n",
            "Euclidean Distance: 3.033763885498047\n",
            "sample number 29:\n",
            "Fake sample : [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    2\n",
            "    0   16 2789    1    1   24    1    1]\n",
            "Closest real : [   0    0    1    0    0    0    0    0    0    1    0    0    0    0\n",
            "    0    0    0    0    1    1    0    0    1    0    0    0    0    0\n",
            "    1    0    1    0    0    0    1    0    0    1    1    0    1    4\n",
            "    2   11 1483    2    1   25    1    1]\n",
            "Euclidean Distance: 2.953584671020508\n",
            "sample number 64:\n",
            "Fake sample : [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    2    0\n",
            "    0   21 3301    1    1   31    1    1]\n",
            "Closest real : [   0    0    1    0    0    1    0    0    0    0    0    0    0    0\n",
            "    0    0    1    0    0    1    0    0    0    1    0    0    0    0\n",
            "    1    0    1    0    0    0    1    0    1    0    1    0    3    2\n",
            "    2   11 1386    2    2   26    1    1]\n",
            "Euclidean Distance: 4.189874172210693\n",
            "sample number 72:\n",
            "Fake sample : [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    1    9 2011    3    1   26    1    1]\n",
            "Closest real : [   0    0    1    0    0    0    0    0    1    0    0    0    0    0\n",
            "    0    0    0    1    0    1    0    0    0    0    1    0    0    0\n",
            "    1    0    1    0    1    0    0    0    1    0    1    0    0    0\n",
            "    0   18 2472    4    1   25    1    1]\n",
            "Euclidean Distance: 4.0378828048706055\n",
            "sample number 83:\n",
            "Fake sample : [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    2   20 2310    3    1   32    1    1]\n",
            "Closest real : [  0   0   1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   1\n",
            "   0   1   0   0   0   0   1   0   0   0   1   0   1   0   0   1   0   0\n",
            "   1   0   1   0   0   0   2   8 653   4   3  28   1   1]\n",
            "Euclidean Distance: 3.6353302001953125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YzFStVWzZb9",
        "outputId": "c0a683f8-01e4-46f2-ca78-dbf908a3ecef"
      },
      "source": [
        "# samples which did not fool the detector\n",
        "indices_not_fooled = [i for i in range(100) if prediction_disc[i] == 0]\n",
        "chosen_samples = random.sample(indices_not_fooled, 5)\n",
        "for i in chosen_samples:\n",
        "  distances_scale = []\n",
        "  for item in data_german_credit_encoded:\n",
        "    distances_scale.append(np.linalg.norm(prediction_gen[i] - item))\n",
        "  closest_index = np.argmin(distances_scale)\n",
        "  print(f\"sample number {i}:\")\n",
        "  print(f\"Fake sample : {predictions_inverse[i].astype(int)}\")\n",
        "  print(f\"Closest real : {real_data[closest_index].astype(int)}\")\n",
        "  print(f\"Euclidean Distance: {distances_scale[closest_index]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample number 17:\n",
            "Fake sample : [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    1    0\n",
            "    3   23 1808    2    3   39    3    1]\n",
            "Closest real : [   0    0    0    0    1    0    0    0    0    1    0    0    0    0\n",
            "    0    0    0    1    0    1    0    0    0    0    1    0    0    0\n",
            "    1    0    1    0    0    0    1    0    0    1    1    0    3    0\n",
            "    4   18 6070    3    4   33    2    1]\n",
            "Euclidean Distance: 3.698437213897705\n",
            "sample number 45:\n",
            "Fake sample : [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    2    3\n",
            "    3   10 1018    2    3   30    2    1]\n",
            "Closest real : [   0    0    0    0    1    0    0    0    0    1    0    0    0    0\n",
            "    0    0    0    1    0    1    0    0    0    1    0    0    0    0\n",
            "    1    0    1    0    0    0    1    0    1    0    1    0    3    4\n",
            "    3   30 6742    2    3   36    2    1]\n",
            "Euclidean Distance: 3.2288851737976074\n",
            "sample number 55:\n",
            "Fake sample : [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   2   0   3  20 920   3   3  32   2   1]\n",
            "Closest real : [   0    0    0    0    1    0    0    0    0    1    0    0    0    0\n",
            "    0    0    0    1    0    1    0    0    0    0    1    0    0    0\n",
            "    1    0    1    0    0    0    1    0    1    0    1    0    3    0\n",
            "    4   28 2743    4    2   28    2    1]\n",
            "Euclidean Distance: 2.87572979927063\n",
            "sample number 2:\n",
            "Fake sample : [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    1    2\n",
            "    3    7 1785    3    3   38    1    1]\n",
            "Closest real : [   0    0    1    0    0    0    0    0    1    0    0    0    0    0\n",
            "    0    0    0    1    0    1    0    0    0    1    0    0    0    0\n",
            "    1    0    1    0    0    1    0    0    1    0    1    0    1    0\n",
            "    2   11 1921    4    2   37    1    1]\n",
            "Euclidean Distance: 2.5849335193634033\n",
            "sample number 97:\n",
            "Fake sample : [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   3   6 704   3   3  28   1   1]\n",
            "Closest real : [   0    0    1    0    0    0    0    0    1    0    0    0    0    0\n",
            "    0    0    0    1    0    1    0    0    0    1    0    0    0    0\n",
            "    1    0    1    0    0    1    0    0    1    0    1    0    1    0\n",
            "    2   11 1921    4    2   37    1    1]\n",
            "Euclidean Distance: 3.365809917449951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2LamDd-zhZy",
        "outputId": "ca7b9c9e-b705-41a3-a157-53ca9ddef6d1"
      },
      "source": [
        "# fooled successfully samples - mean distance\n",
        "indices_fooled = [i for i in range(100) if prediction_disc[i] == 1]\n",
        "closest_distances = []\n",
        "for i in indices_fooled:\n",
        "  distances_scale = []\n",
        "  for item in data_german_credit_encoded:\n",
        "    distances_scale.append(np.linalg.norm(prediction_gen[i] - item))\n",
        "  closest_distances.append(min(distances_scale))\n",
        "print(f\"The mean distance of samples which fooled the detector: {np.mean(closest_distances)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean distance of samples which fooled the detector: 3.4326775074005127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LL9g4hQzjxK",
        "outputId": "ac22f224-1fa4-46b7-9804-aad77fbc6c62"
      },
      "source": [
        "# Samples which did not fooled - mean distance\n",
        "indices_not_fooled = [i for i in range(100) if prediction_disc[i] == 0]\n",
        "closest_distances = []\n",
        "for i in indices_not_fooled:\n",
        "  distances_scale = []\n",
        "  for item in data_german_credit_encoded:\n",
        "    distances_scale.append(np.linalg.norm(prediction_gen[i] - item))\n",
        "  closest_distances.append(min(distances_scale))\n",
        "print(f\"The mean distance of samples which did not fooled the detector: {np.mean(closest_distances)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean distance of samples which did not fooled the detector: 3.432727098464966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIiSmEIuSFam"
      },
      "source": [
        "#### Dimensionality Reudction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFb44z8SSFam"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(real_data)\n",
        "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
        "principalDf['target'] = 'real'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "fzjmibpmz_Fq",
        "outputId": "eb0ded27-5987-42ea-b737-d10830b88bf0"
      },
      "source": [
        "# all fake data\n",
        "a = pca.transform(predictions_inverse)\n",
        "b = pd.DataFrame(a, columns=['principal component 1', 'principal component 2'])\n",
        "b['target'] = 'Fooled'\n",
        "principalDf = principalDf.append(b, ignore_index=True)\n",
        "principalDf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>principal component 1</th>\n",
              "      <th>principal component 2</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-2102.286066</td>\n",
              "      <td>32.968887</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2679.802872</td>\n",
              "      <td>-17.318299</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1175.275817</td>\n",
              "      <td>14.446452</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4610.783135</td>\n",
              "      <td>7.120072</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1598.746901</td>\n",
              "      <td>17.170536</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>-2641.715750</td>\n",
              "      <td>-11.843569</td>\n",
              "      <td>Fooled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>-46.781646</td>\n",
              "      <td>-6.063050</td>\n",
              "      <td>Fooled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>-2026.094821</td>\n",
              "      <td>-4.721135</td>\n",
              "      <td>Fooled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>-2229.254998</td>\n",
              "      <td>-6.827738</td>\n",
              "      <td>Fooled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>2191.949884</td>\n",
              "      <td>-7.920166</td>\n",
              "      <td>Fooled</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      principal component 1  principal component 2  target\n",
              "0              -2102.286066              32.968887    real\n",
              "1               2679.802872             -17.318299    real\n",
              "2              -1175.275817              14.446452    real\n",
              "3               4610.783135               7.120072    real\n",
              "4               1598.746901              17.170536    real\n",
              "...                     ...                    ...     ...\n",
              "1995           -2641.715750             -11.843569  Fooled\n",
              "1996             -46.781646              -6.063050  Fooled\n",
              "1997           -2026.094821              -4.721135  Fooled\n",
              "1998           -2229.254998              -6.827738  Fooled\n",
              "1999            2191.949884              -7.920166  Fooled\n",
              "\n",
              "[2000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7WwQ8MT0GNJ"
      },
      "source": [
        "# fake data which did not fool the detector\n",
        "indices_not_fooled = [i for i in range(100) if prediction_disc[i] == 0]\n",
        "index_0 = len(real_data)\n",
        "for i in indices_not_fooled:\n",
        "  principalDf[\"target\"].iloc[i+index_0] = 'NotFooled'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "i5PKx--JSFan",
        "outputId": "6b4f6394-a5ec-4503-849e-238cdae2942f"
      },
      "source": [
        "# plot real and fake data\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = ['real','NotFooled', 'Fooled']\n",
        "colors = ['r', 'black', 'b']\n",
        "for target, color in zip(targets, colors):\n",
        "    if target ==\"NotFooled\":\n",
        "      s = 60\n",
        "    else:\n",
        "      s=50\n",
        "    indicesToKeep = principalDf[\"target\"] == target\n",
        "    ax.scatter(principalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , principalDf.loc[indicesToKeep, 'principal component 2']\n",
        "               , c = color\n",
        "               , s = s)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "ik2Fi0hjkCw2",
        "outputId": "2bf0f36e-e3df-4d5d-ec5f-8deac14c72e7"
      },
      "source": [
        "# plot real data only\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = ['real']\n",
        "colors = ['r']\n",
        "for target, color in zip(targets, colors):\n",
        "    indicesToKeep = principalDf[\"target\"] == target\n",
        "    ax.scatter(principalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , principalDf.loc[indicesToKeep, 'principal component 2']\n",
        "               , c = color\n",
        "               , s = s)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oajgk_4-VE8"
      },
      "source": [
        "#Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM4w2f7NrM79"
      },
      "source": [
        "##Global Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBJTarNPrPdW"
      },
      "source": [
        "def plot_graphs(a_list, b_list, a_name, b_name, title, x_axis_name, y_axis_name):\n",
        "  plt.plot(a_list, 'b')\n",
        "  plt.plot(b_list, 'r')\n",
        "  plt.legend([a_name, b_name])\n",
        "  plt.title(title)\n",
        "  plt.xlabel(x_axis_name)\n",
        "  plt.ylabel(y_axis_name)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xak94f2hsMLR"
      },
      "source": [
        "def encode_cat_features(df, cat_features_inds):\n",
        "  # encodes categorical features into a number between 0 and 1, depends on the number of labels in the feature.\n",
        "  # returns two dictionaries: one for the bins (the intervals of the values) and one for the labels (group_names) of the encoded value\n",
        "  bins_dict = {}\n",
        "  group_names_dict = {}\n",
        "  df = df.copy()\n",
        "  for cat_feature_ind in cat_features_inds:\n",
        "    cat_feature = df.iloc[:, cat_feature_ind]\n",
        "    unique_vals = cat_feature.unique()\n",
        "    num_unique_vals = len(unique_vals)\n",
        "    i=0.5\n",
        "    hops = 1 / num_unique_vals\n",
        "    bins_dict[cat_feature_ind] = [0]\n",
        "    group_names_dict[cat_feature_ind] = []\n",
        "    for val in unique_vals:\n",
        "      gen_val = round(i * hops, 3)\n",
        "      cat_feature[cat_feature == val] = gen_val\n",
        "      bins_dict[cat_feature_ind].append(round((i+0.5)*hops, 3))\n",
        "      group_names_dict[cat_feature_ind].append(val)\n",
        "      i+=1\n",
        "    df.iloc[:, cat_feature_ind] = cat_feature\n",
        "  \n",
        "  return df, bins_dict, group_names_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJkGoXSlvWVv"
      },
      "source": [
        "def decode_cat_features(df, bins_dict, group_names_dict):\n",
        "  df = df.copy()\n",
        "  for key, val in bins_dict.items():\n",
        "    df.iloc[:, key] = pd.cut(df.iloc[:, key], bins=bins_dict[key], labels=group_names_dict[key])\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oe6U4MXEj2K"
      },
      "source": [
        "##Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJtYpTuQA3_p",
        "outputId": "f6e52ed2-d530-41a3-aa10-c833efcaac35"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9LDyPz4w06v"
      },
      "source": [
        "###Diabetes Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "o3JaiGmBuMnl",
        "outputId": "0cc54f4e-2b46-4ebc-d010-f6ef8e0c6381"
      },
      "source": [
        "\n",
        "home_path = \"drive/MyDrive/Deep Learning Assignment 4\"\n",
        "\n",
        "diabetes_dataset = loadarff(home_path + \"/diabetes.arff\")\n",
        "diabetes_data = pd.DataFrame(diabetes_dataset[0])\n",
        "diabetes_data.loc[diabetes_data[\"class\"] == b'tested_negative', \"class\"] = 0\n",
        "diabetes_data.loc[diabetes_data[\"class\"] == b'tested_positive', \"class\"] = 1\n",
        "\n",
        "diabetes_class =diabetes_data[\"class\"]\n",
        "diabetes_data = diabetes_data.drop(columns=\"class\")\n",
        "\n",
        "display(diabetes_data)\n",
        "display(diabetes_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preg</th>\n",
              "      <th>plas</th>\n",
              "      <th>pres</th>\n",
              "      <th>skin</th>\n",
              "      <th>insu</th>\n",
              "      <th>mass</th>\n",
              "      <th>pedi</th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     preg   plas  pres  skin   insu  mass   pedi   age\n",
              "0     6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0\n",
              "1     1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0\n",
              "2     8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0\n",
              "3     1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0\n",
              "4     0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0\n",
              "..    ...    ...   ...   ...    ...   ...    ...   ...\n",
              "763  10.0  101.0  76.0  48.0  180.0  32.9  0.171  63.0\n",
              "764   2.0  122.0  70.0  27.0    0.0  36.8  0.340  27.0\n",
              "765   5.0  121.0  72.0  23.0  112.0  26.2  0.245  30.0\n",
              "766   1.0  126.0  60.0   0.0    0.0  30.1  0.349  47.0\n",
              "767   1.0   93.0  70.0  31.0    0.0  30.4  0.315  23.0\n",
              "\n",
              "[768 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      0\n",
              "2      1\n",
              "3      0\n",
              "4      1\n",
              "      ..\n",
              "763    0\n",
              "764    0\n",
              "765    0\n",
              "766    1\n",
              "767    0\n",
              "Name: class, Length: 768, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "VZuUsUCnw5vj",
        "outputId": "7bf883a4-b57a-4828-b24a-9e35745f10f4"
      },
      "source": [
        "# scale the data between 0 and 1\n",
        "\n",
        "diabetes_scaler = MinMaxScaler()\n",
        "scaled_diabetes_data = pd.DataFrame(diabetes_scaler.fit_transform(diabetes_data))\n",
        "display(scaled_diabetes_data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.743719</td>\n",
              "      <td>0.590164</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500745</td>\n",
              "      <td>0.234415</td>\n",
              "      <td>0.483333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.427136</td>\n",
              "      <td>0.540984</td>\n",
              "      <td>0.292929</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.396423</td>\n",
              "      <td>0.116567</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.919598</td>\n",
              "      <td>0.524590</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.347243</td>\n",
              "      <td>0.253629</td>\n",
              "      <td>0.183333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.447236</td>\n",
              "      <td>0.540984</td>\n",
              "      <td>0.232323</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.418778</td>\n",
              "      <td>0.038002</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.688442</td>\n",
              "      <td>0.327869</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.198582</td>\n",
              "      <td>0.642325</td>\n",
              "      <td>0.943638</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.507538</td>\n",
              "      <td>0.622951</td>\n",
              "      <td>0.484848</td>\n",
              "      <td>0.212766</td>\n",
              "      <td>0.490313</td>\n",
              "      <td>0.039710</td>\n",
              "      <td>0.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.613065</td>\n",
              "      <td>0.573770</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.548435</td>\n",
              "      <td>0.111870</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.608040</td>\n",
              "      <td>0.590164</td>\n",
              "      <td>0.232323</td>\n",
              "      <td>0.132388</td>\n",
              "      <td>0.390462</td>\n",
              "      <td>0.071307</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.491803</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.448584</td>\n",
              "      <td>0.115713</td>\n",
              "      <td>0.433333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.467337</td>\n",
              "      <td>0.573770</td>\n",
              "      <td>0.313131</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.453055</td>\n",
              "      <td>0.101196</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2  ...         5         6         7\n",
              "0    0.352941  0.743719  0.590164  ...  0.500745  0.234415  0.483333\n",
              "1    0.058824  0.427136  0.540984  ...  0.396423  0.116567  0.166667\n",
              "2    0.470588  0.919598  0.524590  ...  0.347243  0.253629  0.183333\n",
              "3    0.058824  0.447236  0.540984  ...  0.418778  0.038002  0.000000\n",
              "4    0.000000  0.688442  0.327869  ...  0.642325  0.943638  0.200000\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "763  0.588235  0.507538  0.622951  ...  0.490313  0.039710  0.700000\n",
              "764  0.117647  0.613065  0.573770  ...  0.548435  0.111870  0.100000\n",
              "765  0.294118  0.608040  0.590164  ...  0.390462  0.071307  0.150000\n",
              "766  0.058824  0.633166  0.491803  ...  0.448584  0.115713  0.433333\n",
              "767  0.058824  0.467337  0.573770  ...  0.453055  0.101196  0.033333\n",
              "\n",
              "[768 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBsN6AJ8w24_"
      },
      "source": [
        "###German Credit Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbE8Wjv0gCk1",
        "outputId": "2446f6fd-a2b8-4933-ce50-9159f9260c84"
      },
      "source": [
        "home_path = \"drive/MyDrive/Deep Learning Assignment 4\"\n",
        "\n",
        "german_credit_dataset = loadarff(home_path + \"/german_credit.arff\")\n",
        "german_credit_class = pd.DataFrame(german_credit_dataset[0]).iloc[:,-1]\n",
        "german_credit_data = pd.DataFrame(german_credit_dataset[0]).iloc[:, :-1]\n",
        "\n",
        "german_credit_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>6.0</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>1169.0</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>67.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b'A12'</td>\n",
              "      <td>48.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>5951.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A92'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>22.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b'A14'</td>\n",
              "      <td>12.0</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A46'</td>\n",
              "      <td>2096.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A74'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>3.0</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>49.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A172'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>42.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A42'</td>\n",
              "      <td>7882.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A74'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A103'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A122'</td>\n",
              "      <td>45.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A153'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>24.0</td>\n",
              "      <td>b'A33'</td>\n",
              "      <td>b'A40'</td>\n",
              "      <td>4870.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>3.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A124'</td>\n",
              "      <td>53.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A153'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>b'A14'</td>\n",
              "      <td>12.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A42'</td>\n",
              "      <td>1736.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A74'</td>\n",
              "      <td>3.0</td>\n",
              "      <td>b'A92'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>31.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A172'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>30.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A41'</td>\n",
              "      <td>3857.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A91'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A122'</td>\n",
              "      <td>40.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A174'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>b'A14'</td>\n",
              "      <td>12.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>804.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A123'</td>\n",
              "      <td>38.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>45.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>1845.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A124'</td>\n",
              "      <td>23.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A153'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>b'A12'</td>\n",
              "      <td>45.0</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A41'</td>\n",
              "      <td>4576.0</td>\n",
              "      <td>b'A62'</td>\n",
              "      <td>b'A71'</td>\n",
              "      <td>3.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A123'</td>\n",
              "      <td>27.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          1     2       3       4       5  ...   16       17   18       19       20\n",
              "0    b'A11'   6.0  b'A34'  b'A43'  1169.0  ...  2.0  b'A173'  1.0  b'A192'  b'A201'\n",
              "1    b'A12'  48.0  b'A32'  b'A43'  5951.0  ...  1.0  b'A173'  1.0  b'A191'  b'A201'\n",
              "2    b'A14'  12.0  b'A34'  b'A46'  2096.0  ...  1.0  b'A172'  2.0  b'A191'  b'A201'\n",
              "3    b'A11'  42.0  b'A32'  b'A42'  7882.0  ...  1.0  b'A173'  2.0  b'A191'  b'A201'\n",
              "4    b'A11'  24.0  b'A33'  b'A40'  4870.0  ...  2.0  b'A173'  2.0  b'A191'  b'A201'\n",
              "..      ...   ...     ...     ...     ...  ...  ...      ...  ...      ...      ...\n",
              "995  b'A14'  12.0  b'A32'  b'A42'  1736.0  ...  1.0  b'A172'  1.0  b'A191'  b'A201'\n",
              "996  b'A11'  30.0  b'A32'  b'A41'  3857.0  ...  1.0  b'A174'  1.0  b'A192'  b'A201'\n",
              "997  b'A14'  12.0  b'A32'  b'A43'   804.0  ...  1.0  b'A173'  1.0  b'A191'  b'A201'\n",
              "998  b'A11'  45.0  b'A32'  b'A43'  1845.0  ...  1.0  b'A173'  1.0  b'A192'  b'A201'\n",
              "999  b'A12'  45.0  b'A34'  b'A41'  4576.0  ...  1.0  b'A173'  1.0  b'A191'  b'A201'\n",
              "\n",
              "[1000 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI8MBmyZ8dUx",
        "outputId": "427c50c3-34a2-4372-e2de-1a460d8c54c4"
      },
      "source": [
        "cat_col_numbers=[0, 2, 3, 5, 6, 8, 9, 11, 13,14,16,18,19]\n",
        "german_data_encoded, bins_dict, group_names_dict = encode_cat_features(german_credit_data, cat_col_numbers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNFkKcjl_j-s",
        "outputId": "793b4cd7-3ca0-44a3-d57c-78d27effb8ba"
      },
      "source": [
        "german_data_encoded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.125</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1169.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.375</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>5951.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.167</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.625</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.15</td>\n",
              "      <td>2096.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.125</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.25</td>\n",
              "      <td>7882.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.125</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.35</td>\n",
              "      <td>4870.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>53.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>0.625</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1736.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0.125</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.45</td>\n",
              "      <td>3857.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0.625</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>804.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.875</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0.125</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1845.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0.375</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.45</td>\n",
              "      <td>4576.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.875</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1     2    3     4       5    6  ...     15   16     17   18    19    20\n",
              "0    0.125   6.0  0.1  0.05  1169.0  0.1  ...  0.167  2.0  0.125  1.0  0.25  0.25\n",
              "1    0.375  48.0  0.3  0.05  5951.0  0.3  ...  0.167  1.0  0.125  1.0  0.75  0.25\n",
              "2    0.625  12.0  0.1  0.15  2096.0  0.3  ...  0.167  1.0  0.375  2.0  0.75  0.25\n",
              "3    0.125  42.0  0.3  0.25  7882.0  0.3  ...    0.5  1.0  0.125  2.0  0.75  0.25\n",
              "4    0.125  24.0  0.5  0.35  4870.0  0.3  ...    0.5  2.0  0.125  2.0  0.75  0.25\n",
              "..     ...   ...  ...   ...     ...  ...  ...    ...  ...    ...  ...   ...   ...\n",
              "995  0.625  12.0  0.3  0.25  1736.0  0.3  ...  0.167  1.0  0.375  1.0  0.75  0.25\n",
              "996  0.125  30.0  0.3  0.45  3857.0  0.3  ...  0.167  1.0  0.625  1.0  0.25  0.25\n",
              "997  0.625  12.0  0.3  0.05   804.0  0.3  ...  0.167  1.0  0.125  1.0  0.75  0.25\n",
              "998  0.125  45.0  0.3  0.05  1845.0  0.3  ...    0.5  1.0  0.125  1.0  0.25  0.25\n",
              "999  0.375  45.0  0.1  0.45  4576.0  0.9  ...  0.167  1.0  0.125  1.0  0.75  0.25\n",
              "\n",
              "[1000 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SvaH5Gn4V61"
      },
      "source": [
        "scale_cols = [1, 4, 7, 10, 12, 15, 17]\n",
        "\n",
        "def scale_german_credit_data(german_data, scale_cols = scale_cols):\n",
        "  # scales the data with minmax scale\n",
        "  german_data_scaler = MinMaxScaler()\n",
        "  german_data = german_data.copy()\n",
        "  german_data.iloc[:, scale_cols] = german_data_scaler.fit_transform(german_data.iloc[:, scale_cols])\n",
        "\n",
        "  return german_data, german_data_scaler\n",
        "\n",
        "def descale_german_credit_data(german_data_scaled, german_data_scaler, scale_cols=scale_cols):\n",
        "  # inverse transforms the data\n",
        "  german_data_scaled = german_data_scaled.copy()\n",
        "  german_data_scaled.iloc[:, scale_cols] = german_data_scaler.inverse_transform(german_data_scaled.iloc[:, scale_cols])\n",
        "  return german_data_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwcSJGO_B_uS"
      },
      "source": [
        "german_data_scaled, german_data_scaler = scale_german_credit_data(german_data_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8la09FDyDVJE"
      },
      "source": [
        "german_data_descaled = descale_german_credit_data(german_data_scaled, german_data_scaler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3dOqF1DDc5w",
        "outputId": "5cdef967-be48-453d-e12b-b62de6af8380"
      },
      "source": [
        "german_data_descaled"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.125</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1169.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.375</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>5951.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.167</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.625</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.15</td>\n",
              "      <td>2096.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.125</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.25</td>\n",
              "      <td>7882.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.125</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.35</td>\n",
              "      <td>4870.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>53.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>0.625</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1736.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0.125</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.45</td>\n",
              "      <td>3857.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0.625</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>804.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.875</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0.125</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1845.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0.375</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.45</td>\n",
              "      <td>4576.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.875</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1     2    3     4       5    6  ...     15   16     17   18    19    20\n",
              "0    0.125   6.0  0.1  0.05  1169.0  0.1  ...  0.167  2.0  0.125  1.0  0.25  0.25\n",
              "1    0.375  48.0  0.3  0.05  5951.0  0.3  ...  0.167  1.0  0.125  1.0  0.75  0.25\n",
              "2    0.625  12.0  0.1  0.15  2096.0  0.3  ...  0.167  1.0  0.375  2.0  0.75  0.25\n",
              "3    0.125  42.0  0.3  0.25  7882.0  0.3  ...    0.5  1.0  0.125  2.0  0.75  0.25\n",
              "4    0.125  24.0  0.5  0.35  4870.0  0.3  ...    0.5  2.0  0.125  2.0  0.75  0.25\n",
              "..     ...   ...  ...   ...     ...  ...  ...    ...  ...    ...  ...   ...   ...\n",
              "995  0.625  12.0  0.3  0.25  1736.0  0.3  ...  0.167  1.0  0.375  1.0  0.75  0.25\n",
              "996  0.125  30.0  0.3  0.45  3857.0  0.3  ...  0.167  1.0  0.625  1.0  0.25  0.25\n",
              "997  0.625  12.0  0.3  0.05   804.0  0.3  ...  0.167  1.0  0.125  1.0  0.75  0.25\n",
              "998  0.125  45.0  0.3  0.05  1845.0  0.3  ...    0.5  1.0  0.125  1.0  0.25  0.25\n",
              "999  0.375  45.0  0.1  0.45  4576.0  0.9  ...  0.167  1.0  0.125  1.0  0.75  0.25\n",
              "\n",
              "[1000 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thTX8UfsCaXp",
        "outputId": "001a2749-9609-4cc5-d923-90e88fc347fa"
      },
      "source": [
        "german_data_scaled"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.029412</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.050567</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.375</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.313690</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.053571</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.625</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.101574</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.535714</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.419941</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.464286</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.254209</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.607143</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>0.625</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.081765</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.382353</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.198470</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0.625</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.030483</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.339286</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.602941</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.087763</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0.375</td>\n",
              "      <td>0.602941</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.238032</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1         2    3     4         5  ...        16     17   18    19    20\n",
              "0    0.125  0.029412  0.1  0.05  0.050567  ...  0.333333  0.125  0.0  0.25  0.25\n",
              "1    0.375  0.647059  0.3  0.05  0.313690  ...  0.000000  0.125  0.0  0.75  0.25\n",
              "2    0.625  0.117647  0.1  0.15  0.101574  ...  0.000000  0.375  1.0  0.75  0.25\n",
              "3    0.125  0.558824  0.3  0.25  0.419941  ...  0.000000  0.125  1.0  0.75  0.25\n",
              "4    0.125  0.294118  0.5  0.35  0.254209  ...  0.333333  0.125  1.0  0.75  0.25\n",
              "..     ...       ...  ...   ...       ...  ...       ...    ...  ...   ...   ...\n",
              "995  0.625  0.117647  0.3  0.25  0.081765  ...  0.000000  0.375  0.0  0.75  0.25\n",
              "996  0.125  0.382353  0.3  0.45  0.198470  ...  0.000000  0.625  0.0  0.25  0.25\n",
              "997  0.625  0.117647  0.3  0.05  0.030483  ...  0.000000  0.125  0.0  0.75  0.25\n",
              "998  0.125  0.602941  0.3  0.05  0.087763  ...  0.000000  0.125  0.0  0.25  0.25\n",
              "999  0.375  0.602941  0.1  0.45  0.238032  ...  0.000000  0.125  0.0  0.75  0.25\n",
              "\n",
              "[1000 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGqsD1vR69PO",
        "outputId": "1f736d59-d410-4d5d-ac76-7b4fcb3bdd80"
      },
      "source": [
        "german_data_class = german_credit_class\n",
        "german_data_class[german_data_class == b'1'] = 0\n",
        "german_data_class[german_data_class == b'2'] = 1\n",
        "german_data_class"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      1\n",
              "2      0\n",
              "3      0\n",
              "4      1\n",
              "      ..\n",
              "995    0\n",
              "996    0\n",
              "997    0\n",
              "998    1\n",
              "999    0\n",
              "Name: 21, Length: 1000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHk1YNpUdmRf",
        "outputId": "e4187d8c-ded2-4f7a-82ae-abb7f17ef219"
      },
      "source": [
        "german_data_class.sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8jFPq9AYAjK"
      },
      "source": [
        "##BB GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZAxVwEqMI3_"
      },
      "source": [
        "###BB GAN Components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwIMT8Z6MhpL"
      },
      "source": [
        "class BBGAN():\n",
        "  def __init__(self, data_dim, noise_dim):\n",
        "    self.data_dim = data_dim\n",
        "    self.D = None\n",
        "    self.G = None\n",
        "    self.AM = None\n",
        "    self.DM = None\n",
        "\n",
        "    self.noise_dim = noise_dim\n",
        "\n",
        "  def discriminator(self, dropout = 0.5):\n",
        "    # the discriminator here will be trained on a data dimension of data_dim + 2, where the y will be probability that the last vector component holds the true probability (from the BB model)\n",
        "    if self.D:\n",
        "      return self.D\n",
        "\n",
        "    sample_input = Input((self.data_dim,))\n",
        "    c_input = Input((1,))\n",
        "    y_input = Input((1,))\n",
        "\n",
        "    concat_layer = Concatenate()([sample_input, c_input, y_input])\n",
        "\n",
        "    layer_1 = Dense(256 , activation='relu')(concat_layer)\n",
        "    layer_1_dropout = Dropout(dropout)(layer_1)\n",
        "\n",
        "    layer_2 = Dense(256 , activation='relu')(layer_1_dropout)\n",
        "    layer_2_dropout = Dropout(dropout)(layer_2)\n",
        "\n",
        "    layer_3 = Dense(128 , activation='relu')(layer_2_dropout)\n",
        "    layer_3_dropout = Dropout(dropout)(layer_3)\n",
        "\n",
        "    output = Dense(1, activation = 'sigmoid')(layer_3_dropout)\n",
        "\n",
        "    self.D = Model(inputs = [sample_input, c_input, y_input], outputs = output)\n",
        "\n",
        "    print(\"Discriminator summary\")\n",
        "    self.D.summary()\n",
        "\n",
        "    return self.D\n",
        "\n",
        "\n",
        "  def generator(self, dropout=0.5):\n",
        "    # gets the y input but does nothing with it (just ouputs it)\n",
        "\n",
        "    if self.G:\n",
        "      return self.G\n",
        "\n",
        "    z_input = Input((self.noise_dim,))\n",
        "    c_input = Input((1,))\n",
        "\n",
        "    concat_layer = Concatenate()([z_input, c_input])\n",
        "\n",
        "    layer_1 = Dense(128 , activation='relu')(concat_layer)\n",
        "    layer_1_dropout = Dropout(dropout)(layer_1)\n",
        "\n",
        "    layer_2 = Dense(256 , activation='relu')(layer_1_dropout)\n",
        "    layer_2_dropout = Dropout(dropout)(layer_2)\n",
        "\n",
        "    layer_3 = Dense(256 , activation='relu')(layer_2_dropout)\n",
        "    layer_3_dropout = Dropout(dropout)(layer_3)\n",
        "\n",
        "    output = Dense(self.data_dim, activation = 'sigmoid', name=\"samples_generator_layer\")(layer_3_dropout)\n",
        "\n",
        "    self.G = Model(inputs=[z_input, c_input], outputs=output)\n",
        "\n",
        "    print(\"Generator summary\")\n",
        "    self.G.summary()\n",
        "\n",
        "    return self.G\n",
        "\n",
        "  def discriminator_model(self):\n",
        "    # creates the discriminator model and compiles it\n",
        "    if self.DM:\n",
        "      return self.DM\n",
        "\n",
        "    optimizer = Adam()\n",
        "    self.DM = self.discriminator()\n",
        "    self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
        "                    metrics=['accuracy'])\n",
        "    return self.DM\n",
        "\n",
        "\n",
        "  def adversarial_model(self):\n",
        "    # creates the adversarial model and compiles it\n",
        "    if self.AM:\n",
        "      return self.AM\n",
        "\n",
        "    optimizer = Adam()\n",
        "\n",
        "    # Adverserial Model\n",
        "    z_input = Input((self.noise_dim,))\n",
        "    c_input = Input((1,))\n",
        "    y_input = Input((1,))\n",
        "    \n",
        "    # Generator\n",
        "    generator = self.generator()([z_input, c_input])\n",
        "    discriminator = self.discriminator()([generator, c_input, y_input])\n",
        "\n",
        "    self.AM = Model(inputs = [z_input, c_input, y_input], outputs = discriminator)\n",
        "\n",
        "    print(\"Adversarial model summary:\")\n",
        "    self.AM.summary()\n",
        "\n",
        "    self.AM.compile(loss='binary_crossentropy', optimizer = optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "    return self.AM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezNAECdwP60H"
      },
      "source": [
        "class IMPLEMENTED_BBGAN(object):\n",
        "  def __init__(self, bbmodel, data_dim, noise_dim = 2):\n",
        "    self.data_dim = data_dim\n",
        "\n",
        "    self.noise_dim = noise_dim\n",
        "\n",
        "    self.BBGAN = BBGAN(data_dim=self.data_dim, noise_dim = self.noise_dim)\n",
        "    self.discriminator =  self.BBGAN.discriminator_model()\n",
        "    self.adversarial = self.BBGAN.adversarial_model()\n",
        "    self.generator = self.BBGAN.generator()\n",
        "\n",
        "    self.bbmodel = bbmodel\n",
        "\n",
        "\n",
        "  def train(self, train_steps=300, batch_size=256):\n",
        "    # trains the model\n",
        "\n",
        "    discriminator_accuracies = []\n",
        "    adversarial_accuracies = []\n",
        "\n",
        "    discriminator_losses = []\n",
        "    adversarial_losses = []\n",
        "\n",
        "    for i in range(train_steps):\n",
        "      z = np.random.uniform(-1.0, 1.0, size=[batch_size, self.noise_dim])\n",
        "      c = np.random.uniform(0.0, 1.0, size=[batch_size, 1]) #generated fake confidence\n",
        "\n",
        "      samples_fake = self.generator([z, c]).numpy()\n",
        "\n",
        "      y = self.bbmodel.predict_proba(samples_fake)[:,1]\n",
        "\n",
        "      y_hat = np.ones([batch_size, 1]) # 1 if y has the real value, 0 otherwise\n",
        "\n",
        "      for ind in range(y_hat.shape[0]):\n",
        "        rand = random.random()\n",
        "        # in a probability of 50% - shuffle the components\n",
        "        if rand >= 0.5:\n",
        "          temp = c[ind]\n",
        "          c[ind] = y[ind]\n",
        "          y[ind] = temp\n",
        "          y_hat[ind] = 0\n",
        "\n",
        "      d_loss = self.discriminator.train_on_batch([samples_fake, c, y], y_hat)\n",
        "\n",
        "      # we need to fool the discriminator by the samples that are generated by the generator, thus we will switch the labels on y_hat (that they will be the opposite)\n",
        "      for ind_fool in range(len(y_hat)):\n",
        "        if y_hat[ind_fool] == 0:\n",
        "          y_hat[ind_fool] = 1\n",
        "        else:\n",
        "          y_hat[ind_fool] = 0\n",
        "\n",
        "      # works better when not freezing the discriminator\n",
        "      # freeze the discriminator\n",
        "      self.discriminator.trainable = False\n",
        "      # train the generator on the fooling data\n",
        "      a_loss = self.adversarial.train_on_batch([z, c, y], y_hat)\n",
        "      # un-freeze the discriminator\n",
        "      self.discriminator.trainable = True\n",
        "\n",
        "      log_mesg = \"%d: [Discriminator loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
        "      log_mesg = \"%s  [Adversarial loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
        "      print(log_mesg)\n",
        "\n",
        "      discriminator_acc = d_loss[1]\n",
        "      adversarial_acc = a_loss[1]\n",
        "      discriminator_loss = d_loss[0]\n",
        "      adversarial_loss = a_loss[0]\n",
        "\n",
        "      discriminator_accuracies.append(discriminator_acc)\n",
        "      adversarial_accuracies.append(adversarial_acc)\n",
        "      discriminator_losses.append(discriminator_loss)\n",
        "      adversarial_losses.append(adversarial_loss)\n",
        "      \n",
        "    return discriminator_losses, discriminator_accuracies, adversarial_losses, adversarial_accuracies          \n",
        "\n",
        "  def generate_samples(self, num_samples = 100):\n",
        "    noise = np.random.uniform(-1.0, 1.0, size=[num_samples, self.BBGAN.noise_dim])\n",
        "    c = np.random.uniform(0.0, 1.0, size=[num_samples, 1]) #generated fake confidence\n",
        "    generated_samples = self.generator.predict([noise,c])\n",
        "    return generated_samples, c\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9xmDRn59fMC"
      },
      "source": [
        "##BB Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fok-gp11rlCC"
      },
      "source": [
        "###Diabetes Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBTaJXz3rwYq"
      },
      "source": [
        "####Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_ZvqOd99iTT",
        "outputId": "70377b7d-3888-458f-f411-53f321867fe9"
      },
      "source": [
        "diabetes_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preg</th>\n",
              "      <th>plas</th>\n",
              "      <th>pres</th>\n",
              "      <th>skin</th>\n",
              "      <th>insu</th>\n",
              "      <th>mass</th>\n",
              "      <th>pedi</th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     preg   plas  pres  skin   insu  mass   pedi   age\n",
              "0     6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0\n",
              "1     1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0\n",
              "2     8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0\n",
              "3     1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0\n",
              "4     0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0\n",
              "..    ...    ...   ...   ...    ...   ...    ...   ...\n",
              "763  10.0  101.0  76.0  48.0  180.0  32.9  0.171  63.0\n",
              "764   2.0  122.0  70.0  27.0    0.0  36.8  0.340  27.0\n",
              "765   5.0  121.0  72.0  23.0  112.0  26.2  0.245  30.0\n",
              "766   1.0  126.0  60.0   0.0    0.0  30.1  0.349  47.0\n",
              "767   1.0   93.0  70.0  31.0    0.0  30.4  0.315  23.0\n",
              "\n",
              "[768 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcWh6Y9X9qOI"
      },
      "source": [
        "X = diabetes_data\n",
        "y = diabetes_class.astype('int')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHQxbeC5-QIq"
      },
      "source": [
        "####Diabetes Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdWIYYya-M01",
        "outputId": "6a9f4c0c-6107-413d-c733-336eb9b3857b"
      },
      "source": [
        "diabetes_rf_clf = RandomForestClassifier()\n",
        "diabetes_rf_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVN-JAM3_dJy",
        "outputId": "4e3e2e42-24aa-4904-f0a9-ffbaed6b6e45"
      },
      "source": [
        "diabetes_confidence = diabetes_rf_clf.predict_proba(X_test)[:, 1]\n",
        "print(diabetes_confidence)\n",
        "print(f\"max confidence score: {max(diabetes_confidence)}\")\n",
        "print(f\"min confidence score: {min(diabetes_confidence)}\")\n",
        "print(f\"avg confidence score: {np.mean(diabetes_confidence)}\")\n",
        "diabetes_pred = diabetes_rf_clf.predict(X_test)\n",
        "print(f\"diabetes rf classifier accuracy: {accuracy_score(y_test, diabetes_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.5  0.18 0.17 0.21 0.46 0.51 0.   0.61 0.62 0.68 0.25 0.84 0.29 0.34\n",
            " 0.   0.45 0.08 0.02 0.66 0.54 0.35 0.03 0.25 0.02 0.64 0.91 0.06 0.\n",
            " 0.19 0.2  0.77 0.81 0.75 0.75 0.54 0.8  0.81 0.35 0.13 0.65 0.03 0.29\n",
            " 0.57 0.52 0.05 0.71 0.54 0.14 0.14 0.78 0.   0.77 0.79 0.31 0.06 0.\n",
            " 0.66 0.03 0.25 0.83 0.69 0.19 0.31 0.41 0.06 0.71 0.   0.54 0.06 0.8\n",
            " 0.66 0.16 0.09 0.11 0.13 0.4  0.19 0.11 0.15 0.2  0.9  0.14 0.09 0.56\n",
            " 0.22 0.96 0.67 0.43 0.27 0.05 0.02 0.19 0.01 0.54 0.42 0.59 0.64 0.08\n",
            " 0.7  0.06 0.84 0.04 0.52 0.61 0.86 0.17 0.25 0.91 0.14 0.69 0.03 0.44\n",
            " 0.19 0.81 0.24 0.46 0.71 0.29 0.01 0.53 0.01 0.14 0.3  0.07 0.28 0.44\n",
            " 0.19 0.79 0.71 0.62 0.57 0.71 0.04 0.3  0.77 0.43 0.26 0.78 0.79 0.02\n",
            " 0.01 0.   0.24 0.42 0.16 0.35 0.18 0.   0.3  0.88 0.14 0.32 0.57 0.42\n",
            " 0.02 0.49 0.23 0.44 0.5  0.11 0.42 0.63 0.23 0.02 0.13 0.86 0.03 0.27\n",
            " 0.82 0.65 0.62 0.07 0.58 0.49 0.78 0.11 0.43 0.56 0.41 0.53 0.36 0.37\n",
            " 0.39 0.71 0.84 0.01 0.1  0.05 0.54 0.52 0.04 0.04 0.72 0.18 0.19 0.1\n",
            " 0.02 0.02 0.16 0.82 0.38 0.08 0.33 0.26 0.8  0.14 0.05 0.31 0.85 0.35\n",
            " 0.44 0.15 0.04 0.13 0.47 0.04 0.54 0.6  0.34 0.89 0.69 0.33 0.   0.2\n",
            " 0.02 0.72 0.38 0.36 0.34 0.29 0.13]\n",
            "max confidence score: 0.96\n",
            "min confidence score: 0.0\n",
            "avg confidence score: 0.37060606060606066\n",
            "diabetes rf classifier accuracy: 0.7575757575757576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0ailK52-yQF",
        "outputId": "64492560-11b5-4761-aaec-99c713226b74"
      },
      "source": [
        "plt.hist(diabetes_confidence, rwidth=0.8)\n",
        "plt.xlabel(\"Confidence\")\n",
        "plt.ylabel(\"Number of cases\")\n",
        "plt.title(\"Confidence scores distribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj913KM-P2la"
      },
      "source": [
        "###Diabetes BBModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Siimu4-UkyyP",
        "outputId": "85342ddb-324a-4b90-abcc-74fae34622a2"
      },
      "source": [
        "diabetes_bbgan = IMPLEMENTED_BBGAN(diabetes_rf_clf, diabetes_data.shape[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator summary\n",
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_36 (InputLayer)           [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_37 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_38 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 10)           0           input_36[0][0]                   \n",
            "                                                                 input_37[0][0]                   \n",
            "                                                                 input_38[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_37 (Dense)                (None, 256)          2816        concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 256)          0           dense_37[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_38 (Dense)                (None, 256)          65792       dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 256)          0           dense_38[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_39 (Dense)                (None, 128)          32896       dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 128)          0           dense_39[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_40 (Dense)                (None, 1)            129         dropout_33[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 101,633\n",
            "Trainable params: 101,633\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Generator summary\n",
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_42 (InputLayer)           [(None, 2)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_43 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 3)            0           input_42[0][0]                   \n",
            "                                                                 input_43[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_41 (Dense)                (None, 128)          512         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 128)          0           dense_41[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_42 (Dense)                (None, 256)          33024       dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 256)          0           dense_42[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_43 (Dense)                (None, 256)          65792       dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 256)          0           dense_43[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "samples_generator_layer (Dense) (None, 8)            2056        dropout_36[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 101,384\n",
            "Trainable params: 101,384\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Adversarial model summary:\n",
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_39 (InputLayer)           [(None, 2)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_40 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_13 (Functional)           (None, 8)            101384      input_39[0][0]                   \n",
            "                                                                 input_40[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_41 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_12 (Functional)           (None, 1)            101633      model_13[0][0]                   \n",
            "                                                                 input_40[0][0]                   \n",
            "                                                                 input_41[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 203,017\n",
            "Trainable params: 203,017\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apfv2QAxo7Sl",
        "outputId": "9debcf08-66d1-4f59-beb7-5cb1c9f51d02"
      },
      "source": [
        "discriminator_losses, discriminator_accuracies, adversarial_losses, adversarial_accuracies = diabetes_bbgan.train(train_steps=1500, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: [Discriminator loss: 0.686056, acc: 0.562500]  [Adversarial loss: 0.735293, acc: 0.378906]\n",
            "1: [Discriminator loss: 0.697526, acc: 0.507812]  [Adversarial loss: 0.699257, acc: 0.496094]\n",
            "2: [Discriminator loss: 0.695965, acc: 0.500000]  [Adversarial loss: 0.708739, acc: 0.464844]\n",
            "3: [Discriminator loss: 0.694597, acc: 0.480469]  [Adversarial loss: 0.702925, acc: 0.480469]\n",
            "4: [Discriminator loss: 0.694267, acc: 0.472656]  [Adversarial loss: 0.704486, acc: 0.472656]\n",
            "5: [Discriminator loss: 0.682663, acc: 0.589844]  [Adversarial loss: 0.700775, acc: 0.468750]\n",
            "6: [Discriminator loss: 0.699483, acc: 0.484375]  [Adversarial loss: 0.709176, acc: 0.472656]\n",
            "7: [Discriminator loss: 0.689900, acc: 0.531250]  [Adversarial loss: 0.703457, acc: 0.488281]\n",
            "8: [Discriminator loss: 0.690366, acc: 0.511719]  [Adversarial loss: 0.700440, acc: 0.457031]\n",
            "9: [Discriminator loss: 0.687216, acc: 0.539062]  [Adversarial loss: 0.699426, acc: 0.468750]\n",
            "10: [Discriminator loss: 0.690706, acc: 0.507812]  [Adversarial loss: 0.704879, acc: 0.464844]\n",
            "11: [Discriminator loss: 0.685176, acc: 0.558594]  [Adversarial loss: 0.705499, acc: 0.398438]\n",
            "12: [Discriminator loss: 0.690412, acc: 0.511719]  [Adversarial loss: 0.707265, acc: 0.464844]\n",
            "13: [Discriminator loss: 0.688626, acc: 0.542969]  [Adversarial loss: 0.698327, acc: 0.472656]\n",
            "14: [Discriminator loss: 0.687801, acc: 0.550781]  [Adversarial loss: 0.706662, acc: 0.464844]\n",
            "15: [Discriminator loss: 0.693052, acc: 0.511719]  [Adversarial loss: 0.695968, acc: 0.484375]\n",
            "16: [Discriminator loss: 0.695728, acc: 0.472656]  [Adversarial loss: 0.702868, acc: 0.449219]\n",
            "17: [Discriminator loss: 0.694637, acc: 0.511719]  [Adversarial loss: 0.709574, acc: 0.437500]\n",
            "18: [Discriminator loss: 0.687061, acc: 0.566406]  [Adversarial loss: 0.693191, acc: 0.523438]\n",
            "19: [Discriminator loss: 0.690658, acc: 0.503906]  [Adversarial loss: 0.701800, acc: 0.472656]\n",
            "20: [Discriminator loss: 0.695698, acc: 0.539062]  [Adversarial loss: 0.702181, acc: 0.421875]\n",
            "21: [Discriminator loss: 0.700427, acc: 0.445312]  [Adversarial loss: 0.704288, acc: 0.441406]\n",
            "22: [Discriminator loss: 0.692086, acc: 0.539062]  [Adversarial loss: 0.702618, acc: 0.417969]\n",
            "23: [Discriminator loss: 0.695101, acc: 0.492188]  [Adversarial loss: 0.700234, acc: 0.457031]\n",
            "24: [Discriminator loss: 0.693669, acc: 0.500000]  [Adversarial loss: 0.695468, acc: 0.484375]\n",
            "25: [Discriminator loss: 0.690781, acc: 0.546875]  [Adversarial loss: 0.700713, acc: 0.449219]\n",
            "26: [Discriminator loss: 0.694638, acc: 0.519531]  [Adversarial loss: 0.697631, acc: 0.472656]\n",
            "27: [Discriminator loss: 0.689020, acc: 0.527344]  [Adversarial loss: 0.699046, acc: 0.425781]\n",
            "28: [Discriminator loss: 0.692431, acc: 0.519531]  [Adversarial loss: 0.697585, acc: 0.488281]\n",
            "29: [Discriminator loss: 0.695523, acc: 0.492188]  [Adversarial loss: 0.698517, acc: 0.476562]\n",
            "30: [Discriminator loss: 0.690898, acc: 0.519531]  [Adversarial loss: 0.701215, acc: 0.453125]\n",
            "31: [Discriminator loss: 0.695754, acc: 0.480469]  [Adversarial loss: 0.695099, acc: 0.519531]\n",
            "32: [Discriminator loss: 0.691230, acc: 0.523438]  [Adversarial loss: 0.699567, acc: 0.496094]\n",
            "33: [Discriminator loss: 0.691045, acc: 0.554688]  [Adversarial loss: 0.700372, acc: 0.453125]\n",
            "34: [Discriminator loss: 0.690416, acc: 0.507812]  [Adversarial loss: 0.700430, acc: 0.414062]\n",
            "35: [Discriminator loss: 0.691271, acc: 0.546875]  [Adversarial loss: 0.700758, acc: 0.425781]\n",
            "36: [Discriminator loss: 0.694564, acc: 0.492188]  [Adversarial loss: 0.694365, acc: 0.476562]\n",
            "37: [Discriminator loss: 0.689912, acc: 0.527344]  [Adversarial loss: 0.699542, acc: 0.476562]\n",
            "38: [Discriminator loss: 0.692800, acc: 0.523438]  [Adversarial loss: 0.694624, acc: 0.511719]\n",
            "39: [Discriminator loss: 0.692290, acc: 0.535156]  [Adversarial loss: 0.694130, acc: 0.484375]\n",
            "40: [Discriminator loss: 0.692076, acc: 0.531250]  [Adversarial loss: 0.690941, acc: 0.535156]\n",
            "41: [Discriminator loss: 0.693013, acc: 0.500000]  [Adversarial loss: 0.694455, acc: 0.507812]\n",
            "42: [Discriminator loss: 0.694170, acc: 0.488281]  [Adversarial loss: 0.694278, acc: 0.515625]\n",
            "43: [Discriminator loss: 0.691217, acc: 0.503906]  [Adversarial loss: 0.696430, acc: 0.480469]\n",
            "44: [Discriminator loss: 0.690501, acc: 0.527344]  [Adversarial loss: 0.695103, acc: 0.457031]\n",
            "45: [Discriminator loss: 0.690967, acc: 0.507812]  [Adversarial loss: 0.691969, acc: 0.500000]\n",
            "46: [Discriminator loss: 0.696832, acc: 0.468750]  [Adversarial loss: 0.689075, acc: 0.558594]\n",
            "47: [Discriminator loss: 0.692459, acc: 0.519531]  [Adversarial loss: 0.687162, acc: 0.550781]\n",
            "48: [Discriminator loss: 0.691741, acc: 0.511719]  [Adversarial loss: 0.686619, acc: 0.550781]\n",
            "49: [Discriminator loss: 0.694234, acc: 0.460938]  [Adversarial loss: 0.686248, acc: 0.566406]\n",
            "50: [Discriminator loss: 0.690086, acc: 0.546875]  [Adversarial loss: 0.688710, acc: 0.539062]\n",
            "51: [Discriminator loss: 0.691318, acc: 0.503906]  [Adversarial loss: 0.679562, acc: 0.582031]\n",
            "52: [Discriminator loss: 0.689802, acc: 0.523438]  [Adversarial loss: 0.681331, acc: 0.609375]\n",
            "53: [Discriminator loss: 0.695271, acc: 0.500000]  [Adversarial loss: 0.678300, acc: 0.660156]\n",
            "54: [Discriminator loss: 0.686648, acc: 0.542969]  [Adversarial loss: 0.675347, acc: 0.625000]\n",
            "55: [Discriminator loss: 0.697781, acc: 0.507812]  [Adversarial loss: 0.662765, acc: 0.726562]\n",
            "56: [Discriminator loss: 0.690894, acc: 0.507812]  [Adversarial loss: 0.655856, acc: 0.730469]\n",
            "57: [Discriminator loss: 0.682577, acc: 0.523438]  [Adversarial loss: 0.653909, acc: 0.714844]\n",
            "58: [Discriminator loss: 0.688599, acc: 0.503906]  [Adversarial loss: 0.639791, acc: 0.761719]\n",
            "59: [Discriminator loss: 0.709433, acc: 0.464844]  [Adversarial loss: 0.621706, acc: 0.812500]\n",
            "60: [Discriminator loss: 0.690706, acc: 0.531250]  [Adversarial loss: 0.608970, acc: 0.785156]\n",
            "61: [Discriminator loss: 0.696880, acc: 0.535156]  [Adversarial loss: 0.587298, acc: 0.824219]\n",
            "62: [Discriminator loss: 0.705597, acc: 0.492188]  [Adversarial loss: 0.565629, acc: 0.847656]\n",
            "63: [Discriminator loss: 0.716861, acc: 0.503906]  [Adversarial loss: 0.548413, acc: 0.835938]\n",
            "64: [Discriminator loss: 0.724733, acc: 0.523438]  [Adversarial loss: 0.546691, acc: 0.832031]\n",
            "65: [Discriminator loss: 0.726812, acc: 0.546875]  [Adversarial loss: 0.495932, acc: 0.882812]\n",
            "66: [Discriminator loss: 0.770902, acc: 0.492188]  [Adversarial loss: 0.506318, acc: 0.871094]\n",
            "67: [Discriminator loss: 0.739643, acc: 0.546875]  [Adversarial loss: 0.499806, acc: 0.882812]\n",
            "68: [Discriminator loss: 0.798342, acc: 0.484375]  [Adversarial loss: 0.474684, acc: 0.867188]\n",
            "69: [Discriminator loss: 0.872694, acc: 0.421875]  [Adversarial loss: 0.457936, acc: 0.898438]\n",
            "70: [Discriminator loss: 0.782055, acc: 0.500000]  [Adversarial loss: 0.473851, acc: 0.851562]\n",
            "71: [Discriminator loss: 0.869125, acc: 0.460938]  [Adversarial loss: 0.469068, acc: 0.898438]\n",
            "72: [Discriminator loss: 0.791215, acc: 0.511719]  [Adversarial loss: 0.505899, acc: 0.820312]\n",
            "73: [Discriminator loss: 0.834752, acc: 0.488281]  [Adversarial loss: 0.423509, acc: 0.917969]\n",
            "74: [Discriminator loss: 0.866803, acc: 0.453125]  [Adversarial loss: 0.442552, acc: 0.902344]\n",
            "75: [Discriminator loss: 0.849388, acc: 0.468750]  [Adversarial loss: 0.472567, acc: 0.867188]\n",
            "76: [Discriminator loss: 0.820184, acc: 0.507812]  [Adversarial loss: 0.501017, acc: 0.843750]\n",
            "77: [Discriminator loss: 0.832661, acc: 0.472656]  [Adversarial loss: 0.452094, acc: 0.886719]\n",
            "78: [Discriminator loss: 0.767080, acc: 0.531250]  [Adversarial loss: 0.466883, acc: 0.878906]\n",
            "79: [Discriminator loss: 0.804789, acc: 0.492188]  [Adversarial loss: 0.473002, acc: 0.882812]\n",
            "80: [Discriminator loss: 0.762263, acc: 0.531250]  [Adversarial loss: 0.479694, acc: 0.863281]\n",
            "81: [Discriminator loss: 0.804857, acc: 0.472656]  [Adversarial loss: 0.483997, acc: 0.867188]\n",
            "82: [Discriminator loss: 0.758856, acc: 0.503906]  [Adversarial loss: 0.464184, acc: 0.882812]\n",
            "83: [Discriminator loss: 0.794156, acc: 0.488281]  [Adversarial loss: 0.479986, acc: 0.906250]\n",
            "84: [Discriminator loss: 0.772940, acc: 0.492188]  [Adversarial loss: 0.445132, acc: 0.910156]\n",
            "85: [Discriminator loss: 0.776407, acc: 0.480469]  [Adversarial loss: 0.477669, acc: 0.886719]\n",
            "86: [Discriminator loss: 0.701802, acc: 0.570312]  [Adversarial loss: 0.489579, acc: 0.871094]\n",
            "87: [Discriminator loss: 0.750064, acc: 0.531250]  [Adversarial loss: 0.447311, acc: 0.906250]\n",
            "88: [Discriminator loss: 0.755920, acc: 0.527344]  [Adversarial loss: 0.427277, acc: 0.929688]\n",
            "89: [Discriminator loss: 0.759773, acc: 0.507812]  [Adversarial loss: 0.450120, acc: 0.902344]\n",
            "90: [Discriminator loss: 0.772959, acc: 0.507812]  [Adversarial loss: 0.460641, acc: 0.886719]\n",
            "91: [Discriminator loss: 0.840177, acc: 0.453125]  [Adversarial loss: 0.457552, acc: 0.902344]\n",
            "92: [Discriminator loss: 0.730132, acc: 0.566406]  [Adversarial loss: 0.424312, acc: 0.914062]\n",
            "93: [Discriminator loss: 0.712184, acc: 0.582031]  [Adversarial loss: 0.442914, acc: 0.890625]\n",
            "94: [Discriminator loss: 0.775256, acc: 0.492188]  [Adversarial loss: 0.473108, acc: 0.882812]\n",
            "95: [Discriminator loss: 0.781805, acc: 0.484375]  [Adversarial loss: 0.481739, acc: 0.875000]\n",
            "96: [Discriminator loss: 0.824337, acc: 0.492188]  [Adversarial loss: 0.431129, acc: 0.894531]\n",
            "97: [Discriminator loss: 0.793947, acc: 0.488281]  [Adversarial loss: 0.399372, acc: 0.933594]\n",
            "98: [Discriminator loss: 0.822965, acc: 0.453125]  [Adversarial loss: 0.416505, acc: 0.929688]\n",
            "99: [Discriminator loss: 0.766993, acc: 0.542969]  [Adversarial loss: 0.432441, acc: 0.906250]\n",
            "100: [Discriminator loss: 0.760345, acc: 0.503906]  [Adversarial loss: 0.447535, acc: 0.917969]\n",
            "101: [Discriminator loss: 0.785261, acc: 0.480469]  [Adversarial loss: 0.436056, acc: 0.914062]\n",
            "102: [Discriminator loss: 0.771358, acc: 0.539062]  [Adversarial loss: 0.470887, acc: 0.871094]\n",
            "103: [Discriminator loss: 0.774399, acc: 0.527344]  [Adversarial loss: 0.461522, acc: 0.875000]\n",
            "104: [Discriminator loss: 0.793493, acc: 0.500000]  [Adversarial loss: 0.464558, acc: 0.878906]\n",
            "105: [Discriminator loss: 0.783080, acc: 0.488281]  [Adversarial loss: 0.449160, acc: 0.910156]\n",
            "106: [Discriminator loss: 0.745470, acc: 0.507812]  [Adversarial loss: 0.441418, acc: 0.894531]\n",
            "107: [Discriminator loss: 0.819246, acc: 0.488281]  [Adversarial loss: 0.489517, acc: 0.886719]\n",
            "108: [Discriminator loss: 0.807232, acc: 0.500000]  [Adversarial loss: 0.472697, acc: 0.875000]\n",
            "109: [Discriminator loss: 0.753292, acc: 0.535156]  [Adversarial loss: 0.480914, acc: 0.871094]\n",
            "110: [Discriminator loss: 0.757427, acc: 0.523438]  [Adversarial loss: 0.454830, acc: 0.890625]\n",
            "111: [Discriminator loss: 0.763265, acc: 0.503906]  [Adversarial loss: 0.419272, acc: 0.929688]\n",
            "112: [Discriminator loss: 0.755385, acc: 0.503906]  [Adversarial loss: 0.457423, acc: 0.898438]\n",
            "113: [Discriminator loss: 0.753448, acc: 0.511719]  [Adversarial loss: 0.400787, acc: 0.953125]\n",
            "114: [Discriminator loss: 0.773926, acc: 0.468750]  [Adversarial loss: 0.438488, acc: 0.898438]\n",
            "115: [Discriminator loss: 0.757937, acc: 0.511719]  [Adversarial loss: 0.466929, acc: 0.894531]\n",
            "116: [Discriminator loss: 0.683377, acc: 0.566406]  [Adversarial loss: 0.457451, acc: 0.882812]\n",
            "117: [Discriminator loss: 0.804326, acc: 0.476562]  [Adversarial loss: 0.431705, acc: 0.914062]\n",
            "118: [Discriminator loss: 0.771324, acc: 0.484375]  [Adversarial loss: 0.418528, acc: 0.933594]\n",
            "119: [Discriminator loss: 0.707426, acc: 0.562500]  [Adversarial loss: 0.447319, acc: 0.906250]\n",
            "120: [Discriminator loss: 0.804880, acc: 0.492188]  [Adversarial loss: 0.409023, acc: 0.933594]\n",
            "121: [Discriminator loss: 0.777100, acc: 0.484375]  [Adversarial loss: 0.447171, acc: 0.890625]\n",
            "122: [Discriminator loss: 0.777624, acc: 0.503906]  [Adversarial loss: 0.419180, acc: 0.921875]\n",
            "123: [Discriminator loss: 0.781935, acc: 0.511719]  [Adversarial loss: 0.416911, acc: 0.914062]\n",
            "124: [Discriminator loss: 0.789865, acc: 0.511719]  [Adversarial loss: 0.418362, acc: 0.906250]\n",
            "125: [Discriminator loss: 0.772974, acc: 0.492188]  [Adversarial loss: 0.428995, acc: 0.902344]\n",
            "126: [Discriminator loss: 0.739102, acc: 0.500000]  [Adversarial loss: 0.408436, acc: 0.925781]\n",
            "127: [Discriminator loss: 0.749738, acc: 0.527344]  [Adversarial loss: 0.420732, acc: 0.910156]\n",
            "128: [Discriminator loss: 0.750390, acc: 0.523438]  [Adversarial loss: 0.438507, acc: 0.902344]\n",
            "129: [Discriminator loss: 0.728061, acc: 0.531250]  [Adversarial loss: 0.471902, acc: 0.878906]\n",
            "130: [Discriminator loss: 0.816975, acc: 0.453125]  [Adversarial loss: 0.414125, acc: 0.921875]\n",
            "131: [Discriminator loss: 0.768239, acc: 0.531250]  [Adversarial loss: 0.428871, acc: 0.906250]\n",
            "132: [Discriminator loss: 0.862466, acc: 0.433594]  [Adversarial loss: 0.424251, acc: 0.914062]\n",
            "133: [Discriminator loss: 0.778215, acc: 0.507812]  [Adversarial loss: 0.422342, acc: 0.914062]\n",
            "134: [Discriminator loss: 0.753921, acc: 0.515625]  [Adversarial loss: 0.388425, acc: 0.941406]\n",
            "135: [Discriminator loss: 0.730784, acc: 0.535156]  [Adversarial loss: 0.415695, acc: 0.921875]\n",
            "136: [Discriminator loss: 0.750467, acc: 0.515625]  [Adversarial loss: 0.426279, acc: 0.914062]\n",
            "137: [Discriminator loss: 0.783691, acc: 0.480469]  [Adversarial loss: 0.420756, acc: 0.917969]\n",
            "138: [Discriminator loss: 0.766480, acc: 0.507812]  [Adversarial loss: 0.399558, acc: 0.937500]\n",
            "139: [Discriminator loss: 0.721069, acc: 0.515625]  [Adversarial loss: 0.427118, acc: 0.902344]\n",
            "140: [Discriminator loss: 0.800122, acc: 0.476562]  [Adversarial loss: 0.423283, acc: 0.910156]\n",
            "141: [Discriminator loss: 0.756253, acc: 0.519531]  [Adversarial loss: 0.417276, acc: 0.910156]\n",
            "142: [Discriminator loss: 0.761983, acc: 0.503906]  [Adversarial loss: 0.435778, acc: 0.925781]\n",
            "143: [Discriminator loss: 0.850749, acc: 0.472656]  [Adversarial loss: 0.410979, acc: 0.914062]\n",
            "144: [Discriminator loss: 0.802623, acc: 0.472656]  [Adversarial loss: 0.422962, acc: 0.929688]\n",
            "145: [Discriminator loss: 0.761369, acc: 0.503906]  [Adversarial loss: 0.434937, acc: 0.917969]\n",
            "146: [Discriminator loss: 0.712722, acc: 0.531250]  [Adversarial loss: 0.433843, acc: 0.910156]\n",
            "147: [Discriminator loss: 0.721767, acc: 0.523438]  [Adversarial loss: 0.405951, acc: 0.921875]\n",
            "148: [Discriminator loss: 0.761088, acc: 0.515625]  [Adversarial loss: 0.429487, acc: 0.914062]\n",
            "149: [Discriminator loss: 0.788161, acc: 0.468750]  [Adversarial loss: 0.429588, acc: 0.925781]\n",
            "150: [Discriminator loss: 0.785244, acc: 0.484375]  [Adversarial loss: 0.413149, acc: 0.929688]\n",
            "151: [Discriminator loss: 0.723907, acc: 0.519531]  [Adversarial loss: 0.402180, acc: 0.933594]\n",
            "152: [Discriminator loss: 0.761353, acc: 0.535156]  [Adversarial loss: 0.398466, acc: 0.937500]\n",
            "153: [Discriminator loss: 0.780267, acc: 0.488281]  [Adversarial loss: 0.384218, acc: 0.937500]\n",
            "154: [Discriminator loss: 0.793532, acc: 0.472656]  [Adversarial loss: 0.415733, acc: 0.906250]\n",
            "155: [Discriminator loss: 0.850519, acc: 0.441406]  [Adversarial loss: 0.396212, acc: 0.937500]\n",
            "156: [Discriminator loss: 0.681509, acc: 0.562500]  [Adversarial loss: 0.396564, acc: 0.921875]\n",
            "157: [Discriminator loss: 0.763072, acc: 0.523438]  [Adversarial loss: 0.393851, acc: 0.906250]\n",
            "158: [Discriminator loss: 0.751486, acc: 0.500000]  [Adversarial loss: 0.421737, acc: 0.898438]\n",
            "159: [Discriminator loss: 0.699819, acc: 0.562500]  [Adversarial loss: 0.370131, acc: 0.933594]\n",
            "160: [Discriminator loss: 0.773196, acc: 0.507812]  [Adversarial loss: 0.428618, acc: 0.914062]\n",
            "161: [Discriminator loss: 0.787904, acc: 0.492188]  [Adversarial loss: 0.360318, acc: 0.957031]\n",
            "162: [Discriminator loss: 0.749402, acc: 0.515625]  [Adversarial loss: 0.418962, acc: 0.921875]\n",
            "163: [Discriminator loss: 0.804800, acc: 0.484375]  [Adversarial loss: 0.434867, acc: 0.882812]\n",
            "164: [Discriminator loss: 0.755192, acc: 0.519531]  [Adversarial loss: 0.412050, acc: 0.902344]\n",
            "165: [Discriminator loss: 0.796429, acc: 0.468750]  [Adversarial loss: 0.404656, acc: 0.933594]\n",
            "166: [Discriminator loss: 0.758189, acc: 0.507812]  [Adversarial loss: 0.462881, acc: 0.878906]\n",
            "167: [Discriminator loss: 0.801550, acc: 0.468750]  [Adversarial loss: 0.428134, acc: 0.917969]\n",
            "168: [Discriminator loss: 0.749958, acc: 0.500000]  [Adversarial loss: 0.431590, acc: 0.914062]\n",
            "169: [Discriminator loss: 0.737516, acc: 0.503906]  [Adversarial loss: 0.379464, acc: 0.960938]\n",
            "170: [Discriminator loss: 0.749968, acc: 0.531250]  [Adversarial loss: 0.403535, acc: 0.929688]\n",
            "171: [Discriminator loss: 0.716407, acc: 0.527344]  [Adversarial loss: 0.413256, acc: 0.914062]\n",
            "172: [Discriminator loss: 0.693341, acc: 0.554688]  [Adversarial loss: 0.387279, acc: 0.929688]\n",
            "173: [Discriminator loss: 0.740236, acc: 0.496094]  [Adversarial loss: 0.435194, acc: 0.894531]\n",
            "174: [Discriminator loss: 0.803406, acc: 0.488281]  [Adversarial loss: 0.416331, acc: 0.906250]\n",
            "175: [Discriminator loss: 0.711310, acc: 0.515625]  [Adversarial loss: 0.448218, acc: 0.875000]\n",
            "176: [Discriminator loss: 0.751253, acc: 0.519531]  [Adversarial loss: 0.426510, acc: 0.906250]\n",
            "177: [Discriminator loss: 0.760560, acc: 0.488281]  [Adversarial loss: 0.390014, acc: 0.933594]\n",
            "178: [Discriminator loss: 0.813258, acc: 0.433594]  [Adversarial loss: 0.442034, acc: 0.878906]\n",
            "179: [Discriminator loss: 0.723555, acc: 0.503906]  [Adversarial loss: 0.425040, acc: 0.914062]\n",
            "180: [Discriminator loss: 0.704552, acc: 0.523438]  [Adversarial loss: 0.468786, acc: 0.851562]\n",
            "181: [Discriminator loss: 0.711380, acc: 0.519531]  [Adversarial loss: 0.370305, acc: 0.925781]\n",
            "182: [Discriminator loss: 0.726455, acc: 0.496094]  [Adversarial loss: 0.403987, acc: 0.921875]\n",
            "183: [Discriminator loss: 0.770861, acc: 0.468750]  [Adversarial loss: 0.453972, acc: 0.890625]\n",
            "184: [Discriminator loss: 0.728003, acc: 0.500000]  [Adversarial loss: 0.443514, acc: 0.894531]\n",
            "185: [Discriminator loss: 0.736948, acc: 0.507812]  [Adversarial loss: 0.367767, acc: 0.937500]\n",
            "186: [Discriminator loss: 0.713606, acc: 0.539062]  [Adversarial loss: 0.428340, acc: 0.890625]\n",
            "187: [Discriminator loss: 0.792994, acc: 0.500000]  [Adversarial loss: 0.342260, acc: 0.949219]\n",
            "188: [Discriminator loss: 0.722952, acc: 0.535156]  [Adversarial loss: 0.359156, acc: 0.953125]\n",
            "189: [Discriminator loss: 0.783636, acc: 0.472656]  [Adversarial loss: 0.394836, acc: 0.929688]\n",
            "190: [Discriminator loss: 0.778847, acc: 0.484375]  [Adversarial loss: 0.362689, acc: 0.941406]\n",
            "191: [Discriminator loss: 0.723235, acc: 0.535156]  [Adversarial loss: 0.466094, acc: 0.843750]\n",
            "192: [Discriminator loss: 0.727115, acc: 0.542969]  [Adversarial loss: 0.358681, acc: 0.941406]\n",
            "193: [Discriminator loss: 0.722476, acc: 0.542969]  [Adversarial loss: 0.405624, acc: 0.917969]\n",
            "194: [Discriminator loss: 0.793883, acc: 0.484375]  [Adversarial loss: 0.416786, acc: 0.917969]\n",
            "195: [Discriminator loss: 0.723729, acc: 0.515625]  [Adversarial loss: 0.396266, acc: 0.917969]\n",
            "196: [Discriminator loss: 0.822612, acc: 0.460938]  [Adversarial loss: 0.404459, acc: 0.937500]\n",
            "197: [Discriminator loss: 0.703291, acc: 0.535156]  [Adversarial loss: 0.364698, acc: 0.925781]\n",
            "198: [Discriminator loss: 0.749600, acc: 0.507812]  [Adversarial loss: 0.399184, acc: 0.945312]\n",
            "199: [Discriminator loss: 0.743361, acc: 0.519531]  [Adversarial loss: 0.374082, acc: 0.929688]\n",
            "200: [Discriminator loss: 0.729532, acc: 0.503906]  [Adversarial loss: 0.376152, acc: 0.945312]\n",
            "201: [Discriminator loss: 0.725226, acc: 0.519531]  [Adversarial loss: 0.430473, acc: 0.894531]\n",
            "202: [Discriminator loss: 0.783727, acc: 0.472656]  [Adversarial loss: 0.421313, acc: 0.898438]\n",
            "203: [Discriminator loss: 0.687607, acc: 0.535156]  [Adversarial loss: 0.369957, acc: 0.929688]\n",
            "204: [Discriminator loss: 0.709149, acc: 0.531250]  [Adversarial loss: 0.384653, acc: 0.921875]\n",
            "205: [Discriminator loss: 0.722980, acc: 0.531250]  [Adversarial loss: 0.385867, acc: 0.921875]\n",
            "206: [Discriminator loss: 0.708823, acc: 0.500000]  [Adversarial loss: 0.394426, acc: 0.937500]\n",
            "207: [Discriminator loss: 0.780043, acc: 0.472656]  [Adversarial loss: 0.362633, acc: 0.949219]\n",
            "208: [Discriminator loss: 0.724702, acc: 0.527344]  [Adversarial loss: 0.396827, acc: 0.914062]\n",
            "209: [Discriminator loss: 0.734135, acc: 0.515625]  [Adversarial loss: 0.387247, acc: 0.914062]\n",
            "210: [Discriminator loss: 0.724916, acc: 0.515625]  [Adversarial loss: 0.379655, acc: 0.929688]\n",
            "211: [Discriminator loss: 0.721504, acc: 0.558594]  [Adversarial loss: 0.373635, acc: 0.921875]\n",
            "212: [Discriminator loss: 0.643956, acc: 0.585938]  [Adversarial loss: 0.373405, acc: 0.929688]\n",
            "213: [Discriminator loss: 0.772972, acc: 0.464844]  [Adversarial loss: 0.385601, acc: 0.945312]\n",
            "214: [Discriminator loss: 0.726336, acc: 0.523438]  [Adversarial loss: 0.379357, acc: 0.917969]\n",
            "215: [Discriminator loss: 0.730542, acc: 0.527344]  [Adversarial loss: 0.395981, acc: 0.929688]\n",
            "216: [Discriminator loss: 0.730893, acc: 0.519531]  [Adversarial loss: 0.407324, acc: 0.921875]\n",
            "217: [Discriminator loss: 0.665004, acc: 0.546875]  [Adversarial loss: 0.364006, acc: 0.953125]\n",
            "218: [Discriminator loss: 0.745780, acc: 0.507812]  [Adversarial loss: 0.415697, acc: 0.921875]\n",
            "219: [Discriminator loss: 0.721203, acc: 0.523438]  [Adversarial loss: 0.400332, acc: 0.921875]\n",
            "220: [Discriminator loss: 0.755529, acc: 0.484375]  [Adversarial loss: 0.409368, acc: 0.929688]\n",
            "221: [Discriminator loss: 0.775018, acc: 0.468750]  [Adversarial loss: 0.385562, acc: 0.929688]\n",
            "222: [Discriminator loss: 0.759446, acc: 0.488281]  [Adversarial loss: 0.420078, acc: 0.914062]\n",
            "223: [Discriminator loss: 0.788270, acc: 0.488281]  [Adversarial loss: 0.385396, acc: 0.929688]\n",
            "224: [Discriminator loss: 0.744760, acc: 0.511719]  [Adversarial loss: 0.346065, acc: 0.953125]\n",
            "225: [Discriminator loss: 0.752512, acc: 0.507812]  [Adversarial loss: 0.414713, acc: 0.910156]\n",
            "226: [Discriminator loss: 0.725866, acc: 0.507812]  [Adversarial loss: 0.434367, acc: 0.914062]\n",
            "227: [Discriminator loss: 0.712931, acc: 0.488281]  [Adversarial loss: 0.364678, acc: 0.945312]\n",
            "228: [Discriminator loss: 0.726832, acc: 0.488281]  [Adversarial loss: 0.433629, acc: 0.890625]\n",
            "229: [Discriminator loss: 0.686813, acc: 0.507812]  [Adversarial loss: 0.402126, acc: 0.914062]\n",
            "230: [Discriminator loss: 0.712462, acc: 0.472656]  [Adversarial loss: 0.415936, acc: 0.914062]\n",
            "231: [Discriminator loss: 0.680369, acc: 0.511719]  [Adversarial loss: 0.381332, acc: 0.921875]\n",
            "232: [Discriminator loss: 0.714765, acc: 0.527344]  [Adversarial loss: 0.416440, acc: 0.906250]\n",
            "233: [Discriminator loss: 0.715051, acc: 0.515625]  [Adversarial loss: 0.396710, acc: 0.917969]\n",
            "234: [Discriminator loss: 0.776227, acc: 0.464844]  [Adversarial loss: 0.394796, acc: 0.914062]\n",
            "235: [Discriminator loss: 0.693150, acc: 0.511719]  [Adversarial loss: 0.409436, acc: 0.890625]\n",
            "236: [Discriminator loss: 0.710962, acc: 0.500000]  [Adversarial loss: 0.386081, acc: 0.917969]\n",
            "237: [Discriminator loss: 0.749847, acc: 0.472656]  [Adversarial loss: 0.395863, acc: 0.917969]\n",
            "238: [Discriminator loss: 0.655366, acc: 0.542969]  [Adversarial loss: 0.398641, acc: 0.917969]\n",
            "239: [Discriminator loss: 0.734020, acc: 0.453125]  [Adversarial loss: 0.419631, acc: 0.925781]\n",
            "240: [Discriminator loss: 0.702932, acc: 0.531250]  [Adversarial loss: 0.320804, acc: 0.972656]\n",
            "241: [Discriminator loss: 0.744118, acc: 0.503906]  [Adversarial loss: 0.421443, acc: 0.902344]\n",
            "242: [Discriminator loss: 0.719395, acc: 0.527344]  [Adversarial loss: 0.373166, acc: 0.941406]\n",
            "243: [Discriminator loss: 0.752152, acc: 0.492188]  [Adversarial loss: 0.391938, acc: 0.941406]\n",
            "244: [Discriminator loss: 0.673718, acc: 0.535156]  [Adversarial loss: 0.395521, acc: 0.933594]\n",
            "245: [Discriminator loss: 0.716621, acc: 0.527344]  [Adversarial loss: 0.393071, acc: 0.937500]\n",
            "246: [Discriminator loss: 0.737647, acc: 0.476562]  [Adversarial loss: 0.432208, acc: 0.929688]\n",
            "247: [Discriminator loss: 0.720636, acc: 0.484375]  [Adversarial loss: 0.430215, acc: 0.906250]\n",
            "248: [Discriminator loss: 0.703880, acc: 0.503906]  [Adversarial loss: 0.397493, acc: 0.925781]\n",
            "249: [Discriminator loss: 0.655599, acc: 0.527344]  [Adversarial loss: 0.391911, acc: 0.925781]\n",
            "250: [Discriminator loss: 0.703340, acc: 0.519531]  [Adversarial loss: 0.388057, acc: 0.933594]\n",
            "251: [Discriminator loss: 0.646465, acc: 0.562500]  [Adversarial loss: 0.379892, acc: 0.937500]\n",
            "252: [Discriminator loss: 0.691196, acc: 0.519531]  [Adversarial loss: 0.410592, acc: 0.902344]\n",
            "253: [Discriminator loss: 0.676240, acc: 0.503906]  [Adversarial loss: 0.415291, acc: 0.914062]\n",
            "254: [Discriminator loss: 0.680022, acc: 0.511719]  [Adversarial loss: 0.392396, acc: 0.933594]\n",
            "255: [Discriminator loss: 0.730720, acc: 0.453125]  [Adversarial loss: 0.386312, acc: 0.921875]\n",
            "256: [Discriminator loss: 0.638772, acc: 0.546875]  [Adversarial loss: 0.372412, acc: 0.945312]\n",
            "257: [Discriminator loss: 0.687774, acc: 0.511719]  [Adversarial loss: 0.395930, acc: 0.933594]\n",
            "258: [Discriminator loss: 0.642368, acc: 0.523438]  [Adversarial loss: 0.384564, acc: 0.929688]\n",
            "259: [Discriminator loss: 0.759911, acc: 0.484375]  [Adversarial loss: 0.398005, acc: 0.933594]\n",
            "260: [Discriminator loss: 0.724529, acc: 0.496094]  [Adversarial loss: 0.379567, acc: 0.949219]\n",
            "261: [Discriminator loss: 0.674479, acc: 0.550781]  [Adversarial loss: 0.373115, acc: 0.925781]\n",
            "262: [Discriminator loss: 0.743853, acc: 0.500000]  [Adversarial loss: 0.386205, acc: 0.921875]\n",
            "263: [Discriminator loss: 0.762307, acc: 0.503906]  [Adversarial loss: 0.365015, acc: 0.953125]\n",
            "264: [Discriminator loss: 0.698510, acc: 0.523438]  [Adversarial loss: 0.382672, acc: 0.937500]\n",
            "265: [Discriminator loss: 0.680103, acc: 0.496094]  [Adversarial loss: 0.440506, acc: 0.906250]\n",
            "266: [Discriminator loss: 0.739044, acc: 0.500000]  [Adversarial loss: 0.362368, acc: 0.941406]\n",
            "267: [Discriminator loss: 0.646104, acc: 0.539062]  [Adversarial loss: 0.366271, acc: 0.945312]\n",
            "268: [Discriminator loss: 0.659077, acc: 0.531250]  [Adversarial loss: 0.369152, acc: 0.933594]\n",
            "269: [Discriminator loss: 0.712179, acc: 0.496094]  [Adversarial loss: 0.403738, acc: 0.949219]\n",
            "270: [Discriminator loss: 0.711401, acc: 0.464844]  [Adversarial loss: 0.407686, acc: 0.921875]\n",
            "271: [Discriminator loss: 0.614537, acc: 0.542969]  [Adversarial loss: 0.408676, acc: 0.910156]\n",
            "272: [Discriminator loss: 0.751712, acc: 0.468750]  [Adversarial loss: 0.390380, acc: 0.937500]\n",
            "273: [Discriminator loss: 0.661002, acc: 0.515625]  [Adversarial loss: 0.402277, acc: 0.941406]\n",
            "274: [Discriminator loss: 0.693756, acc: 0.492188]  [Adversarial loss: 0.386270, acc: 0.914062]\n",
            "275: [Discriminator loss: 0.700198, acc: 0.503906]  [Adversarial loss: 0.403812, acc: 0.949219]\n",
            "276: [Discriminator loss: 0.691630, acc: 0.515625]  [Adversarial loss: 0.412794, acc: 0.925781]\n",
            "277: [Discriminator loss: 0.742465, acc: 0.496094]  [Adversarial loss: 0.385641, acc: 0.945312]\n",
            "278: [Discriminator loss: 0.688521, acc: 0.523438]  [Adversarial loss: 0.384041, acc: 0.937500]\n",
            "279: [Discriminator loss: 0.677081, acc: 0.554688]  [Adversarial loss: 0.374138, acc: 0.914062]\n",
            "280: [Discriminator loss: 0.735497, acc: 0.464844]  [Adversarial loss: 0.376587, acc: 0.937500]\n",
            "281: [Discriminator loss: 0.717226, acc: 0.507812]  [Adversarial loss: 0.353465, acc: 0.937500]\n",
            "282: [Discriminator loss: 0.742620, acc: 0.464844]  [Adversarial loss: 0.360613, acc: 0.953125]\n",
            "283: [Discriminator loss: 0.764305, acc: 0.457031]  [Adversarial loss: 0.411377, acc: 0.937500]\n",
            "284: [Discriminator loss: 0.672627, acc: 0.527344]  [Adversarial loss: 0.386040, acc: 0.929688]\n",
            "285: [Discriminator loss: 0.773223, acc: 0.464844]  [Adversarial loss: 0.411862, acc: 0.933594]\n",
            "286: [Discriminator loss: 0.690550, acc: 0.500000]  [Adversarial loss: 0.397458, acc: 0.933594]\n",
            "287: [Discriminator loss: 0.681735, acc: 0.511719]  [Adversarial loss: 0.412528, acc: 0.925781]\n",
            "288: [Discriminator loss: 0.705017, acc: 0.488281]  [Adversarial loss: 0.418101, acc: 0.933594]\n",
            "289: [Discriminator loss: 0.587676, acc: 0.546875]  [Adversarial loss: 0.375618, acc: 0.949219]\n",
            "290: [Discriminator loss: 0.656910, acc: 0.550781]  [Adversarial loss: 0.385280, acc: 0.933594]\n",
            "291: [Discriminator loss: 0.691739, acc: 0.492188]  [Adversarial loss: 0.401790, acc: 0.933594]\n",
            "292: [Discriminator loss: 0.631603, acc: 0.550781]  [Adversarial loss: 0.369708, acc: 0.941406]\n",
            "293: [Discriminator loss: 0.709387, acc: 0.515625]  [Adversarial loss: 0.409889, acc: 0.933594]\n",
            "294: [Discriminator loss: 0.658394, acc: 0.570312]  [Adversarial loss: 0.325867, acc: 0.968750]\n",
            "295: [Discriminator loss: 0.674580, acc: 0.511719]  [Adversarial loss: 0.363175, acc: 0.937500]\n",
            "296: [Discriminator loss: 0.686655, acc: 0.500000]  [Adversarial loss: 0.367212, acc: 0.949219]\n",
            "297: [Discriminator loss: 0.703512, acc: 0.515625]  [Adversarial loss: 0.383727, acc: 0.933594]\n",
            "298: [Discriminator loss: 0.720676, acc: 0.515625]  [Adversarial loss: 0.403855, acc: 0.925781]\n",
            "299: [Discriminator loss: 0.759477, acc: 0.468750]  [Adversarial loss: 0.361771, acc: 0.960938]\n",
            "300: [Discriminator loss: 0.641031, acc: 0.527344]  [Adversarial loss: 0.403008, acc: 0.929688]\n",
            "301: [Discriminator loss: 0.726560, acc: 0.480469]  [Adversarial loss: 0.421596, acc: 0.917969]\n",
            "302: [Discriminator loss: 0.650878, acc: 0.515625]  [Adversarial loss: 0.373082, acc: 0.925781]\n",
            "303: [Discriminator loss: 0.647974, acc: 0.546875]  [Adversarial loss: 0.380594, acc: 0.925781]\n",
            "304: [Discriminator loss: 0.698636, acc: 0.511719]  [Adversarial loss: 0.359086, acc: 0.941406]\n",
            "305: [Discriminator loss: 0.608883, acc: 0.535156]  [Adversarial loss: 0.387093, acc: 0.910156]\n",
            "306: [Discriminator loss: 0.718274, acc: 0.480469]  [Adversarial loss: 0.399525, acc: 0.937500]\n",
            "307: [Discriminator loss: 0.659800, acc: 0.539062]  [Adversarial loss: 0.354712, acc: 0.953125]\n",
            "308: [Discriminator loss: 0.688845, acc: 0.503906]  [Adversarial loss: 0.368346, acc: 0.945312]\n",
            "309: [Discriminator loss: 0.673262, acc: 0.539062]  [Adversarial loss: 0.336495, acc: 0.953125]\n",
            "310: [Discriminator loss: 0.630363, acc: 0.539062]  [Adversarial loss: 0.404968, acc: 0.914062]\n",
            "311: [Discriminator loss: 0.723247, acc: 0.468750]  [Adversarial loss: 0.427799, acc: 0.933594]\n",
            "312: [Discriminator loss: 0.725200, acc: 0.464844]  [Adversarial loss: 0.403891, acc: 0.933594]\n",
            "313: [Discriminator loss: 0.682827, acc: 0.523438]  [Adversarial loss: 0.361127, acc: 0.945312]\n",
            "314: [Discriminator loss: 0.688507, acc: 0.523438]  [Adversarial loss: 0.389446, acc: 0.929688]\n",
            "315: [Discriminator loss: 0.611076, acc: 0.511719]  [Adversarial loss: 0.400773, acc: 0.941406]\n",
            "316: [Discriminator loss: 0.698992, acc: 0.503906]  [Adversarial loss: 0.380280, acc: 0.929688]\n",
            "317: [Discriminator loss: 0.666587, acc: 0.550781]  [Adversarial loss: 0.347435, acc: 0.949219]\n",
            "318: [Discriminator loss: 0.689428, acc: 0.496094]  [Adversarial loss: 0.377815, acc: 0.937500]\n",
            "319: [Discriminator loss: 0.782831, acc: 0.460938]  [Adversarial loss: 0.401858, acc: 0.921875]\n",
            "320: [Discriminator loss: 0.664099, acc: 0.562500]  [Adversarial loss: 0.387610, acc: 0.925781]\n",
            "321: [Discriminator loss: 0.729915, acc: 0.484375]  [Adversarial loss: 0.379364, acc: 0.941406]\n",
            "322: [Discriminator loss: 0.640178, acc: 0.554688]  [Adversarial loss: 0.367354, acc: 0.937500]\n",
            "323: [Discriminator loss: 0.724517, acc: 0.480469]  [Adversarial loss: 0.399087, acc: 0.929688]\n",
            "324: [Discriminator loss: 0.681217, acc: 0.507812]  [Adversarial loss: 0.367108, acc: 0.945312]\n",
            "325: [Discriminator loss: 0.710187, acc: 0.472656]  [Adversarial loss: 0.411897, acc: 0.917969]\n",
            "326: [Discriminator loss: 0.658300, acc: 0.535156]  [Adversarial loss: 0.367783, acc: 0.933594]\n",
            "327: [Discriminator loss: 0.623411, acc: 0.566406]  [Adversarial loss: 0.399705, acc: 0.929688]\n",
            "328: [Discriminator loss: 0.699866, acc: 0.507812]  [Adversarial loss: 0.395894, acc: 0.925781]\n",
            "329: [Discriminator loss: 0.661534, acc: 0.500000]  [Adversarial loss: 0.367638, acc: 0.937500]\n",
            "330: [Discriminator loss: 0.664420, acc: 0.519531]  [Adversarial loss: 0.371862, acc: 0.945312]\n",
            "331: [Discriminator loss: 0.606697, acc: 0.574219]  [Adversarial loss: 0.356798, acc: 0.945312]\n",
            "332: [Discriminator loss: 0.668659, acc: 0.492188]  [Adversarial loss: 0.389084, acc: 0.925781]\n",
            "333: [Discriminator loss: 0.612115, acc: 0.531250]  [Adversarial loss: 0.389564, acc: 0.921875]\n",
            "334: [Discriminator loss: 0.659606, acc: 0.515625]  [Adversarial loss: 0.395119, acc: 0.914062]\n",
            "335: [Discriminator loss: 0.673937, acc: 0.515625]  [Adversarial loss: 0.354519, acc: 0.960938]\n",
            "336: [Discriminator loss: 0.759111, acc: 0.472656]  [Adversarial loss: 0.364302, acc: 0.960938]\n",
            "337: [Discriminator loss: 0.784610, acc: 0.457031]  [Adversarial loss: 0.394447, acc: 0.941406]\n",
            "338: [Discriminator loss: 0.711693, acc: 0.492188]  [Adversarial loss: 0.373018, acc: 0.949219]\n",
            "339: [Discriminator loss: 0.652872, acc: 0.523438]  [Adversarial loss: 0.369983, acc: 0.937500]\n",
            "340: [Discriminator loss: 0.724525, acc: 0.500000]  [Adversarial loss: 0.393326, acc: 0.937500]\n",
            "341: [Discriminator loss: 0.659762, acc: 0.531250]  [Adversarial loss: 0.381604, acc: 0.941406]\n",
            "342: [Discriminator loss: 0.676710, acc: 0.492188]  [Adversarial loss: 0.368430, acc: 0.960938]\n",
            "343: [Discriminator loss: 0.682013, acc: 0.492188]  [Adversarial loss: 0.379200, acc: 0.949219]\n",
            "344: [Discriminator loss: 0.685642, acc: 0.531250]  [Adversarial loss: 0.385181, acc: 0.921875]\n",
            "345: [Discriminator loss: 0.646729, acc: 0.554688]  [Adversarial loss: 0.390680, acc: 0.910156]\n",
            "346: [Discriminator loss: 0.740819, acc: 0.445312]  [Adversarial loss: 0.418501, acc: 0.906250]\n",
            "347: [Discriminator loss: 0.612783, acc: 0.535156]  [Adversarial loss: 0.392799, acc: 0.933594]\n",
            "348: [Discriminator loss: 0.663269, acc: 0.500000]  [Adversarial loss: 0.373917, acc: 0.933594]\n",
            "349: [Discriminator loss: 0.665503, acc: 0.503906]  [Adversarial loss: 0.354170, acc: 0.957031]\n",
            "350: [Discriminator loss: 0.673811, acc: 0.511719]  [Adversarial loss: 0.347508, acc: 0.957031]\n",
            "351: [Discriminator loss: 0.674369, acc: 0.488281]  [Adversarial loss: 0.401572, acc: 0.929688]\n",
            "352: [Discriminator loss: 0.655129, acc: 0.523438]  [Adversarial loss: 0.355525, acc: 0.957031]\n",
            "353: [Discriminator loss: 0.695889, acc: 0.496094]  [Adversarial loss: 0.378749, acc: 0.941406]\n",
            "354: [Discriminator loss: 0.742320, acc: 0.457031]  [Adversarial loss: 0.367390, acc: 0.960938]\n",
            "355: [Discriminator loss: 0.720327, acc: 0.476562]  [Adversarial loss: 0.379344, acc: 0.949219]\n",
            "356: [Discriminator loss: 0.675657, acc: 0.519531]  [Adversarial loss: 0.349701, acc: 0.960938]\n",
            "357: [Discriminator loss: 0.656392, acc: 0.503906]  [Adversarial loss: 0.352569, acc: 0.949219]\n",
            "358: [Discriminator loss: 0.639139, acc: 0.531250]  [Adversarial loss: 0.347592, acc: 0.949219]\n",
            "359: [Discriminator loss: 0.683478, acc: 0.515625]  [Adversarial loss: 0.339884, acc: 0.929688]\n",
            "360: [Discriminator loss: 0.735802, acc: 0.437500]  [Adversarial loss: 0.363114, acc: 0.953125]\n",
            "361: [Discriminator loss: 0.652110, acc: 0.492188]  [Adversarial loss: 0.391915, acc: 0.921875]\n",
            "362: [Discriminator loss: 0.660382, acc: 0.542969]  [Adversarial loss: 0.376453, acc: 0.929688]\n",
            "363: [Discriminator loss: 0.668474, acc: 0.500000]  [Adversarial loss: 0.359485, acc: 0.960938]\n",
            "364: [Discriminator loss: 0.656346, acc: 0.539062]  [Adversarial loss: 0.344197, acc: 0.945312]\n",
            "365: [Discriminator loss: 0.701070, acc: 0.488281]  [Adversarial loss: 0.356438, acc: 0.937500]\n",
            "366: [Discriminator loss: 0.731867, acc: 0.480469]  [Adversarial loss: 0.357835, acc: 0.960938]\n",
            "367: [Discriminator loss: 0.639368, acc: 0.515625]  [Adversarial loss: 0.383850, acc: 0.910156]\n",
            "368: [Discriminator loss: 0.709115, acc: 0.539062]  [Adversarial loss: 0.357866, acc: 0.953125]\n",
            "369: [Discriminator loss: 0.715186, acc: 0.500000]  [Adversarial loss: 0.357278, acc: 0.937500]\n",
            "370: [Discriminator loss: 0.702011, acc: 0.503906]  [Adversarial loss: 0.357227, acc: 0.945312]\n",
            "371: [Discriminator loss: 0.683936, acc: 0.535156]  [Adversarial loss: 0.375678, acc: 0.929688]\n",
            "372: [Discriminator loss: 0.645011, acc: 0.539062]  [Adversarial loss: 0.388524, acc: 0.914062]\n",
            "373: [Discriminator loss: 0.634043, acc: 0.539062]  [Adversarial loss: 0.352511, acc: 0.953125]\n",
            "374: [Discriminator loss: 0.702139, acc: 0.503906]  [Adversarial loss: 0.364616, acc: 0.941406]\n",
            "375: [Discriminator loss: 0.677605, acc: 0.519531]  [Adversarial loss: 0.334706, acc: 0.964844]\n",
            "376: [Discriminator loss: 0.679268, acc: 0.468750]  [Adversarial loss: 0.414008, acc: 0.917969]\n",
            "377: [Discriminator loss: 0.615086, acc: 0.554688]  [Adversarial loss: 0.354656, acc: 0.937500]\n",
            "378: [Discriminator loss: 0.679143, acc: 0.511719]  [Adversarial loss: 0.320784, acc: 0.968750]\n",
            "379: [Discriminator loss: 0.672743, acc: 0.539062]  [Adversarial loss: 0.367419, acc: 0.925781]\n",
            "380: [Discriminator loss: 0.724430, acc: 0.507812]  [Adversarial loss: 0.348370, acc: 0.937500]\n",
            "381: [Discriminator loss: 0.642532, acc: 0.511719]  [Adversarial loss: 0.383180, acc: 0.906250]\n",
            "382: [Discriminator loss: 0.712265, acc: 0.519531]  [Adversarial loss: 0.334475, acc: 0.949219]\n",
            "383: [Discriminator loss: 0.699895, acc: 0.449219]  [Adversarial loss: 0.386883, acc: 0.957031]\n",
            "384: [Discriminator loss: 0.679035, acc: 0.523438]  [Adversarial loss: 0.336466, acc: 0.949219]\n",
            "385: [Discriminator loss: 0.646913, acc: 0.519531]  [Adversarial loss: 0.330832, acc: 0.957031]\n",
            "386: [Discriminator loss: 0.670630, acc: 0.554688]  [Adversarial loss: 0.359996, acc: 0.929688]\n",
            "387: [Discriminator loss: 0.688565, acc: 0.500000]  [Adversarial loss: 0.353154, acc: 0.957031]\n",
            "388: [Discriminator loss: 0.736905, acc: 0.476562]  [Adversarial loss: 0.346044, acc: 0.957031]\n",
            "389: [Discriminator loss: 0.730529, acc: 0.488281]  [Adversarial loss: 0.329462, acc: 0.964844]\n",
            "390: [Discriminator loss: 0.773350, acc: 0.484375]  [Adversarial loss: 0.324808, acc: 0.957031]\n",
            "391: [Discriminator loss: 0.665747, acc: 0.535156]  [Adversarial loss: 0.359965, acc: 0.929688]\n",
            "392: [Discriminator loss: 0.663993, acc: 0.500000]  [Adversarial loss: 0.378351, acc: 0.925781]\n",
            "393: [Discriminator loss: 0.679573, acc: 0.527344]  [Adversarial loss: 0.354982, acc: 0.953125]\n",
            "394: [Discriminator loss: 0.669697, acc: 0.527344]  [Adversarial loss: 0.378278, acc: 0.910156]\n",
            "395: [Discriminator loss: 0.624228, acc: 0.550781]  [Adversarial loss: 0.367466, acc: 0.906250]\n",
            "396: [Discriminator loss: 0.672815, acc: 0.507812]  [Adversarial loss: 0.355645, acc: 0.925781]\n",
            "397: [Discriminator loss: 0.683749, acc: 0.507812]  [Adversarial loss: 0.361151, acc: 0.925781]\n",
            "398: [Discriminator loss: 0.715424, acc: 0.468750]  [Adversarial loss: 0.405452, acc: 0.925781]\n",
            "399: [Discriminator loss: 0.748073, acc: 0.480469]  [Adversarial loss: 0.339860, acc: 0.945312]\n",
            "400: [Discriminator loss: 0.657706, acc: 0.535156]  [Adversarial loss: 0.388052, acc: 0.921875]\n",
            "401: [Discriminator loss: 0.715103, acc: 0.464844]  [Adversarial loss: 0.378376, acc: 0.933594]\n",
            "402: [Discriminator loss: 0.556315, acc: 0.570312]  [Adversarial loss: 0.368611, acc: 0.917969]\n",
            "403: [Discriminator loss: 0.682807, acc: 0.496094]  [Adversarial loss: 0.345657, acc: 0.964844]\n",
            "404: [Discriminator loss: 0.628811, acc: 0.531250]  [Adversarial loss: 0.346571, acc: 0.933594]\n",
            "405: [Discriminator loss: 0.663389, acc: 0.511719]  [Adversarial loss: 0.363123, acc: 0.949219]\n",
            "406: [Discriminator loss: 0.607100, acc: 0.542969]  [Adversarial loss: 0.346659, acc: 0.929688]\n",
            "407: [Discriminator loss: 0.612028, acc: 0.550781]  [Adversarial loss: 0.318840, acc: 0.960938]\n",
            "408: [Discriminator loss: 0.739329, acc: 0.492188]  [Adversarial loss: 0.361366, acc: 0.941406]\n",
            "409: [Discriminator loss: 0.624250, acc: 0.519531]  [Adversarial loss: 0.324646, acc: 0.949219]\n",
            "410: [Discriminator loss: 0.696505, acc: 0.507812]  [Adversarial loss: 0.372160, acc: 0.914062]\n",
            "411: [Discriminator loss: 0.696316, acc: 0.507812]  [Adversarial loss: 0.328718, acc: 0.941406]\n",
            "412: [Discriminator loss: 0.656087, acc: 0.542969]  [Adversarial loss: 0.302306, acc: 0.945312]\n",
            "413: [Discriminator loss: 0.718523, acc: 0.523438]  [Adversarial loss: 0.325468, acc: 0.953125]\n",
            "414: [Discriminator loss: 0.654772, acc: 0.527344]  [Adversarial loss: 0.343819, acc: 0.917969]\n",
            "415: [Discriminator loss: 0.654225, acc: 0.507812]  [Adversarial loss: 0.346277, acc: 0.921875]\n",
            "416: [Discriminator loss: 0.672255, acc: 0.500000]  [Adversarial loss: 0.331050, acc: 0.964844]\n",
            "417: [Discriminator loss: 0.565032, acc: 0.601562]  [Adversarial loss: 0.336605, acc: 0.921875]\n",
            "418: [Discriminator loss: 0.688936, acc: 0.476562]  [Adversarial loss: 0.336387, acc: 0.949219]\n",
            "419: [Discriminator loss: 0.629591, acc: 0.488281]  [Adversarial loss: 0.360310, acc: 0.921875]\n",
            "420: [Discriminator loss: 0.607955, acc: 0.535156]  [Adversarial loss: 0.332100, acc: 0.925781]\n",
            "421: [Discriminator loss: 0.732461, acc: 0.441406]  [Adversarial loss: 0.366837, acc: 0.937500]\n",
            "422: [Discriminator loss: 0.616600, acc: 0.546875]  [Adversarial loss: 0.294702, acc: 0.972656]\n",
            "423: [Discriminator loss: 0.627886, acc: 0.558594]  [Adversarial loss: 0.287868, acc: 0.949219]\n",
            "424: [Discriminator loss: 0.674559, acc: 0.519531]  [Adversarial loss: 0.348075, acc: 0.945312]\n",
            "425: [Discriminator loss: 0.678129, acc: 0.472656]  [Adversarial loss: 0.354566, acc: 0.929688]\n",
            "426: [Discriminator loss: 0.722358, acc: 0.464844]  [Adversarial loss: 0.332293, acc: 0.953125]\n",
            "427: [Discriminator loss: 0.639202, acc: 0.523438]  [Adversarial loss: 0.318988, acc: 0.960938]\n",
            "428: [Discriminator loss: 0.668062, acc: 0.523438]  [Adversarial loss: 0.340821, acc: 0.953125]\n",
            "429: [Discriminator loss: 0.654613, acc: 0.503906]  [Adversarial loss: 0.356214, acc: 0.917969]\n",
            "430: [Discriminator loss: 0.695733, acc: 0.503906]  [Adversarial loss: 0.311239, acc: 0.964844]\n",
            "431: [Discriminator loss: 0.629453, acc: 0.527344]  [Adversarial loss: 0.330158, acc: 0.953125]\n",
            "432: [Discriminator loss: 0.687065, acc: 0.476562]  [Adversarial loss: 0.355419, acc: 0.953125]\n",
            "433: [Discriminator loss: 0.691497, acc: 0.476562]  [Adversarial loss: 0.338702, acc: 0.929688]\n",
            "434: [Discriminator loss: 0.652825, acc: 0.527344]  [Adversarial loss: 0.303726, acc: 0.957031]\n",
            "435: [Discriminator loss: 0.623040, acc: 0.523438]  [Adversarial loss: 0.338043, acc: 0.945312]\n",
            "436: [Discriminator loss: 0.643204, acc: 0.503906]  [Adversarial loss: 0.327650, acc: 0.953125]\n",
            "437: [Discriminator loss: 0.660595, acc: 0.511719]  [Adversarial loss: 0.336344, acc: 0.941406]\n",
            "438: [Discriminator loss: 0.568854, acc: 0.539062]  [Adversarial loss: 0.309099, acc: 0.953125]\n",
            "439: [Discriminator loss: 0.605538, acc: 0.542969]  [Adversarial loss: 0.315475, acc: 0.945312]\n",
            "440: [Discriminator loss: 0.672208, acc: 0.480469]  [Adversarial loss: 0.351594, acc: 0.933594]\n",
            "441: [Discriminator loss: 0.600990, acc: 0.535156]  [Adversarial loss: 0.309394, acc: 0.953125]\n",
            "442: [Discriminator loss: 0.643768, acc: 0.519531]  [Adversarial loss: 0.324069, acc: 0.945312]\n",
            "443: [Discriminator loss: 0.613273, acc: 0.546875]  [Adversarial loss: 0.299353, acc: 0.964844]\n",
            "444: [Discriminator loss: 0.820211, acc: 0.460938]  [Adversarial loss: 0.299472, acc: 0.968750]\n",
            "445: [Discriminator loss: 0.635364, acc: 0.539062]  [Adversarial loss: 0.332194, acc: 0.929688]\n",
            "446: [Discriminator loss: 0.631452, acc: 0.507812]  [Adversarial loss: 0.335890, acc: 0.929688]\n",
            "447: [Discriminator loss: 0.621341, acc: 0.503906]  [Adversarial loss: 0.350149, acc: 0.929688]\n",
            "448: [Discriminator loss: 0.694347, acc: 0.511719]  [Adversarial loss: 0.299961, acc: 0.953125]\n",
            "449: [Discriminator loss: 0.651036, acc: 0.500000]  [Adversarial loss: 0.318275, acc: 0.945312]\n",
            "450: [Discriminator loss: 0.666559, acc: 0.488281]  [Adversarial loss: 0.347121, acc: 0.925781]\n",
            "451: [Discriminator loss: 0.644179, acc: 0.480469]  [Adversarial loss: 0.336074, acc: 0.945312]\n",
            "452: [Discriminator loss: 0.676593, acc: 0.511719]  [Adversarial loss: 0.326728, acc: 0.929688]\n",
            "453: [Discriminator loss: 0.620619, acc: 0.539062]  [Adversarial loss: 0.310364, acc: 0.925781]\n",
            "454: [Discriminator loss: 0.632867, acc: 0.527344]  [Adversarial loss: 0.299881, acc: 0.949219]\n",
            "455: [Discriminator loss: 0.685671, acc: 0.496094]  [Adversarial loss: 0.340027, acc: 0.917969]\n",
            "456: [Discriminator loss: 0.615136, acc: 0.539062]  [Adversarial loss: 0.306598, acc: 0.945312]\n",
            "457: [Discriminator loss: 0.620465, acc: 0.527344]  [Adversarial loss: 0.299636, acc: 0.937500]\n",
            "458: [Discriminator loss: 0.627119, acc: 0.566406]  [Adversarial loss: 0.323438, acc: 0.937500]\n",
            "459: [Discriminator loss: 0.679487, acc: 0.500000]  [Adversarial loss: 0.338032, acc: 0.917969]\n",
            "460: [Discriminator loss: 0.696896, acc: 0.453125]  [Adversarial loss: 0.358165, acc: 0.921875]\n",
            "461: [Discriminator loss: 0.674378, acc: 0.507812]  [Adversarial loss: 0.304282, acc: 0.949219]\n",
            "462: [Discriminator loss: 0.638398, acc: 0.496094]  [Adversarial loss: 0.323660, acc: 0.949219]\n",
            "463: [Discriminator loss: 0.654041, acc: 0.519531]  [Adversarial loss: 0.306027, acc: 0.937500]\n",
            "464: [Discriminator loss: 0.565991, acc: 0.554688]  [Adversarial loss: 0.279950, acc: 0.953125]\n",
            "465: [Discriminator loss: 0.631165, acc: 0.554688]  [Adversarial loss: 0.293983, acc: 0.953125]\n",
            "466: [Discriminator loss: 0.700945, acc: 0.468750]  [Adversarial loss: 0.344456, acc: 0.929688]\n",
            "467: [Discriminator loss: 0.584579, acc: 0.562500]  [Adversarial loss: 0.269120, acc: 0.945312]\n",
            "468: [Discriminator loss: 0.670279, acc: 0.464844]  [Adversarial loss: 0.317475, acc: 0.941406]\n",
            "469: [Discriminator loss: 0.628735, acc: 0.562500]  [Adversarial loss: 0.305957, acc: 0.945312]\n",
            "470: [Discriminator loss: 0.696343, acc: 0.480469]  [Adversarial loss: 0.306225, acc: 0.949219]\n",
            "471: [Discriminator loss: 0.576848, acc: 0.554688]  [Adversarial loss: 0.310062, acc: 0.929688]\n",
            "472: [Discriminator loss: 0.707508, acc: 0.480469]  [Adversarial loss: 0.319091, acc: 0.953125]\n",
            "473: [Discriminator loss: 0.662947, acc: 0.488281]  [Adversarial loss: 0.332793, acc: 0.949219]\n",
            "474: [Discriminator loss: 0.638278, acc: 0.523438]  [Adversarial loss: 0.268343, acc: 0.953125]\n",
            "475: [Discriminator loss: 0.695688, acc: 0.500000]  [Adversarial loss: 0.307848, acc: 0.949219]\n",
            "476: [Discriminator loss: 0.637016, acc: 0.476562]  [Adversarial loss: 0.310807, acc: 0.949219]\n",
            "477: [Discriminator loss: 0.596220, acc: 0.492188]  [Adversarial loss: 0.317932, acc: 0.941406]\n",
            "478: [Discriminator loss: 0.591067, acc: 0.546875]  [Adversarial loss: 0.301182, acc: 0.941406]\n",
            "479: [Discriminator loss: 0.649982, acc: 0.492188]  [Adversarial loss: 0.334620, acc: 0.925781]\n",
            "480: [Discriminator loss: 0.652214, acc: 0.468750]  [Adversarial loss: 0.339149, acc: 0.914062]\n",
            "481: [Discriminator loss: 0.614801, acc: 0.574219]  [Adversarial loss: 0.304763, acc: 0.917969]\n",
            "482: [Discriminator loss: 0.617032, acc: 0.496094]  [Adversarial loss: 0.337359, acc: 0.917969]\n",
            "483: [Discriminator loss: 0.624745, acc: 0.531250]  [Adversarial loss: 0.303871, acc: 0.910156]\n",
            "484: [Discriminator loss: 0.639136, acc: 0.523438]  [Adversarial loss: 0.289765, acc: 0.929688]\n",
            "485: [Discriminator loss: 0.708082, acc: 0.460938]  [Adversarial loss: 0.339646, acc: 0.925781]\n",
            "486: [Discriminator loss: 0.647682, acc: 0.492188]  [Adversarial loss: 0.308747, acc: 0.921875]\n",
            "487: [Discriminator loss: 0.653225, acc: 0.503906]  [Adversarial loss: 0.300059, acc: 0.925781]\n",
            "488: [Discriminator loss: 0.705918, acc: 0.488281]  [Adversarial loss: 0.311023, acc: 0.937500]\n",
            "489: [Discriminator loss: 0.702700, acc: 0.527344]  [Adversarial loss: 0.298549, acc: 0.945312]\n",
            "490: [Discriminator loss: 0.683062, acc: 0.515625]  [Adversarial loss: 0.316175, acc: 0.929688]\n",
            "491: [Discriminator loss: 0.654468, acc: 0.535156]  [Adversarial loss: 0.293781, acc: 0.953125]\n",
            "492: [Discriminator loss: 0.614338, acc: 0.535156]  [Adversarial loss: 0.326648, acc: 0.937500]\n",
            "493: [Discriminator loss: 0.672602, acc: 0.503906]  [Adversarial loss: 0.291451, acc: 0.941406]\n",
            "494: [Discriminator loss: 0.583743, acc: 0.542969]  [Adversarial loss: 0.268046, acc: 0.957031]\n",
            "495: [Discriminator loss: 0.630551, acc: 0.515625]  [Adversarial loss: 0.302538, acc: 0.937500]\n",
            "496: [Discriminator loss: 0.595902, acc: 0.523438]  [Adversarial loss: 0.319817, acc: 0.929688]\n",
            "497: [Discriminator loss: 0.661413, acc: 0.480469]  [Adversarial loss: 0.342189, acc: 0.929688]\n",
            "498: [Discriminator loss: 0.664510, acc: 0.488281]  [Adversarial loss: 0.344388, acc: 0.906250]\n",
            "499: [Discriminator loss: 0.681604, acc: 0.511719]  [Adversarial loss: 0.294233, acc: 0.937500]\n",
            "500: [Discriminator loss: 0.679512, acc: 0.515625]  [Adversarial loss: 0.317289, acc: 0.925781]\n",
            "501: [Discriminator loss: 0.614795, acc: 0.511719]  [Adversarial loss: 0.309335, acc: 0.953125]\n",
            "502: [Discriminator loss: 0.607687, acc: 0.566406]  [Adversarial loss: 0.288074, acc: 0.937500]\n",
            "503: [Discriminator loss: 0.658432, acc: 0.515625]  [Adversarial loss: 0.286862, acc: 0.960938]\n",
            "504: [Discriminator loss: 0.601927, acc: 0.503906]  [Adversarial loss: 0.314223, acc: 0.945312]\n",
            "505: [Discriminator loss: 0.656330, acc: 0.480469]  [Adversarial loss: 0.311467, acc: 0.937500]\n",
            "506: [Discriminator loss: 0.656384, acc: 0.546875]  [Adversarial loss: 0.279088, acc: 0.945312]\n",
            "507: [Discriminator loss: 0.601689, acc: 0.535156]  [Adversarial loss: 0.314006, acc: 0.949219]\n",
            "508: [Discriminator loss: 0.599219, acc: 0.523438]  [Adversarial loss: 0.286242, acc: 0.949219]\n",
            "509: [Discriminator loss: 0.695486, acc: 0.500000]  [Adversarial loss: 0.275354, acc: 0.960938]\n",
            "510: [Discriminator loss: 0.585546, acc: 0.531250]  [Adversarial loss: 0.275987, acc: 0.949219]\n",
            "511: [Discriminator loss: 0.634070, acc: 0.527344]  [Adversarial loss: 0.291240, acc: 0.945312]\n",
            "512: [Discriminator loss: 0.692558, acc: 0.476562]  [Adversarial loss: 0.335002, acc: 0.914062]\n",
            "513: [Discriminator loss: 0.652300, acc: 0.511719]  [Adversarial loss: 0.304439, acc: 0.945312]\n",
            "514: [Discriminator loss: 0.601453, acc: 0.511719]  [Adversarial loss: 0.304688, acc: 0.929688]\n",
            "515: [Discriminator loss: 0.661683, acc: 0.550781]  [Adversarial loss: 0.263500, acc: 0.964844]\n",
            "516: [Discriminator loss: 0.678437, acc: 0.515625]  [Adversarial loss: 0.296673, acc: 0.933594]\n",
            "517: [Discriminator loss: 0.699302, acc: 0.476562]  [Adversarial loss: 0.312562, acc: 0.953125]\n",
            "518: [Discriminator loss: 0.669647, acc: 0.460938]  [Adversarial loss: 0.336967, acc: 0.933594]\n",
            "519: [Discriminator loss: 0.672972, acc: 0.480469]  [Adversarial loss: 0.321070, acc: 0.949219]\n",
            "520: [Discriminator loss: 0.582358, acc: 0.562500]  [Adversarial loss: 0.294941, acc: 0.949219]\n",
            "521: [Discriminator loss: 0.651843, acc: 0.507812]  [Adversarial loss: 0.303623, acc: 0.941406]\n",
            "522: [Discriminator loss: 0.613024, acc: 0.535156]  [Adversarial loss: 0.294302, acc: 0.933594]\n",
            "523: [Discriminator loss: 0.582894, acc: 0.539062]  [Adversarial loss: 0.313149, acc: 0.925781]\n",
            "524: [Discriminator loss: 0.643475, acc: 0.488281]  [Adversarial loss: 0.308838, acc: 0.953125]\n",
            "525: [Discriminator loss: 0.590207, acc: 0.570312]  [Adversarial loss: 0.289039, acc: 0.925781]\n",
            "526: [Discriminator loss: 0.651780, acc: 0.476562]  [Adversarial loss: 0.334960, acc: 0.933594]\n",
            "527: [Discriminator loss: 0.665660, acc: 0.527344]  [Adversarial loss: 0.311517, acc: 0.917969]\n",
            "528: [Discriminator loss: 0.614712, acc: 0.546875]  [Adversarial loss: 0.320486, acc: 0.902344]\n",
            "529: [Discriminator loss: 0.627514, acc: 0.535156]  [Adversarial loss: 0.305610, acc: 0.937500]\n",
            "530: [Discriminator loss: 0.627190, acc: 0.515625]  [Adversarial loss: 0.320089, acc: 0.937500]\n",
            "531: [Discriminator loss: 0.601555, acc: 0.531250]  [Adversarial loss: 0.281701, acc: 0.929688]\n",
            "532: [Discriminator loss: 0.590012, acc: 0.558594]  [Adversarial loss: 0.274744, acc: 0.933594]\n",
            "533: [Discriminator loss: 0.678794, acc: 0.468750]  [Adversarial loss: 0.321769, acc: 0.921875]\n",
            "534: [Discriminator loss: 0.591833, acc: 0.554688]  [Adversarial loss: 0.279781, acc: 0.941406]\n",
            "535: [Discriminator loss: 0.531811, acc: 0.570312]  [Adversarial loss: 0.275133, acc: 0.929688]\n",
            "536: [Discriminator loss: 0.631680, acc: 0.519531]  [Adversarial loss: 0.305955, acc: 0.921875]\n",
            "537: [Discriminator loss: 0.635946, acc: 0.492188]  [Adversarial loss: 0.330478, acc: 0.929688]\n",
            "538: [Discriminator loss: 0.635549, acc: 0.523438]  [Adversarial loss: 0.270551, acc: 0.921875]\n",
            "539: [Discriminator loss: 0.623662, acc: 0.503906]  [Adversarial loss: 0.312873, acc: 0.929688]\n",
            "540: [Discriminator loss: 0.570114, acc: 0.578125]  [Adversarial loss: 0.319445, acc: 0.898438]\n",
            "541: [Discriminator loss: 0.649932, acc: 0.492188]  [Adversarial loss: 0.291814, acc: 0.949219]\n",
            "542: [Discriminator loss: 0.656849, acc: 0.484375]  [Adversarial loss: 0.321972, acc: 0.917969]\n",
            "543: [Discriminator loss: 0.640302, acc: 0.511719]  [Adversarial loss: 0.282145, acc: 0.960938]\n",
            "544: [Discriminator loss: 0.652430, acc: 0.511719]  [Adversarial loss: 0.307587, acc: 0.933594]\n",
            "545: [Discriminator loss: 0.704324, acc: 0.488281]  [Adversarial loss: 0.323195, acc: 0.921875]\n",
            "546: [Discriminator loss: 0.636907, acc: 0.492188]  [Adversarial loss: 0.324701, acc: 0.949219]\n",
            "547: [Discriminator loss: 0.572331, acc: 0.503906]  [Adversarial loss: 0.320668, acc: 0.921875]\n",
            "548: [Discriminator loss: 0.588779, acc: 0.558594]  [Adversarial loss: 0.297445, acc: 0.917969]\n",
            "549: [Discriminator loss: 0.622967, acc: 0.523438]  [Adversarial loss: 0.276439, acc: 0.960938]\n",
            "550: [Discriminator loss: 0.661403, acc: 0.464844]  [Adversarial loss: 0.331535, acc: 0.949219]\n",
            "551: [Discriminator loss: 0.648509, acc: 0.472656]  [Adversarial loss: 0.335887, acc: 0.945312]\n",
            "552: [Discriminator loss: 0.611369, acc: 0.484375]  [Adversarial loss: 0.326996, acc: 0.941406]\n",
            "553: [Discriminator loss: 0.556113, acc: 0.585938]  [Adversarial loss: 0.260175, acc: 0.960938]\n",
            "554: [Discriminator loss: 0.592083, acc: 0.535156]  [Adversarial loss: 0.290162, acc: 0.933594]\n",
            "555: [Discriminator loss: 0.643720, acc: 0.484375]  [Adversarial loss: 0.335573, acc: 0.898438]\n",
            "556: [Discriminator loss: 0.659116, acc: 0.519531]  [Adversarial loss: 0.288794, acc: 0.957031]\n",
            "557: [Discriminator loss: 0.676762, acc: 0.511719]  [Adversarial loss: 0.285986, acc: 0.968750]\n",
            "558: [Discriminator loss: 0.695391, acc: 0.464844]  [Adversarial loss: 0.338543, acc: 0.914062]\n",
            "559: [Discriminator loss: 0.643584, acc: 0.542969]  [Adversarial loss: 0.296732, acc: 0.949219]\n",
            "560: [Discriminator loss: 0.610153, acc: 0.511719]  [Adversarial loss: 0.283564, acc: 0.964844]\n",
            "561: [Discriminator loss: 0.537637, acc: 0.578125]  [Adversarial loss: 0.265214, acc: 0.945312]\n",
            "562: [Discriminator loss: 0.646189, acc: 0.503906]  [Adversarial loss: 0.296624, acc: 0.929688]\n",
            "563: [Discriminator loss: 0.635204, acc: 0.511719]  [Adversarial loss: 0.333986, acc: 0.910156]\n",
            "564: [Discriminator loss: 0.617415, acc: 0.535156]  [Adversarial loss: 0.293964, acc: 0.929688]\n",
            "565: [Discriminator loss: 0.596727, acc: 0.535156]  [Adversarial loss: 0.278487, acc: 0.957031]\n",
            "566: [Discriminator loss: 0.638581, acc: 0.496094]  [Adversarial loss: 0.312892, acc: 0.941406]\n",
            "567: [Discriminator loss: 0.592069, acc: 0.542969]  [Adversarial loss: 0.315945, acc: 0.933594]\n",
            "568: [Discriminator loss: 0.574345, acc: 0.531250]  [Adversarial loss: 0.292855, acc: 0.957031]\n",
            "569: [Discriminator loss: 0.648405, acc: 0.507812]  [Adversarial loss: 0.329998, acc: 0.941406]\n",
            "570: [Discriminator loss: 0.586156, acc: 0.515625]  [Adversarial loss: 0.281074, acc: 0.972656]\n",
            "571: [Discriminator loss: 0.623860, acc: 0.500000]  [Adversarial loss: 0.308002, acc: 0.937500]\n",
            "572: [Discriminator loss: 0.657940, acc: 0.503906]  [Adversarial loss: 0.289640, acc: 0.953125]\n",
            "573: [Discriminator loss: 0.627415, acc: 0.527344]  [Adversarial loss: 0.304722, acc: 0.945312]\n",
            "574: [Discriminator loss: 0.562092, acc: 0.554688]  [Adversarial loss: 0.302992, acc: 0.921875]\n",
            "575: [Discriminator loss: 0.618389, acc: 0.531250]  [Adversarial loss: 0.313049, acc: 0.941406]\n",
            "576: [Discriminator loss: 0.608377, acc: 0.492188]  [Adversarial loss: 0.320872, acc: 0.925781]\n",
            "577: [Discriminator loss: 0.655257, acc: 0.488281]  [Adversarial loss: 0.321167, acc: 0.941406]\n",
            "578: [Discriminator loss: 0.615355, acc: 0.554688]  [Adversarial loss: 0.284254, acc: 0.964844]\n",
            "579: [Discriminator loss: 0.597581, acc: 0.539062]  [Adversarial loss: 0.292548, acc: 0.945312]\n",
            "580: [Discriminator loss: 0.649707, acc: 0.496094]  [Adversarial loss: 0.307241, acc: 0.957031]\n",
            "581: [Discriminator loss: 0.741298, acc: 0.445312]  [Adversarial loss: 0.331563, acc: 0.964844]\n",
            "582: [Discriminator loss: 0.573823, acc: 0.554688]  [Adversarial loss: 0.257987, acc: 0.960938]\n",
            "583: [Discriminator loss: 0.641094, acc: 0.531250]  [Adversarial loss: 0.289853, acc: 0.933594]\n",
            "584: [Discriminator loss: 0.631844, acc: 0.500000]  [Adversarial loss: 0.311525, acc: 0.953125]\n",
            "585: [Discriminator loss: 0.642300, acc: 0.500000]  [Adversarial loss: 0.328137, acc: 0.957031]\n",
            "586: [Discriminator loss: 0.618010, acc: 0.503906]  [Adversarial loss: 0.325086, acc: 0.953125]\n",
            "587: [Discriminator loss: 0.597675, acc: 0.531250]  [Adversarial loss: 0.323238, acc: 0.937500]\n",
            "588: [Discriminator loss: 0.592212, acc: 0.550781]  [Adversarial loss: 0.288410, acc: 0.945312]\n",
            "589: [Discriminator loss: 0.532787, acc: 0.562500]  [Adversarial loss: 0.301941, acc: 0.921875]\n",
            "590: [Discriminator loss: 0.565687, acc: 0.593750]  [Adversarial loss: 0.255664, acc: 0.960938]\n",
            "591: [Discriminator loss: 0.600343, acc: 0.550781]  [Adversarial loss: 0.263189, acc: 0.972656]\n",
            "592: [Discriminator loss: 0.581539, acc: 0.546875]  [Adversarial loss: 0.289393, acc: 0.960938]\n",
            "593: [Discriminator loss: 0.618224, acc: 0.496094]  [Adversarial loss: 0.315881, acc: 0.968750]\n",
            "594: [Discriminator loss: 0.593121, acc: 0.511719]  [Adversarial loss: 0.294246, acc: 0.953125]\n",
            "595: [Discriminator loss: 0.617121, acc: 0.503906]  [Adversarial loss: 0.325169, acc: 0.906250]\n",
            "596: [Discriminator loss: 0.639618, acc: 0.531250]  [Adversarial loss: 0.279007, acc: 0.960938]\n",
            "597: [Discriminator loss: 0.628607, acc: 0.542969]  [Adversarial loss: 0.287111, acc: 0.941406]\n",
            "598: [Discriminator loss: 0.670106, acc: 0.496094]  [Adversarial loss: 0.314143, acc: 0.925781]\n",
            "599: [Discriminator loss: 0.572871, acc: 0.550781]  [Adversarial loss: 0.290608, acc: 0.949219]\n",
            "600: [Discriminator loss: 0.624176, acc: 0.527344]  [Adversarial loss: 0.316320, acc: 0.949219]\n",
            "601: [Discriminator loss: 0.543209, acc: 0.546875]  [Adversarial loss: 0.289698, acc: 0.953125]\n",
            "602: [Discriminator loss: 0.637563, acc: 0.515625]  [Adversarial loss: 0.333056, acc: 0.898438]\n",
            "603: [Discriminator loss: 0.575128, acc: 0.554688]  [Adversarial loss: 0.287627, acc: 0.937500]\n",
            "604: [Discriminator loss: 0.618727, acc: 0.496094]  [Adversarial loss: 0.323148, acc: 0.917969]\n",
            "605: [Discriminator loss: 0.575533, acc: 0.515625]  [Adversarial loss: 0.342025, acc: 0.921875]\n",
            "606: [Discriminator loss: 0.595477, acc: 0.472656]  [Adversarial loss: 0.330511, acc: 0.941406]\n",
            "607: [Discriminator loss: 0.638484, acc: 0.472656]  [Adversarial loss: 0.347539, acc: 0.953125]\n",
            "608: [Discriminator loss: 0.653288, acc: 0.480469]  [Adversarial loss: 0.338204, acc: 0.910156]\n",
            "609: [Discriminator loss: 0.630134, acc: 0.507812]  [Adversarial loss: 0.329675, acc: 0.917969]\n",
            "610: [Discriminator loss: 0.604028, acc: 0.527344]  [Adversarial loss: 0.322801, acc: 0.960938]\n",
            "611: [Discriminator loss: 0.652325, acc: 0.488281]  [Adversarial loss: 0.330803, acc: 0.953125]\n",
            "612: [Discriminator loss: 0.601493, acc: 0.484375]  [Adversarial loss: 0.349085, acc: 0.953125]\n",
            "613: [Discriminator loss: 0.623271, acc: 0.496094]  [Adversarial loss: 0.306642, acc: 0.945312]\n",
            "614: [Discriminator loss: 0.640994, acc: 0.460938]  [Adversarial loss: 0.345795, acc: 0.914062]\n",
            "615: [Discriminator loss: 0.581539, acc: 0.539062]  [Adversarial loss: 0.271641, acc: 0.968750]\n",
            "616: [Discriminator loss: 0.631894, acc: 0.507812]  [Adversarial loss: 0.313768, acc: 0.941406]\n",
            "617: [Discriminator loss: 0.605150, acc: 0.535156]  [Adversarial loss: 0.298876, acc: 0.957031]\n",
            "618: [Discriminator loss: 0.658795, acc: 0.457031]  [Adversarial loss: 0.334636, acc: 0.941406]\n",
            "619: [Discriminator loss: 0.572846, acc: 0.554688]  [Adversarial loss: 0.276022, acc: 0.941406]\n",
            "620: [Discriminator loss: 0.641141, acc: 0.484375]  [Adversarial loss: 0.313006, acc: 0.968750]\n",
            "621: [Discriminator loss: 0.647033, acc: 0.511719]  [Adversarial loss: 0.334272, acc: 0.941406]\n",
            "622: [Discriminator loss: 0.588993, acc: 0.527344]  [Adversarial loss: 0.326572, acc: 0.937500]\n",
            "623: [Discriminator loss: 0.589199, acc: 0.542969]  [Adversarial loss: 0.305363, acc: 0.933594]\n",
            "624: [Discriminator loss: 0.611924, acc: 0.507812]  [Adversarial loss: 0.353889, acc: 0.917969]\n",
            "625: [Discriminator loss: 0.650302, acc: 0.457031]  [Adversarial loss: 0.372058, acc: 0.929688]\n",
            "626: [Discriminator loss: 0.584810, acc: 0.546875]  [Adversarial loss: 0.299929, acc: 0.953125]\n",
            "627: [Discriminator loss: 0.628346, acc: 0.500000]  [Adversarial loss: 0.289997, acc: 0.964844]\n",
            "628: [Discriminator loss: 0.619803, acc: 0.480469]  [Adversarial loss: 0.337910, acc: 0.949219]\n",
            "629: [Discriminator loss: 0.621607, acc: 0.511719]  [Adversarial loss: 0.334666, acc: 0.921875]\n",
            "630: [Discriminator loss: 0.619129, acc: 0.519531]  [Adversarial loss: 0.305521, acc: 0.945312]\n",
            "631: [Discriminator loss: 0.602659, acc: 0.527344]  [Adversarial loss: 0.281653, acc: 0.960938]\n",
            "632: [Discriminator loss: 0.619209, acc: 0.503906]  [Adversarial loss: 0.306718, acc: 0.945312]\n",
            "633: [Discriminator loss: 0.698930, acc: 0.511719]  [Adversarial loss: 0.289192, acc: 0.964844]\n",
            "634: [Discriminator loss: 0.579127, acc: 0.546875]  [Adversarial loss: 0.291540, acc: 0.964844]\n",
            "635: [Discriminator loss: 0.602394, acc: 0.554688]  [Adversarial loss: 0.305119, acc: 0.949219]\n",
            "636: [Discriminator loss: 0.535890, acc: 0.582031]  [Adversarial loss: 0.300157, acc: 0.941406]\n",
            "637: [Discriminator loss: 0.551679, acc: 0.558594]  [Adversarial loss: 0.275770, acc: 0.972656]\n",
            "638: [Discriminator loss: 0.589731, acc: 0.476562]  [Adversarial loss: 0.330228, acc: 0.941406]\n",
            "639: [Discriminator loss: 0.595170, acc: 0.511719]  [Adversarial loss: 0.315934, acc: 0.937500]\n",
            "640: [Discriminator loss: 0.674923, acc: 0.476562]  [Adversarial loss: 0.324257, acc: 0.945312]\n",
            "641: [Discriminator loss: 0.626317, acc: 0.531250]  [Adversarial loss: 0.299270, acc: 0.953125]\n",
            "642: [Discriminator loss: 0.564072, acc: 0.558594]  [Adversarial loss: 0.311649, acc: 0.929688]\n",
            "643: [Discriminator loss: 0.507209, acc: 0.550781]  [Adversarial loss: 0.290979, acc: 0.960938]\n",
            "644: [Discriminator loss: 0.635456, acc: 0.554688]  [Adversarial loss: 0.285477, acc: 0.941406]\n",
            "645: [Discriminator loss: 0.668686, acc: 0.492188]  [Adversarial loss: 0.340105, acc: 0.960938]\n",
            "646: [Discriminator loss: 0.620616, acc: 0.488281]  [Adversarial loss: 0.339494, acc: 0.937500]\n",
            "647: [Discriminator loss: 0.606684, acc: 0.542969]  [Adversarial loss: 0.394468, acc: 0.863281]\n",
            "648: [Discriminator loss: 0.593172, acc: 0.523438]  [Adversarial loss: 0.347684, acc: 0.906250]\n",
            "649: [Discriminator loss: 0.604923, acc: 0.531250]  [Adversarial loss: 0.309722, acc: 0.925781]\n",
            "650: [Discriminator loss: 0.618034, acc: 0.480469]  [Adversarial loss: 0.340565, acc: 0.949219]\n",
            "651: [Discriminator loss: 0.622622, acc: 0.519531]  [Adversarial loss: 0.338594, acc: 0.894531]\n",
            "652: [Discriminator loss: 0.550826, acc: 0.527344]  [Adversarial loss: 0.331613, acc: 0.937500]\n",
            "653: [Discriminator loss: 0.558447, acc: 0.527344]  [Adversarial loss: 0.308948, acc: 0.945312]\n",
            "654: [Discriminator loss: 0.597744, acc: 0.511719]  [Adversarial loss: 0.315894, acc: 0.941406]\n",
            "655: [Discriminator loss: 0.577655, acc: 0.535156]  [Adversarial loss: 0.352252, acc: 0.910156]\n",
            "656: [Discriminator loss: 0.554264, acc: 0.566406]  [Adversarial loss: 0.286388, acc: 0.949219]\n",
            "657: [Discriminator loss: 0.588232, acc: 0.515625]  [Adversarial loss: 0.346037, acc: 0.898438]\n",
            "658: [Discriminator loss: 0.598638, acc: 0.511719]  [Adversarial loss: 0.331461, acc: 0.925781]\n",
            "659: [Discriminator loss: 0.573641, acc: 0.570312]  [Adversarial loss: 0.302624, acc: 0.925781]\n",
            "660: [Discriminator loss: 0.597114, acc: 0.566406]  [Adversarial loss: 0.275190, acc: 0.941406]\n",
            "661: [Discriminator loss: 0.702953, acc: 0.429688]  [Adversarial loss: 0.345103, acc: 0.921875]\n",
            "662: [Discriminator loss: 0.629855, acc: 0.511719]  [Adversarial loss: 0.305685, acc: 0.949219]\n",
            "663: [Discriminator loss: 0.616441, acc: 0.523438]  [Adversarial loss: 0.280552, acc: 0.953125]\n",
            "664: [Discriminator loss: 0.641690, acc: 0.480469]  [Adversarial loss: 0.317373, acc: 0.957031]\n",
            "665: [Discriminator loss: 0.788623, acc: 0.437500]  [Adversarial loss: 0.337850, acc: 0.933594]\n",
            "666: [Discriminator loss: 0.561409, acc: 0.539062]  [Adversarial loss: 0.276809, acc: 0.949219]\n",
            "667: [Discriminator loss: 0.635202, acc: 0.488281]  [Adversarial loss: 0.333182, acc: 0.929688]\n",
            "668: [Discriminator loss: 0.571138, acc: 0.523438]  [Adversarial loss: 0.295016, acc: 0.976562]\n",
            "669: [Discriminator loss: 0.651770, acc: 0.472656]  [Adversarial loss: 0.327314, acc: 0.945312]\n",
            "670: [Discriminator loss: 0.653541, acc: 0.476562]  [Adversarial loss: 0.328015, acc: 0.972656]\n",
            "671: [Discriminator loss: 0.649513, acc: 0.480469]  [Adversarial loss: 0.361337, acc: 0.914062]\n",
            "672: [Discriminator loss: 0.613097, acc: 0.492188]  [Adversarial loss: 0.333184, acc: 0.945312]\n",
            "673: [Discriminator loss: 0.589114, acc: 0.496094]  [Adversarial loss: 0.302303, acc: 0.960938]\n",
            "674: [Discriminator loss: 0.568101, acc: 0.546875]  [Adversarial loss: 0.284636, acc: 0.960938]\n",
            "675: [Discriminator loss: 0.617612, acc: 0.472656]  [Adversarial loss: 0.331142, acc: 0.929688]\n",
            "676: [Discriminator loss: 0.602669, acc: 0.531250]  [Adversarial loss: 0.308879, acc: 0.941406]\n",
            "677: [Discriminator loss: 0.596510, acc: 0.515625]  [Adversarial loss: 0.290833, acc: 0.949219]\n",
            "678: [Discriminator loss: 0.566287, acc: 0.550781]  [Adversarial loss: 0.302461, acc: 0.972656]\n",
            "679: [Discriminator loss: 0.564447, acc: 0.492188]  [Adversarial loss: 0.339846, acc: 0.910156]\n",
            "680: [Discriminator loss: 0.634456, acc: 0.492188]  [Adversarial loss: 0.295652, acc: 0.957031]\n",
            "681: [Discriminator loss: 0.650368, acc: 0.476562]  [Adversarial loss: 0.340865, acc: 0.937500]\n",
            "682: [Discriminator loss: 0.599325, acc: 0.570312]  [Adversarial loss: 0.259241, acc: 0.964844]\n",
            "683: [Discriminator loss: 0.556949, acc: 0.546875]  [Adversarial loss: 0.266994, acc: 0.964844]\n",
            "684: [Discriminator loss: 0.705124, acc: 0.472656]  [Adversarial loss: 0.329029, acc: 0.949219]\n",
            "685: [Discriminator loss: 0.621130, acc: 0.539062]  [Adversarial loss: 0.311612, acc: 0.964844]\n",
            "686: [Discriminator loss: 0.621895, acc: 0.503906]  [Adversarial loss: 0.313462, acc: 0.960938]\n",
            "687: [Discriminator loss: 0.628591, acc: 0.535156]  [Adversarial loss: 0.292892, acc: 0.953125]\n",
            "688: [Discriminator loss: 0.579942, acc: 0.523438]  [Adversarial loss: 0.304416, acc: 0.925781]\n",
            "689: [Discriminator loss: 0.577622, acc: 0.531250]  [Adversarial loss: 0.310842, acc: 0.941406]\n",
            "690: [Discriminator loss: 0.529231, acc: 0.570312]  [Adversarial loss: 0.292166, acc: 0.945312]\n",
            "691: [Discriminator loss: 0.598115, acc: 0.523438]  [Adversarial loss: 0.322278, acc: 0.953125]\n",
            "692: [Discriminator loss: 0.626839, acc: 0.546875]  [Adversarial loss: 0.289708, acc: 0.957031]\n",
            "693: [Discriminator loss: 0.628995, acc: 0.472656]  [Adversarial loss: 0.342888, acc: 0.960938]\n",
            "694: [Discriminator loss: 0.581906, acc: 0.488281]  [Adversarial loss: 0.328289, acc: 0.941406]\n",
            "695: [Discriminator loss: 0.584485, acc: 0.511719]  [Adversarial loss: 0.313078, acc: 0.933594]\n",
            "696: [Discriminator loss: 0.566320, acc: 0.562500]  [Adversarial loss: 0.281798, acc: 0.937500]\n",
            "697: [Discriminator loss: 0.572080, acc: 0.531250]  [Adversarial loss: 0.293775, acc: 0.941406]\n",
            "698: [Discriminator loss: 0.621957, acc: 0.484375]  [Adversarial loss: 0.316091, acc: 0.960938]\n",
            "699: [Discriminator loss: 0.660948, acc: 0.492188]  [Adversarial loss: 0.318787, acc: 0.953125]\n",
            "700: [Discriminator loss: 0.641699, acc: 0.453125]  [Adversarial loss: 0.338157, acc: 0.949219]\n",
            "701: [Discriminator loss: 0.625515, acc: 0.484375]  [Adversarial loss: 0.347025, acc: 0.960938]\n",
            "702: [Discriminator loss: 0.607664, acc: 0.531250]  [Adversarial loss: 0.274693, acc: 0.960938]\n",
            "703: [Discriminator loss: 0.622776, acc: 0.527344]  [Adversarial loss: 0.274383, acc: 0.964844]\n",
            "704: [Discriminator loss: 0.560363, acc: 0.539062]  [Adversarial loss: 0.280932, acc: 0.960938]\n",
            "705: [Discriminator loss: 0.541560, acc: 0.558594]  [Adversarial loss: 0.285166, acc: 0.953125]\n",
            "706: [Discriminator loss: 0.646974, acc: 0.519531]  [Adversarial loss: 0.315819, acc: 0.917969]\n",
            "707: [Discriminator loss: 0.623348, acc: 0.542969]  [Adversarial loss: 0.277865, acc: 0.953125]\n",
            "708: [Discriminator loss: 0.588030, acc: 0.531250]  [Adversarial loss: 0.285902, acc: 0.953125]\n",
            "709: [Discriminator loss: 0.580819, acc: 0.511719]  [Adversarial loss: 0.305721, acc: 0.960938]\n",
            "710: [Discriminator loss: 0.618315, acc: 0.484375]  [Adversarial loss: 0.305420, acc: 0.968750]\n",
            "711: [Discriminator loss: 0.523128, acc: 0.593750]  [Adversarial loss: 0.303905, acc: 0.953125]\n",
            "712: [Discriminator loss: 0.587494, acc: 0.500000]  [Adversarial loss: 0.327739, acc: 0.964844]\n",
            "713: [Discriminator loss: 0.573017, acc: 0.578125]  [Adversarial loss: 0.258602, acc: 0.945312]\n",
            "714: [Discriminator loss: 0.637974, acc: 0.488281]  [Adversarial loss: 0.302572, acc: 0.976562]\n",
            "715: [Discriminator loss: 0.581084, acc: 0.523438]  [Adversarial loss: 0.299046, acc: 0.949219]\n",
            "716: [Discriminator loss: 0.597070, acc: 0.468750]  [Adversarial loss: 0.314654, acc: 0.945312]\n",
            "717: [Discriminator loss: 0.622921, acc: 0.500000]  [Adversarial loss: 0.333661, acc: 0.941406]\n",
            "718: [Discriminator loss: 0.592351, acc: 0.484375]  [Adversarial loss: 0.311278, acc: 0.957031]\n",
            "719: [Discriminator loss: 0.542902, acc: 0.589844]  [Adversarial loss: 0.286429, acc: 0.957031]\n",
            "720: [Discriminator loss: 0.603747, acc: 0.523438]  [Adversarial loss: 0.292879, acc: 0.953125]\n",
            "721: [Discriminator loss: 0.614517, acc: 0.511719]  [Adversarial loss: 0.292014, acc: 0.984375]\n",
            "722: [Discriminator loss: 0.599071, acc: 0.539062]  [Adversarial loss: 0.292547, acc: 0.945312]\n",
            "723: [Discriminator loss: 0.563419, acc: 0.531250]  [Adversarial loss: 0.294846, acc: 0.960938]\n",
            "724: [Discriminator loss: 0.685570, acc: 0.453125]  [Adversarial loss: 0.345037, acc: 0.937500]\n",
            "725: [Discriminator loss: 0.604738, acc: 0.488281]  [Adversarial loss: 0.320311, acc: 0.960938]\n",
            "726: [Discriminator loss: 0.617254, acc: 0.519531]  [Adversarial loss: 0.304236, acc: 0.953125]\n",
            "727: [Discriminator loss: 0.567759, acc: 0.531250]  [Adversarial loss: 0.312497, acc: 0.933594]\n",
            "728: [Discriminator loss: 0.635589, acc: 0.519531]  [Adversarial loss: 0.326478, acc: 0.964844]\n",
            "729: [Discriminator loss: 0.600145, acc: 0.542969]  [Adversarial loss: 0.292047, acc: 0.960938]\n",
            "730: [Discriminator loss: 0.661701, acc: 0.476562]  [Adversarial loss: 0.303901, acc: 0.968750]\n",
            "731: [Discriminator loss: 0.558290, acc: 0.523438]  [Adversarial loss: 0.289742, acc: 0.964844]\n",
            "732: [Discriminator loss: 0.567219, acc: 0.554688]  [Adversarial loss: 0.280503, acc: 0.964844]\n",
            "733: [Discriminator loss: 0.599207, acc: 0.539062]  [Adversarial loss: 0.300862, acc: 0.957031]\n",
            "734: [Discriminator loss: 0.612774, acc: 0.500000]  [Adversarial loss: 0.329131, acc: 0.953125]\n",
            "735: [Discriminator loss: 0.565876, acc: 0.546875]  [Adversarial loss: 0.306566, acc: 0.933594]\n",
            "736: [Discriminator loss: 0.590524, acc: 0.468750]  [Adversarial loss: 0.332694, acc: 0.949219]\n",
            "737: [Discriminator loss: 0.560353, acc: 0.574219]  [Adversarial loss: 0.254714, acc: 0.964844]\n",
            "738: [Discriminator loss: 0.576235, acc: 0.507812]  [Adversarial loss: 0.314647, acc: 0.929688]\n",
            "739: [Discriminator loss: 0.584219, acc: 0.507812]  [Adversarial loss: 0.306006, acc: 0.949219]\n",
            "740: [Discriminator loss: 0.520803, acc: 0.582031]  [Adversarial loss: 0.260178, acc: 0.957031]\n",
            "741: [Discriminator loss: 0.673106, acc: 0.488281]  [Adversarial loss: 0.351098, acc: 0.933594]\n",
            "742: [Discriminator loss: 0.639674, acc: 0.515625]  [Adversarial loss: 0.291161, acc: 0.972656]\n",
            "743: [Discriminator loss: 0.655513, acc: 0.566406]  [Adversarial loss: 0.315517, acc: 0.921875]\n",
            "744: [Discriminator loss: 0.575654, acc: 0.570312]  [Adversarial loss: 0.264607, acc: 0.949219]\n",
            "745: [Discriminator loss: 0.685692, acc: 0.511719]  [Adversarial loss: 0.301352, acc: 0.957031]\n",
            "746: [Discriminator loss: 0.669650, acc: 0.472656]  [Adversarial loss: 0.323762, acc: 0.941406]\n",
            "747: [Discriminator loss: 0.675293, acc: 0.496094]  [Adversarial loss: 0.335082, acc: 0.917969]\n",
            "748: [Discriminator loss: 0.579907, acc: 0.519531]  [Adversarial loss: 0.315725, acc: 0.941406]\n",
            "749: [Discriminator loss: 0.607498, acc: 0.492188]  [Adversarial loss: 0.329189, acc: 0.968750]\n",
            "750: [Discriminator loss: 0.562802, acc: 0.539062]  [Adversarial loss: 0.315562, acc: 0.941406]\n",
            "751: [Discriminator loss: 0.539896, acc: 0.578125]  [Adversarial loss: 0.277544, acc: 0.949219]\n",
            "752: [Discriminator loss: 0.554976, acc: 0.460938]  [Adversarial loss: 0.338316, acc: 0.953125]\n",
            "753: [Discriminator loss: 0.581752, acc: 0.546875]  [Adversarial loss: 0.299590, acc: 0.960938]\n",
            "754: [Discriminator loss: 0.591339, acc: 0.527344]  [Adversarial loss: 0.310203, acc: 0.957031]\n",
            "755: [Discriminator loss: 0.599317, acc: 0.531250]  [Adversarial loss: 0.299546, acc: 0.960938]\n",
            "756: [Discriminator loss: 0.550357, acc: 0.539062]  [Adversarial loss: 0.300908, acc: 0.937500]\n",
            "757: [Discriminator loss: 0.560773, acc: 0.523438]  [Adversarial loss: 0.287838, acc: 0.960938]\n",
            "758: [Discriminator loss: 0.618503, acc: 0.457031]  [Adversarial loss: 0.330086, acc: 0.949219]\n",
            "759: [Discriminator loss: 0.602744, acc: 0.500000]  [Adversarial loss: 0.305215, acc: 0.945312]\n",
            "760: [Discriminator loss: 0.557726, acc: 0.550781]  [Adversarial loss: 0.295602, acc: 0.929688]\n",
            "761: [Discriminator loss: 0.655420, acc: 0.476562]  [Adversarial loss: 0.308434, acc: 0.980469]\n",
            "762: [Discriminator loss: 0.611076, acc: 0.539062]  [Adversarial loss: 0.304268, acc: 0.929688]\n",
            "763: [Discriminator loss: 0.689949, acc: 0.523438]  [Adversarial loss: 0.302162, acc: 0.945312]\n",
            "764: [Discriminator loss: 0.737708, acc: 0.492188]  [Adversarial loss: 0.333252, acc: 0.921875]\n",
            "765: [Discriminator loss: 0.609723, acc: 0.527344]  [Adversarial loss: 0.294413, acc: 0.953125]\n",
            "766: [Discriminator loss: 0.646280, acc: 0.488281]  [Adversarial loss: 0.312872, acc: 0.953125]\n",
            "767: [Discriminator loss: 0.613361, acc: 0.472656]  [Adversarial loss: 0.317504, acc: 0.945312]\n",
            "768: [Discriminator loss: 0.581738, acc: 0.507812]  [Adversarial loss: 0.320463, acc: 0.960938]\n",
            "769: [Discriminator loss: 0.555936, acc: 0.558594]  [Adversarial loss: 0.268452, acc: 0.953125]\n",
            "770: [Discriminator loss: 0.622435, acc: 0.503906]  [Adversarial loss: 0.296996, acc: 0.968750]\n",
            "771: [Discriminator loss: 0.536224, acc: 0.562500]  [Adversarial loss: 0.282796, acc: 0.957031]\n",
            "772: [Discriminator loss: 0.671697, acc: 0.519531]  [Adversarial loss: 0.308695, acc: 0.953125]\n",
            "773: [Discriminator loss: 0.561451, acc: 0.601562]  [Adversarial loss: 0.259968, acc: 0.945312]\n",
            "774: [Discriminator loss: 0.599294, acc: 0.515625]  [Adversarial loss: 0.301034, acc: 0.964844]\n",
            "775: [Discriminator loss: 0.588901, acc: 0.503906]  [Adversarial loss: 0.326949, acc: 0.945312]\n",
            "776: [Discriminator loss: 0.605981, acc: 0.511719]  [Adversarial loss: 0.306478, acc: 0.968750]\n",
            "777: [Discriminator loss: 0.613133, acc: 0.476562]  [Adversarial loss: 0.328062, acc: 0.960938]\n",
            "778: [Discriminator loss: 0.639419, acc: 0.496094]  [Adversarial loss: 0.316150, acc: 0.945312]\n",
            "779: [Discriminator loss: 0.538191, acc: 0.539062]  [Adversarial loss: 0.324662, acc: 0.925781]\n",
            "780: [Discriminator loss: 0.597604, acc: 0.453125]  [Adversarial loss: 0.354514, acc: 0.933594]\n",
            "781: [Discriminator loss: 0.635570, acc: 0.464844]  [Adversarial loss: 0.337143, acc: 0.953125]\n",
            "782: [Discriminator loss: 0.618726, acc: 0.515625]  [Adversarial loss: 0.325944, acc: 0.937500]\n",
            "783: [Discriminator loss: 0.595603, acc: 0.519531]  [Adversarial loss: 0.310893, acc: 0.937500]\n",
            "784: [Discriminator loss: 0.578870, acc: 0.519531]  [Adversarial loss: 0.320013, acc: 0.933594]\n",
            "785: [Discriminator loss: 0.615209, acc: 0.500000]  [Adversarial loss: 0.343048, acc: 0.941406]\n",
            "786: [Discriminator loss: 0.540532, acc: 0.554688]  [Adversarial loss: 0.309797, acc: 0.921875]\n",
            "787: [Discriminator loss: 0.553918, acc: 0.511719]  [Adversarial loss: 0.310648, acc: 0.960938]\n",
            "788: [Discriminator loss: 0.674394, acc: 0.488281]  [Adversarial loss: 0.329107, acc: 0.937500]\n",
            "789: [Discriminator loss: 0.661673, acc: 0.464844]  [Adversarial loss: 0.329902, acc: 0.957031]\n",
            "790: [Discriminator loss: 0.538786, acc: 0.515625]  [Adversarial loss: 0.315539, acc: 0.968750]\n",
            "791: [Discriminator loss: 0.624424, acc: 0.500000]  [Adversarial loss: 0.376168, acc: 0.953125]\n",
            "792: [Discriminator loss: 0.609498, acc: 0.515625]  [Adversarial loss: 0.320738, acc: 0.937500]\n",
            "793: [Discriminator loss: 0.643035, acc: 0.511719]  [Adversarial loss: 0.314404, acc: 0.941406]\n",
            "794: [Discriminator loss: 0.652968, acc: 0.496094]  [Adversarial loss: 0.321446, acc: 0.933594]\n",
            "795: [Discriminator loss: 0.606071, acc: 0.484375]  [Adversarial loss: 0.319988, acc: 0.953125]\n",
            "796: [Discriminator loss: 0.580202, acc: 0.519531]  [Adversarial loss: 0.308836, acc: 0.953125]\n",
            "797: [Discriminator loss: 0.629674, acc: 0.503906]  [Adversarial loss: 0.296752, acc: 0.964844]\n",
            "798: [Discriminator loss: 0.615803, acc: 0.519531]  [Adversarial loss: 0.312246, acc: 0.945312]\n",
            "799: [Discriminator loss: 0.498285, acc: 0.539062]  [Adversarial loss: 0.292295, acc: 0.953125]\n",
            "800: [Discriminator loss: 0.685637, acc: 0.464844]  [Adversarial loss: 0.342391, acc: 0.933594]\n",
            "801: [Discriminator loss: 0.565900, acc: 0.539062]  [Adversarial loss: 0.311528, acc: 0.937500]\n",
            "802: [Discriminator loss: 0.652056, acc: 0.519531]  [Adversarial loss: 0.302590, acc: 0.949219]\n",
            "803: [Discriminator loss: 0.616883, acc: 0.511719]  [Adversarial loss: 0.289544, acc: 0.960938]\n",
            "804: [Discriminator loss: 0.565874, acc: 0.535156]  [Adversarial loss: 0.296852, acc: 0.957031]\n",
            "805: [Discriminator loss: 0.584376, acc: 0.527344]  [Adversarial loss: 0.309629, acc: 0.941406]\n",
            "806: [Discriminator loss: 0.536110, acc: 0.570312]  [Adversarial loss: 0.275115, acc: 0.953125]\n",
            "807: [Discriminator loss: 0.526418, acc: 0.519531]  [Adversarial loss: 0.300271, acc: 0.968750]\n",
            "808: [Discriminator loss: 0.630358, acc: 0.539062]  [Adversarial loss: 0.271960, acc: 0.960938]\n",
            "809: [Discriminator loss: 0.547410, acc: 0.558594]  [Adversarial loss: 0.302205, acc: 0.937500]\n",
            "810: [Discriminator loss: 0.596655, acc: 0.507812]  [Adversarial loss: 0.311225, acc: 0.941406]\n",
            "811: [Discriminator loss: 0.519526, acc: 0.574219]  [Adversarial loss: 0.296443, acc: 0.929688]\n",
            "812: [Discriminator loss: 0.683565, acc: 0.464844]  [Adversarial loss: 0.323274, acc: 0.980469]\n",
            "813: [Discriminator loss: 0.609203, acc: 0.519531]  [Adversarial loss: 0.304871, acc: 0.960938]\n",
            "814: [Discriminator loss: 0.607867, acc: 0.535156]  [Adversarial loss: 0.281067, acc: 0.964844]\n",
            "815: [Discriminator loss: 0.615440, acc: 0.480469]  [Adversarial loss: 0.318026, acc: 0.968750]\n",
            "816: [Discriminator loss: 0.572449, acc: 0.496094]  [Adversarial loss: 0.321016, acc: 0.980469]\n",
            "817: [Discriminator loss: 0.564528, acc: 0.535156]  [Adversarial loss: 0.315762, acc: 0.925781]\n",
            "818: [Discriminator loss: 0.671930, acc: 0.507812]  [Adversarial loss: 0.331152, acc: 0.933594]\n",
            "819: [Discriminator loss: 0.622223, acc: 0.472656]  [Adversarial loss: 0.318883, acc: 0.949219]\n",
            "820: [Discriminator loss: 0.617642, acc: 0.472656]  [Adversarial loss: 0.345388, acc: 0.921875]\n",
            "821: [Discriminator loss: 0.622121, acc: 0.484375]  [Adversarial loss: 0.329426, acc: 0.933594]\n",
            "822: [Discriminator loss: 0.598461, acc: 0.507812]  [Adversarial loss: 0.331591, acc: 0.933594]\n",
            "823: [Discriminator loss: 0.533085, acc: 0.539062]  [Adversarial loss: 0.303171, acc: 0.960938]\n",
            "824: [Discriminator loss: 0.621673, acc: 0.511719]  [Adversarial loss: 0.312266, acc: 0.921875]\n",
            "825: [Discriminator loss: 0.608494, acc: 0.531250]  [Adversarial loss: 0.315844, acc: 0.902344]\n",
            "826: [Discriminator loss: 0.599750, acc: 0.539062]  [Adversarial loss: 0.304186, acc: 0.925781]\n",
            "827: [Discriminator loss: 0.556245, acc: 0.503906]  [Adversarial loss: 0.339110, acc: 0.914062]\n",
            "828: [Discriminator loss: 0.555148, acc: 0.546875]  [Adversarial loss: 0.314121, acc: 0.929688]\n",
            "829: [Discriminator loss: 0.651850, acc: 0.476562]  [Adversarial loss: 0.317744, acc: 0.937500]\n",
            "830: [Discriminator loss: 0.597262, acc: 0.476562]  [Adversarial loss: 0.308081, acc: 0.968750]\n",
            "831: [Discriminator loss: 0.589093, acc: 0.484375]  [Adversarial loss: 0.320671, acc: 0.937500]\n",
            "832: [Discriminator loss: 0.563205, acc: 0.566406]  [Adversarial loss: 0.281493, acc: 0.929688]\n",
            "833: [Discriminator loss: 0.657738, acc: 0.500000]  [Adversarial loss: 0.293041, acc: 0.957031]\n",
            "834: [Discriminator loss: 0.641360, acc: 0.523438]  [Adversarial loss: 0.306567, acc: 0.929688]\n",
            "835: [Discriminator loss: 0.576934, acc: 0.531250]  [Adversarial loss: 0.296637, acc: 0.945312]\n",
            "836: [Discriminator loss: 0.520374, acc: 0.546875]  [Adversarial loss: 0.288516, acc: 0.949219]\n",
            "837: [Discriminator loss: 0.629029, acc: 0.515625]  [Adversarial loss: 0.305882, acc: 0.945312]\n",
            "838: [Discriminator loss: 0.683259, acc: 0.445312]  [Adversarial loss: 0.325951, acc: 0.945312]\n",
            "839: [Discriminator loss: 0.577786, acc: 0.531250]  [Adversarial loss: 0.308024, acc: 0.945312]\n",
            "840: [Discriminator loss: 0.650249, acc: 0.464844]  [Adversarial loss: 0.321869, acc: 0.972656]\n",
            "841: [Discriminator loss: 0.511392, acc: 0.574219]  [Adversarial loss: 0.262957, acc: 0.968750]\n",
            "842: [Discriminator loss: 0.585470, acc: 0.519531]  [Adversarial loss: 0.286955, acc: 0.960938]\n",
            "843: [Discriminator loss: 0.551463, acc: 0.554688]  [Adversarial loss: 0.280059, acc: 0.964844]\n",
            "844: [Discriminator loss: 0.579746, acc: 0.535156]  [Adversarial loss: 0.287674, acc: 0.949219]\n",
            "845: [Discriminator loss: 0.646613, acc: 0.511719]  [Adversarial loss: 0.309189, acc: 0.945312]\n",
            "846: [Discriminator loss: 0.607109, acc: 0.507812]  [Adversarial loss: 0.278247, acc: 0.980469]\n",
            "847: [Discriminator loss: 0.622163, acc: 0.496094]  [Adversarial loss: 0.312961, acc: 0.949219]\n",
            "848: [Discriminator loss: 0.562782, acc: 0.546875]  [Adversarial loss: 0.283937, acc: 0.941406]\n",
            "849: [Discriminator loss: 0.597296, acc: 0.507812]  [Adversarial loss: 0.295504, acc: 0.941406]\n",
            "850: [Discriminator loss: 0.578519, acc: 0.484375]  [Adversarial loss: 0.311586, acc: 0.949219]\n",
            "851: [Discriminator loss: 0.583557, acc: 0.503906]  [Adversarial loss: 0.292683, acc: 0.972656]\n",
            "852: [Discriminator loss: 0.677026, acc: 0.500000]  [Adversarial loss: 0.303839, acc: 0.968750]\n",
            "853: [Discriminator loss: 0.686343, acc: 0.464844]  [Adversarial loss: 0.307894, acc: 0.972656]\n",
            "854: [Discriminator loss: 0.665949, acc: 0.472656]  [Adversarial loss: 0.304627, acc: 0.957031]\n",
            "855: [Discriminator loss: 0.568320, acc: 0.515625]  [Adversarial loss: 0.300316, acc: 0.960938]\n",
            "856: [Discriminator loss: 0.633462, acc: 0.507812]  [Adversarial loss: 0.326391, acc: 0.929688]\n",
            "857: [Discriminator loss: 0.626346, acc: 0.539062]  [Adversarial loss: 0.283160, acc: 0.941406]\n",
            "858: [Discriminator loss: 0.694887, acc: 0.472656]  [Adversarial loss: 0.299657, acc: 0.945312]\n",
            "859: [Discriminator loss: 0.666836, acc: 0.492188]  [Adversarial loss: 0.299728, acc: 0.953125]\n",
            "860: [Discriminator loss: 0.680475, acc: 0.468750]  [Adversarial loss: 0.317474, acc: 0.953125]\n",
            "861: [Discriminator loss: 0.621135, acc: 0.523438]  [Adversarial loss: 0.323592, acc: 0.960938]\n",
            "862: [Discriminator loss: 0.723509, acc: 0.457031]  [Adversarial loss: 0.341080, acc: 0.949219]\n",
            "863: [Discriminator loss: 0.613791, acc: 0.484375]  [Adversarial loss: 0.354600, acc: 0.953125]\n",
            "864: [Discriminator loss: 0.627934, acc: 0.492188]  [Adversarial loss: 0.316160, acc: 0.953125]\n",
            "865: [Discriminator loss: 0.626098, acc: 0.464844]  [Adversarial loss: 0.324802, acc: 0.980469]\n",
            "866: [Discriminator loss: 0.609275, acc: 0.500000]  [Adversarial loss: 0.311925, acc: 0.941406]\n",
            "867: [Discriminator loss: 0.553137, acc: 0.523438]  [Adversarial loss: 0.318821, acc: 0.945312]\n",
            "868: [Discriminator loss: 0.641995, acc: 0.484375]  [Adversarial loss: 0.314492, acc: 0.957031]\n",
            "869: [Discriminator loss: 0.593746, acc: 0.503906]  [Adversarial loss: 0.322499, acc: 0.957031]\n",
            "870: [Discriminator loss: 0.592138, acc: 0.527344]  [Adversarial loss: 0.306890, acc: 0.953125]\n",
            "871: [Discriminator loss: 0.602465, acc: 0.507812]  [Adversarial loss: 0.324970, acc: 0.953125]\n",
            "872: [Discriminator loss: 0.582897, acc: 0.515625]  [Adversarial loss: 0.324539, acc: 0.937500]\n",
            "873: [Discriminator loss: 0.617130, acc: 0.527344]  [Adversarial loss: 0.291070, acc: 0.957031]\n",
            "874: [Discriminator loss: 0.535710, acc: 0.558594]  [Adversarial loss: 0.317321, acc: 0.941406]\n",
            "875: [Discriminator loss: 0.572723, acc: 0.542969]  [Adversarial loss: 0.326221, acc: 0.953125]\n",
            "876: [Discriminator loss: 0.629143, acc: 0.492188]  [Adversarial loss: 0.310633, acc: 0.957031]\n",
            "877: [Discriminator loss: 0.567060, acc: 0.519531]  [Adversarial loss: 0.321977, acc: 0.957031]\n",
            "878: [Discriminator loss: 0.640463, acc: 0.496094]  [Adversarial loss: 0.308810, acc: 0.957031]\n",
            "879: [Discriminator loss: 0.631238, acc: 0.527344]  [Adversarial loss: 0.275934, acc: 0.964844]\n",
            "880: [Discriminator loss: 0.731847, acc: 0.437500]  [Adversarial loss: 0.333164, acc: 0.945312]\n",
            "881: [Discriminator loss: 0.608687, acc: 0.523438]  [Adversarial loss: 0.288733, acc: 0.964844]\n",
            "882: [Discriminator loss: 0.569010, acc: 0.539062]  [Adversarial loss: 0.277432, acc: 0.953125]\n",
            "883: [Discriminator loss: 0.613748, acc: 0.527344]  [Adversarial loss: 0.286023, acc: 0.960938]\n",
            "884: [Discriminator loss: 0.580811, acc: 0.531250]  [Adversarial loss: 0.303668, acc: 0.933594]\n",
            "885: [Discriminator loss: 0.524161, acc: 0.582031]  [Adversarial loss: 0.281397, acc: 0.933594]\n",
            "886: [Discriminator loss: 0.582377, acc: 0.566406]  [Adversarial loss: 0.291669, acc: 0.929688]\n",
            "887: [Discriminator loss: 0.650729, acc: 0.492188]  [Adversarial loss: 0.323687, acc: 0.921875]\n",
            "888: [Discriminator loss: 0.586338, acc: 0.535156]  [Adversarial loss: 0.293629, acc: 0.968750]\n",
            "889: [Discriminator loss: 0.573415, acc: 0.535156]  [Adversarial loss: 0.309565, acc: 0.921875]\n",
            "890: [Discriminator loss: 0.644504, acc: 0.562500]  [Adversarial loss: 0.310681, acc: 0.949219]\n",
            "891: [Discriminator loss: 0.693225, acc: 0.468750]  [Adversarial loss: 0.328587, acc: 0.957031]\n",
            "892: [Discriminator loss: 0.602271, acc: 0.503906]  [Adversarial loss: 0.338518, acc: 0.937500]\n",
            "893: [Discriminator loss: 0.675199, acc: 0.496094]  [Adversarial loss: 0.345087, acc: 0.925781]\n",
            "894: [Discriminator loss: 0.644400, acc: 0.507812]  [Adversarial loss: 0.300675, acc: 0.933594]\n",
            "895: [Discriminator loss: 0.567163, acc: 0.542969]  [Adversarial loss: 0.290060, acc: 0.945312]\n",
            "896: [Discriminator loss: 0.589812, acc: 0.480469]  [Adversarial loss: 0.338125, acc: 0.937500]\n",
            "897: [Discriminator loss: 0.645681, acc: 0.527344]  [Adversarial loss: 0.294217, acc: 0.957031]\n",
            "898: [Discriminator loss: 0.672048, acc: 0.488281]  [Adversarial loss: 0.302517, acc: 0.949219]\n",
            "899: [Discriminator loss: 0.616285, acc: 0.476562]  [Adversarial loss: 0.323910, acc: 0.968750]\n",
            "900: [Discriminator loss: 0.600176, acc: 0.535156]  [Adversarial loss: 0.297478, acc: 0.949219]\n",
            "901: [Discriminator loss: 0.577243, acc: 0.546875]  [Adversarial loss: 0.282415, acc: 0.964844]\n",
            "902: [Discriminator loss: 0.623196, acc: 0.484375]  [Adversarial loss: 0.317288, acc: 0.937500]\n",
            "903: [Discriminator loss: 0.615333, acc: 0.523438]  [Adversarial loss: 0.324573, acc: 0.933594]\n",
            "904: [Discriminator loss: 0.617023, acc: 0.531250]  [Adversarial loss: 0.295105, acc: 0.945312]\n",
            "905: [Discriminator loss: 0.577148, acc: 0.480469]  [Adversarial loss: 0.345547, acc: 0.937500]\n",
            "906: [Discriminator loss: 0.586603, acc: 0.542969]  [Adversarial loss: 0.290039, acc: 0.953125]\n",
            "907: [Discriminator loss: 0.585062, acc: 0.519531]  [Adversarial loss: 0.293505, acc: 0.980469]\n",
            "908: [Discriminator loss: 0.557248, acc: 0.507812]  [Adversarial loss: 0.320851, acc: 0.941406]\n",
            "909: [Discriminator loss: 0.716476, acc: 0.468750]  [Adversarial loss: 0.319736, acc: 0.957031]\n",
            "910: [Discriminator loss: 0.589536, acc: 0.562500]  [Adversarial loss: 0.284935, acc: 0.937500]\n",
            "911: [Discriminator loss: 0.680135, acc: 0.480469]  [Adversarial loss: 0.308622, acc: 0.972656]\n",
            "912: [Discriminator loss: 0.580044, acc: 0.550781]  [Adversarial loss: 0.283633, acc: 0.949219]\n",
            "913: [Discriminator loss: 0.587195, acc: 0.558594]  [Adversarial loss: 0.282944, acc: 0.960938]\n",
            "914: [Discriminator loss: 0.568347, acc: 0.566406]  [Adversarial loss: 0.277979, acc: 0.945312]\n",
            "915: [Discriminator loss: 0.552417, acc: 0.535156]  [Adversarial loss: 0.310881, acc: 0.949219]\n",
            "916: [Discriminator loss: 0.614320, acc: 0.535156]  [Adversarial loss: 0.296062, acc: 0.929688]\n",
            "917: [Discriminator loss: 0.587781, acc: 0.546875]  [Adversarial loss: 0.286448, acc: 0.960938]\n",
            "918: [Discriminator loss: 0.726918, acc: 0.511719]  [Adversarial loss: 0.313764, acc: 0.964844]\n",
            "919: [Discriminator loss: 0.641437, acc: 0.519531]  [Adversarial loss: 0.303740, acc: 0.957031]\n",
            "920: [Discriminator loss: 0.537575, acc: 0.597656]  [Adversarial loss: 0.268900, acc: 0.937500]\n",
            "921: [Discriminator loss: 0.611695, acc: 0.550781]  [Adversarial loss: 0.280801, acc: 0.949219]\n",
            "922: [Discriminator loss: 0.659982, acc: 0.453125]  [Adversarial loss: 0.341949, acc: 0.960938]\n",
            "923: [Discriminator loss: 0.590416, acc: 0.527344]  [Adversarial loss: 0.330856, acc: 0.937500]\n",
            "924: [Discriminator loss: 0.480521, acc: 0.578125]  [Adversarial loss: 0.283744, acc: 0.953125]\n",
            "925: [Discriminator loss: 0.607008, acc: 0.535156]  [Adversarial loss: 0.285763, acc: 0.957031]\n",
            "926: [Discriminator loss: 0.594272, acc: 0.523438]  [Adversarial loss: 0.319681, acc: 0.953125]\n",
            "927: [Discriminator loss: 0.576141, acc: 0.484375]  [Adversarial loss: 0.299848, acc: 0.953125]\n",
            "928: [Discriminator loss: 0.577963, acc: 0.519531]  [Adversarial loss: 0.333158, acc: 0.957031]\n",
            "929: [Discriminator loss: 0.536297, acc: 0.593750]  [Adversarial loss: 0.257999, acc: 0.957031]\n",
            "930: [Discriminator loss: 0.680458, acc: 0.503906]  [Adversarial loss: 0.324472, acc: 0.957031]\n",
            "931: [Discriminator loss: 0.629215, acc: 0.488281]  [Adversarial loss: 0.386151, acc: 0.945312]\n",
            "932: [Discriminator loss: 0.622539, acc: 0.460938]  [Adversarial loss: 0.338577, acc: 0.949219]\n",
            "933: [Discriminator loss: 0.578480, acc: 0.484375]  [Adversarial loss: 0.321265, acc: 0.960938]\n",
            "934: [Discriminator loss: 0.645124, acc: 0.535156]  [Adversarial loss: 0.298606, acc: 0.941406]\n",
            "935: [Discriminator loss: 0.562315, acc: 0.550781]  [Adversarial loss: 0.303217, acc: 0.945312]\n",
            "936: [Discriminator loss: 0.541896, acc: 0.554688]  [Adversarial loss: 0.295995, acc: 0.937500]\n",
            "937: [Discriminator loss: 0.623633, acc: 0.503906]  [Adversarial loss: 0.326172, acc: 0.945312]\n",
            "938: [Discriminator loss: 0.650063, acc: 0.429688]  [Adversarial loss: 0.345861, acc: 0.964844]\n",
            "939: [Discriminator loss: 0.649439, acc: 0.476562]  [Adversarial loss: 0.313952, acc: 0.964844]\n",
            "940: [Discriminator loss: 0.568562, acc: 0.500000]  [Adversarial loss: 0.314732, acc: 0.957031]\n",
            "941: [Discriminator loss: 0.542507, acc: 0.566406]  [Adversarial loss: 0.268658, acc: 0.964844]\n",
            "942: [Discriminator loss: 0.625602, acc: 0.496094]  [Adversarial loss: 0.303355, acc: 0.964844]\n",
            "943: [Discriminator loss: 0.570651, acc: 0.527344]  [Adversarial loss: 0.297867, acc: 0.953125]\n",
            "944: [Discriminator loss: 0.631238, acc: 0.511719]  [Adversarial loss: 0.297309, acc: 0.957031]\n",
            "945: [Discriminator loss: 0.569422, acc: 0.527344]  [Adversarial loss: 0.298143, acc: 0.953125]\n",
            "946: [Discriminator loss: 0.631223, acc: 0.492188]  [Adversarial loss: 0.320372, acc: 0.945312]\n",
            "947: [Discriminator loss: 0.610649, acc: 0.527344]  [Adversarial loss: 0.293363, acc: 0.960938]\n",
            "948: [Discriminator loss: 0.579927, acc: 0.500000]  [Adversarial loss: 0.343338, acc: 0.921875]\n",
            "949: [Discriminator loss: 0.562383, acc: 0.550781]  [Adversarial loss: 0.285346, acc: 0.957031]\n",
            "950: [Discriminator loss: 0.609682, acc: 0.492188]  [Adversarial loss: 0.326217, acc: 0.945312]\n",
            "951: [Discriminator loss: 0.628195, acc: 0.496094]  [Adversarial loss: 0.303788, acc: 0.953125]\n",
            "952: [Discriminator loss: 0.597416, acc: 0.476562]  [Adversarial loss: 0.345047, acc: 0.957031]\n",
            "953: [Discriminator loss: 0.617870, acc: 0.472656]  [Adversarial loss: 0.367371, acc: 0.949219]\n",
            "954: [Discriminator loss: 0.606514, acc: 0.496094]  [Adversarial loss: 0.322502, acc: 0.945312]\n",
            "955: [Discriminator loss: 0.600560, acc: 0.476562]  [Adversarial loss: 0.326603, acc: 0.964844]\n",
            "956: [Discriminator loss: 0.537302, acc: 0.535156]  [Adversarial loss: 0.309098, acc: 0.960938]\n",
            "957: [Discriminator loss: 0.606272, acc: 0.492188]  [Adversarial loss: 0.329729, acc: 0.945312]\n",
            "958: [Discriminator loss: 0.603003, acc: 0.464844]  [Adversarial loss: 0.333118, acc: 0.960938]\n",
            "959: [Discriminator loss: 0.525134, acc: 0.519531]  [Adversarial loss: 0.319835, acc: 0.964844]\n",
            "960: [Discriminator loss: 0.597825, acc: 0.527344]  [Adversarial loss: 0.295041, acc: 0.953125]\n",
            "961: [Discriminator loss: 0.690787, acc: 0.488281]  [Adversarial loss: 0.324064, acc: 0.949219]\n",
            "962: [Discriminator loss: 0.522114, acc: 0.523438]  [Adversarial loss: 0.316313, acc: 0.933594]\n",
            "963: [Discriminator loss: 0.532300, acc: 0.542969]  [Adversarial loss: 0.294943, acc: 0.945312]\n",
            "964: [Discriminator loss: 0.640604, acc: 0.496094]  [Adversarial loss: 0.317490, acc: 0.953125]\n",
            "965: [Discriminator loss: 0.638075, acc: 0.476562]  [Adversarial loss: 0.323156, acc: 0.957031]\n",
            "966: [Discriminator loss: 0.658363, acc: 0.484375]  [Adversarial loss: 0.340652, acc: 0.957031]\n",
            "967: [Discriminator loss: 0.613028, acc: 0.519531]  [Adversarial loss: 0.309431, acc: 0.949219]\n",
            "968: [Discriminator loss: 0.695280, acc: 0.453125]  [Adversarial loss: 0.348182, acc: 0.945312]\n",
            "969: [Discriminator loss: 0.689557, acc: 0.464844]  [Adversarial loss: 0.329803, acc: 0.960938]\n",
            "970: [Discriminator loss: 0.561512, acc: 0.511719]  [Adversarial loss: 0.304411, acc: 0.960938]\n",
            "971: [Discriminator loss: 0.573553, acc: 0.492188]  [Adversarial loss: 0.335063, acc: 0.945312]\n",
            "972: [Discriminator loss: 0.620133, acc: 0.472656]  [Adversarial loss: 0.338836, acc: 0.953125]\n",
            "973: [Discriminator loss: 0.581426, acc: 0.437500]  [Adversarial loss: 0.365278, acc: 0.945312]\n",
            "974: [Discriminator loss: 0.602919, acc: 0.539062]  [Adversarial loss: 0.300842, acc: 0.972656]\n",
            "975: [Discriminator loss: 0.561838, acc: 0.507812]  [Adversarial loss: 0.332730, acc: 0.949219]\n",
            "976: [Discriminator loss: 0.568731, acc: 0.562500]  [Adversarial loss: 0.263924, acc: 0.980469]\n",
            "977: [Discriminator loss: 0.561831, acc: 0.515625]  [Adversarial loss: 0.335685, acc: 0.917969]\n",
            "978: [Discriminator loss: 0.573529, acc: 0.531250]  [Adversarial loss: 0.323339, acc: 0.964844]\n",
            "979: [Discriminator loss: 0.567599, acc: 0.527344]  [Adversarial loss: 0.326305, acc: 0.933594]\n",
            "980: [Discriminator loss: 0.599206, acc: 0.476562]  [Adversarial loss: 0.325825, acc: 0.968750]\n",
            "981: [Discriminator loss: 0.637365, acc: 0.453125]  [Adversarial loss: 0.329150, acc: 0.945312]\n",
            "982: [Discriminator loss: 0.569430, acc: 0.531250]  [Adversarial loss: 0.311288, acc: 0.941406]\n",
            "983: [Discriminator loss: 0.607259, acc: 0.531250]  [Adversarial loss: 0.290163, acc: 0.945312]\n",
            "984: [Discriminator loss: 0.591682, acc: 0.515625]  [Adversarial loss: 0.293125, acc: 0.953125]\n",
            "985: [Discriminator loss: 0.560997, acc: 0.546875]  [Adversarial loss: 0.296571, acc: 0.945312]\n",
            "986: [Discriminator loss: 0.639965, acc: 0.468750]  [Adversarial loss: 0.315899, acc: 0.980469]\n",
            "987: [Discriminator loss: 0.603871, acc: 0.484375]  [Adversarial loss: 0.315556, acc: 0.957031]\n",
            "988: [Discriminator loss: 0.707300, acc: 0.511719]  [Adversarial loss: 0.313995, acc: 0.953125]\n",
            "989: [Discriminator loss: 0.655105, acc: 0.546875]  [Adversarial loss: 0.271169, acc: 0.964844]\n",
            "990: [Discriminator loss: 0.587620, acc: 0.484375]  [Adversarial loss: 0.332383, acc: 0.953125]\n",
            "991: [Discriminator loss: 0.537171, acc: 0.558594]  [Adversarial loss: 0.264911, acc: 0.957031]\n",
            "992: [Discriminator loss: 0.613662, acc: 0.457031]  [Adversarial loss: 0.350338, acc: 0.945312]\n",
            "993: [Discriminator loss: 0.526805, acc: 0.535156]  [Adversarial loss: 0.298280, acc: 0.968750]\n",
            "994: [Discriminator loss: 0.583915, acc: 0.507812]  [Adversarial loss: 0.324946, acc: 0.945312]\n",
            "995: [Discriminator loss: 0.595920, acc: 0.539062]  [Adversarial loss: 0.306001, acc: 0.929688]\n",
            "996: [Discriminator loss: 0.571813, acc: 0.527344]  [Adversarial loss: 0.282402, acc: 0.968750]\n",
            "997: [Discriminator loss: 0.563762, acc: 0.519531]  [Adversarial loss: 0.298686, acc: 0.957031]\n",
            "998: [Discriminator loss: 0.580460, acc: 0.500000]  [Adversarial loss: 0.309629, acc: 0.957031]\n",
            "999: [Discriminator loss: 0.713064, acc: 0.457031]  [Adversarial loss: 0.329746, acc: 0.941406]\n",
            "1000: [Discriminator loss: 0.567189, acc: 0.503906]  [Adversarial loss: 0.316907, acc: 0.937500]\n",
            "1001: [Discriminator loss: 0.749953, acc: 0.406250]  [Adversarial loss: 0.353729, acc: 0.968750]\n",
            "1002: [Discriminator loss: 0.631738, acc: 0.472656]  [Adversarial loss: 0.340935, acc: 0.949219]\n",
            "1003: [Discriminator loss: 0.603697, acc: 0.496094]  [Adversarial loss: 0.323123, acc: 0.964844]\n",
            "1004: [Discriminator loss: 0.712735, acc: 0.503906]  [Adversarial loss: 0.331409, acc: 0.933594]\n",
            "1005: [Discriminator loss: 0.672177, acc: 0.507812]  [Adversarial loss: 0.330586, acc: 0.929688]\n",
            "1006: [Discriminator loss: 0.586115, acc: 0.546875]  [Adversarial loss: 0.309044, acc: 0.925781]\n",
            "1007: [Discriminator loss: 0.622360, acc: 0.519531]  [Adversarial loss: 0.307809, acc: 0.964844]\n",
            "1008: [Discriminator loss: 0.568948, acc: 0.519531]  [Adversarial loss: 0.325626, acc: 0.949219]\n",
            "1009: [Discriminator loss: 0.540459, acc: 0.562500]  [Adversarial loss: 0.349960, acc: 0.933594]\n",
            "1010: [Discriminator loss: 0.632544, acc: 0.449219]  [Adversarial loss: 0.333935, acc: 0.941406]\n",
            "1011: [Discriminator loss: 0.613339, acc: 0.457031]  [Adversarial loss: 0.338850, acc: 0.953125]\n",
            "1012: [Discriminator loss: 0.635468, acc: 0.511719]  [Adversarial loss: 0.318386, acc: 0.957031]\n",
            "1013: [Discriminator loss: 0.590110, acc: 0.460938]  [Adversarial loss: 0.371762, acc: 0.914062]\n",
            "1014: [Discriminator loss: 0.566323, acc: 0.507812]  [Adversarial loss: 0.334133, acc: 0.925781]\n",
            "1015: [Discriminator loss: 0.535110, acc: 0.527344]  [Adversarial loss: 0.336028, acc: 0.941406]\n",
            "1016: [Discriminator loss: 0.599302, acc: 0.492188]  [Adversarial loss: 0.319616, acc: 0.941406]\n",
            "1017: [Discriminator loss: 0.555574, acc: 0.539062]  [Adversarial loss: 0.304672, acc: 0.941406]\n",
            "1018: [Discriminator loss: 0.582139, acc: 0.503906]  [Adversarial loss: 0.307382, acc: 0.957031]\n",
            "1019: [Discriminator loss: 0.656655, acc: 0.500000]  [Adversarial loss: 0.299692, acc: 0.972656]\n",
            "1020: [Discriminator loss: 0.598075, acc: 0.507812]  [Adversarial loss: 0.303912, acc: 0.960938]\n",
            "1021: [Discriminator loss: 0.510882, acc: 0.554688]  [Adversarial loss: 0.282551, acc: 0.964844]\n",
            "1022: [Discriminator loss: 0.608594, acc: 0.488281]  [Adversarial loss: 0.354927, acc: 0.941406]\n",
            "1023: [Discriminator loss: 0.533346, acc: 0.515625]  [Adversarial loss: 0.375232, acc: 0.937500]\n",
            "1024: [Discriminator loss: 0.601974, acc: 0.550781]  [Adversarial loss: 0.299738, acc: 0.949219]\n",
            "1025: [Discriminator loss: 0.571445, acc: 0.507812]  [Adversarial loss: 0.345941, acc: 0.925781]\n",
            "1026: [Discriminator loss: 0.585252, acc: 0.468750]  [Adversarial loss: 0.346480, acc: 0.960938]\n",
            "1027: [Discriminator loss: 0.556678, acc: 0.539062]  [Adversarial loss: 0.308208, acc: 0.945312]\n",
            "1028: [Discriminator loss: 0.690219, acc: 0.503906]  [Adversarial loss: 0.308881, acc: 0.953125]\n",
            "1029: [Discriminator loss: 0.669961, acc: 0.460938]  [Adversarial loss: 0.332588, acc: 0.945312]\n",
            "1030: [Discriminator loss: 0.569097, acc: 0.539062]  [Adversarial loss: 0.298301, acc: 0.953125]\n",
            "1031: [Discriminator loss: 0.677835, acc: 0.460938]  [Adversarial loss: 0.347399, acc: 0.953125]\n",
            "1032: [Discriminator loss: 0.591698, acc: 0.566406]  [Adversarial loss: 0.320659, acc: 0.910156]\n",
            "1033: [Discriminator loss: 0.528755, acc: 0.542969]  [Adversarial loss: 0.298845, acc: 0.968750]\n",
            "1034: [Discriminator loss: 0.583143, acc: 0.500000]  [Adversarial loss: 0.318003, acc: 0.949219]\n",
            "1035: [Discriminator loss: 0.599821, acc: 0.496094]  [Adversarial loss: 0.308487, acc: 0.968750]\n",
            "1036: [Discriminator loss: 0.636725, acc: 0.492188]  [Adversarial loss: 0.340910, acc: 0.933594]\n",
            "1037: [Discriminator loss: 0.583607, acc: 0.511719]  [Adversarial loss: 0.328288, acc: 0.953125]\n",
            "1038: [Discriminator loss: 0.607952, acc: 0.503906]  [Adversarial loss: 0.329589, acc: 0.953125]\n",
            "1039: [Discriminator loss: 0.573500, acc: 0.503906]  [Adversarial loss: 0.334932, acc: 0.945312]\n",
            "1040: [Discriminator loss: 0.530127, acc: 0.523438]  [Adversarial loss: 0.337740, acc: 0.957031]\n",
            "1041: [Discriminator loss: 0.636209, acc: 0.500000]  [Adversarial loss: 0.342008, acc: 0.949219]\n",
            "1042: [Discriminator loss: 0.539097, acc: 0.503906]  [Adversarial loss: 0.321211, acc: 0.945312]\n",
            "1043: [Discriminator loss: 0.608045, acc: 0.472656]  [Adversarial loss: 0.359304, acc: 0.929688]\n",
            "1044: [Discriminator loss: 0.607386, acc: 0.464844]  [Adversarial loss: 0.336218, acc: 0.964844]\n",
            "1045: [Discriminator loss: 0.624022, acc: 0.488281]  [Adversarial loss: 0.324318, acc: 0.953125]\n",
            "1046: [Discriminator loss: 0.545705, acc: 0.550781]  [Adversarial loss: 0.289942, acc: 0.960938]\n",
            "1047: [Discriminator loss: 0.514701, acc: 0.550781]  [Adversarial loss: 0.317079, acc: 0.937500]\n",
            "1048: [Discriminator loss: 0.567664, acc: 0.500000]  [Adversarial loss: 0.323433, acc: 0.933594]\n",
            "1049: [Discriminator loss: 0.543037, acc: 0.500000]  [Adversarial loss: 0.317340, acc: 0.976562]\n",
            "1050: [Discriminator loss: 0.637431, acc: 0.492188]  [Adversarial loss: 0.332988, acc: 0.945312]\n",
            "1051: [Discriminator loss: 0.573551, acc: 0.511719]  [Adversarial loss: 0.337237, acc: 0.941406]\n",
            "1052: [Discriminator loss: 0.569082, acc: 0.515625]  [Adversarial loss: 0.343205, acc: 0.937500]\n",
            "1053: [Discriminator loss: 0.572490, acc: 0.546875]  [Adversarial loss: 0.320783, acc: 0.949219]\n",
            "1054: [Discriminator loss: 0.594726, acc: 0.484375]  [Adversarial loss: 0.304063, acc: 0.976562]\n",
            "1055: [Discriminator loss: 0.521950, acc: 0.523438]  [Adversarial loss: 0.341723, acc: 0.917969]\n",
            "1056: [Discriminator loss: 0.543684, acc: 0.550781]  [Adversarial loss: 0.311421, acc: 0.929688]\n",
            "1057: [Discriminator loss: 0.601025, acc: 0.488281]  [Adversarial loss: 0.379300, acc: 0.937500]\n",
            "1058: [Discriminator loss: 0.545367, acc: 0.562500]  [Adversarial loss: 0.282919, acc: 0.960938]\n",
            "1059: [Discriminator loss: 0.542231, acc: 0.515625]  [Adversarial loss: 0.294232, acc: 0.960938]\n",
            "1060: [Discriminator loss: 0.552177, acc: 0.527344]  [Adversarial loss: 0.312042, acc: 0.937500]\n",
            "1061: [Discriminator loss: 0.571378, acc: 0.523438]  [Adversarial loss: 0.336070, acc: 0.960938]\n",
            "1062: [Discriminator loss: 0.549922, acc: 0.480469]  [Adversarial loss: 0.331697, acc: 0.964844]\n",
            "1063: [Discriminator loss: 0.560855, acc: 0.503906]  [Adversarial loss: 0.324693, acc: 0.953125]\n",
            "1064: [Discriminator loss: 0.571773, acc: 0.476562]  [Adversarial loss: 0.385917, acc: 0.937500]\n",
            "1065: [Discriminator loss: 0.571261, acc: 0.511719]  [Adversarial loss: 0.320414, acc: 0.953125]\n",
            "1066: [Discriminator loss: 0.538598, acc: 0.500000]  [Adversarial loss: 0.316163, acc: 0.968750]\n",
            "1067: [Discriminator loss: 0.540572, acc: 0.511719]  [Adversarial loss: 0.333609, acc: 0.937500]\n",
            "1068: [Discriminator loss: 0.534405, acc: 0.535156]  [Adversarial loss: 0.303161, acc: 0.945312]\n",
            "1069: [Discriminator loss: 0.623940, acc: 0.492188]  [Adversarial loss: 0.365890, acc: 0.925781]\n",
            "1070: [Discriminator loss: 0.605693, acc: 0.523438]  [Adversarial loss: 0.301356, acc: 0.949219]\n",
            "1071: [Discriminator loss: 0.512494, acc: 0.546875]  [Adversarial loss: 0.338970, acc: 0.945312]\n",
            "1072: [Discriminator loss: 0.566575, acc: 0.488281]  [Adversarial loss: 0.321721, acc: 0.972656]\n",
            "1073: [Discriminator loss: 0.635190, acc: 0.492188]  [Adversarial loss: 0.345528, acc: 0.925781]\n",
            "1074: [Discriminator loss: 0.571777, acc: 0.500000]  [Adversarial loss: 0.341616, acc: 0.945312]\n",
            "1075: [Discriminator loss: 0.639404, acc: 0.500000]  [Adversarial loss: 0.346051, acc: 0.945312]\n",
            "1076: [Discriminator loss: 0.647206, acc: 0.523438]  [Adversarial loss: 0.342485, acc: 0.949219]\n",
            "1077: [Discriminator loss: 0.511776, acc: 0.550781]  [Adversarial loss: 0.323837, acc: 0.933594]\n",
            "1078: [Discriminator loss: 0.505533, acc: 0.500000]  [Adversarial loss: 0.325436, acc: 0.972656]\n",
            "1079: [Discriminator loss: 0.589597, acc: 0.500000]  [Adversarial loss: 0.355259, acc: 0.941406]\n",
            "1080: [Discriminator loss: 0.531077, acc: 0.519531]  [Adversarial loss: 0.365802, acc: 0.933594]\n",
            "1081: [Discriminator loss: 0.572027, acc: 0.484375]  [Adversarial loss: 0.347011, acc: 0.937500]\n",
            "1082: [Discriminator loss: 0.594737, acc: 0.507812]  [Adversarial loss: 0.347580, acc: 0.949219]\n",
            "1083: [Discriminator loss: 0.583950, acc: 0.519531]  [Adversarial loss: 0.317701, acc: 0.968750]\n",
            "1084: [Discriminator loss: 0.622563, acc: 0.472656]  [Adversarial loss: 0.326300, acc: 0.957031]\n",
            "1085: [Discriminator loss: 0.550911, acc: 0.535156]  [Adversarial loss: 0.310877, acc: 0.941406]\n",
            "1086: [Discriminator loss: 0.518085, acc: 0.554688]  [Adversarial loss: 0.281533, acc: 0.957031]\n",
            "1087: [Discriminator loss: 0.580863, acc: 0.523438]  [Adversarial loss: 0.335940, acc: 0.949219]\n",
            "1088: [Discriminator loss: 0.646337, acc: 0.472656]  [Adversarial loss: 0.378857, acc: 0.910156]\n",
            "1089: [Discriminator loss: 0.556968, acc: 0.554688]  [Adversarial loss: 0.312594, acc: 0.933594]\n",
            "1090: [Discriminator loss: 0.576089, acc: 0.519531]  [Adversarial loss: 0.359528, acc: 0.957031]\n",
            "1091: [Discriminator loss: 0.569336, acc: 0.539062]  [Adversarial loss: 0.310456, acc: 0.945312]\n",
            "1092: [Discriminator loss: 0.602853, acc: 0.464844]  [Adversarial loss: 0.344184, acc: 0.960938]\n",
            "1093: [Discriminator loss: 0.556295, acc: 0.484375]  [Adversarial loss: 0.337569, acc: 0.957031]\n",
            "1094: [Discriminator loss: 0.574029, acc: 0.535156]  [Adversarial loss: 0.371444, acc: 0.902344]\n",
            "1095: [Discriminator loss: 0.522142, acc: 0.570312]  [Adversarial loss: 0.297201, acc: 0.957031]\n",
            "1096: [Discriminator loss: 0.612929, acc: 0.488281]  [Adversarial loss: 0.336935, acc: 0.941406]\n",
            "1097: [Discriminator loss: 0.553704, acc: 0.480469]  [Adversarial loss: 0.359432, acc: 0.937500]\n",
            "1098: [Discriminator loss: 0.629957, acc: 0.476562]  [Adversarial loss: 0.326948, acc: 0.968750]\n",
            "1099: [Discriminator loss: 0.589146, acc: 0.496094]  [Adversarial loss: 0.356839, acc: 0.945312]\n",
            "1100: [Discriminator loss: 0.592813, acc: 0.535156]  [Adversarial loss: 0.298812, acc: 0.964844]\n",
            "1101: [Discriminator loss: 0.556929, acc: 0.523438]  [Adversarial loss: 0.312736, acc: 0.960938]\n",
            "1102: [Discriminator loss: 0.524724, acc: 0.500000]  [Adversarial loss: 0.320313, acc: 0.980469]\n",
            "1103: [Discriminator loss: 0.608503, acc: 0.527344]  [Adversarial loss: 0.324133, acc: 0.949219]\n",
            "1104: [Discriminator loss: 0.563030, acc: 0.507812]  [Adversarial loss: 0.327304, acc: 0.949219]\n",
            "1105: [Discriminator loss: 0.527512, acc: 0.507812]  [Adversarial loss: 0.309860, acc: 0.976562]\n",
            "1106: [Discriminator loss: 0.537936, acc: 0.515625]  [Adversarial loss: 0.324359, acc: 0.937500]\n",
            "1107: [Discriminator loss: 0.579521, acc: 0.425781]  [Adversarial loss: 0.372176, acc: 0.972656]\n",
            "1108: [Discriminator loss: 0.539433, acc: 0.523438]  [Adversarial loss: 0.347690, acc: 0.949219]\n",
            "1109: [Discriminator loss: 0.584783, acc: 0.511719]  [Adversarial loss: 0.359109, acc: 0.925781]\n",
            "1110: [Discriminator loss: 0.570243, acc: 0.468750]  [Adversarial loss: 0.351620, acc: 0.953125]\n",
            "1111: [Discriminator loss: 0.591303, acc: 0.507812]  [Adversarial loss: 0.318117, acc: 0.953125]\n",
            "1112: [Discriminator loss: 0.537608, acc: 0.449219]  [Adversarial loss: 0.355547, acc: 0.949219]\n",
            "1113: [Discriminator loss: 0.538344, acc: 0.523438]  [Adversarial loss: 0.311662, acc: 0.953125]\n",
            "1114: [Discriminator loss: 0.558826, acc: 0.542969]  [Adversarial loss: 0.290101, acc: 0.949219]\n",
            "1115: [Discriminator loss: 0.663956, acc: 0.468750]  [Adversarial loss: 0.324117, acc: 0.960938]\n",
            "1116: [Discriminator loss: 0.539454, acc: 0.503906]  [Adversarial loss: 0.329751, acc: 0.953125]\n",
            "1117: [Discriminator loss: 0.575181, acc: 0.488281]  [Adversarial loss: 0.331798, acc: 0.957031]\n",
            "1118: [Discriminator loss: 0.528496, acc: 0.535156]  [Adversarial loss: 0.322230, acc: 0.941406]\n",
            "1119: [Discriminator loss: 0.590042, acc: 0.460938]  [Adversarial loss: 0.355431, acc: 0.960938]\n",
            "1120: [Discriminator loss: 0.505989, acc: 0.507812]  [Adversarial loss: 0.313144, acc: 0.980469]\n",
            "1121: [Discriminator loss: 0.576478, acc: 0.503906]  [Adversarial loss: 0.332633, acc: 0.972656]\n",
            "1122: [Discriminator loss: 0.528488, acc: 0.523438]  [Adversarial loss: 0.343165, acc: 0.941406]\n",
            "1123: [Discriminator loss: 0.589480, acc: 0.460938]  [Adversarial loss: 0.385082, acc: 0.929688]\n",
            "1124: [Discriminator loss: 0.559404, acc: 0.515625]  [Adversarial loss: 0.316436, acc: 0.957031]\n",
            "1125: [Discriminator loss: 0.627801, acc: 0.453125]  [Adversarial loss: 0.365762, acc: 0.953125]\n",
            "1126: [Discriminator loss: 0.496562, acc: 0.546875]  [Adversarial loss: 0.368907, acc: 0.921875]\n",
            "1127: [Discriminator loss: 0.616098, acc: 0.468750]  [Adversarial loss: 0.333907, acc: 0.953125]\n",
            "1128: [Discriminator loss: 0.512328, acc: 0.515625]  [Adversarial loss: 0.334853, acc: 0.960938]\n",
            "1129: [Discriminator loss: 0.521777, acc: 0.531250]  [Adversarial loss: 0.295872, acc: 0.980469]\n",
            "1130: [Discriminator loss: 0.554862, acc: 0.535156]  [Adversarial loss: 0.319634, acc: 0.945312]\n",
            "1131: [Discriminator loss: 0.562599, acc: 0.468750]  [Adversarial loss: 0.344831, acc: 0.957031]\n",
            "1132: [Discriminator loss: 0.549282, acc: 0.507812]  [Adversarial loss: 0.317261, acc: 0.953125]\n",
            "1133: [Discriminator loss: 0.528258, acc: 0.531250]  [Adversarial loss: 0.307264, acc: 0.957031]\n",
            "1134: [Discriminator loss: 0.621740, acc: 0.515625]  [Adversarial loss: 0.274518, acc: 0.976562]\n",
            "1135: [Discriminator loss: 0.530349, acc: 0.539062]  [Adversarial loss: 0.304416, acc: 0.941406]\n",
            "1136: [Discriminator loss: 0.597000, acc: 0.453125]  [Adversarial loss: 0.348090, acc: 0.925781]\n",
            "1137: [Discriminator loss: 0.699866, acc: 0.472656]  [Adversarial loss: 0.385790, acc: 0.914062]\n",
            "1138: [Discriminator loss: 0.560152, acc: 0.531250]  [Adversarial loss: 0.309363, acc: 0.953125]\n",
            "1139: [Discriminator loss: 0.644177, acc: 0.523438]  [Adversarial loss: 0.318336, acc: 0.933594]\n",
            "1140: [Discriminator loss: 0.614370, acc: 0.488281]  [Adversarial loss: 0.347675, acc: 0.953125]\n",
            "1141: [Discriminator loss: 0.608719, acc: 0.464844]  [Adversarial loss: 0.362696, acc: 0.941406]\n",
            "1142: [Discriminator loss: 0.519615, acc: 0.570312]  [Adversarial loss: 0.294587, acc: 0.960938]\n",
            "1143: [Discriminator loss: 0.628731, acc: 0.500000]  [Adversarial loss: 0.329678, acc: 0.929688]\n",
            "1144: [Discriminator loss: 0.542465, acc: 0.496094]  [Adversarial loss: 0.344803, acc: 0.933594]\n",
            "1145: [Discriminator loss: 0.531039, acc: 0.539062]  [Adversarial loss: 0.315554, acc: 0.937500]\n",
            "1146: [Discriminator loss: 0.556607, acc: 0.488281]  [Adversarial loss: 0.375670, acc: 0.953125]\n",
            "1147: [Discriminator loss: 0.572785, acc: 0.476562]  [Adversarial loss: 0.343431, acc: 0.957031]\n",
            "1148: [Discriminator loss: 0.632854, acc: 0.531250]  [Adversarial loss: 0.325356, acc: 0.953125]\n",
            "1149: [Discriminator loss: 0.547756, acc: 0.535156]  [Adversarial loss: 0.306688, acc: 0.953125]\n",
            "1150: [Discriminator loss: 0.606319, acc: 0.484375]  [Adversarial loss: 0.332168, acc: 0.949219]\n",
            "1151: [Discriminator loss: 0.534826, acc: 0.519531]  [Adversarial loss: 0.310251, acc: 0.953125]\n",
            "1152: [Discriminator loss: 0.585513, acc: 0.519531]  [Adversarial loss: 0.302444, acc: 0.964844]\n",
            "1153: [Discriminator loss: 0.545686, acc: 0.503906]  [Adversarial loss: 0.311533, acc: 0.964844]\n",
            "1154: [Discriminator loss: 0.590653, acc: 0.488281]  [Adversarial loss: 0.339547, acc: 0.953125]\n",
            "1155: [Discriminator loss: 0.589711, acc: 0.476562]  [Adversarial loss: 0.336101, acc: 0.941406]\n",
            "1156: [Discriminator loss: 0.505377, acc: 0.542969]  [Adversarial loss: 0.294217, acc: 0.957031]\n",
            "1157: [Discriminator loss: 0.607488, acc: 0.480469]  [Adversarial loss: 0.325765, acc: 0.976562]\n",
            "1158: [Discriminator loss: 0.598550, acc: 0.496094]  [Adversarial loss: 0.332678, acc: 0.968750]\n",
            "1159: [Discriminator loss: 0.562941, acc: 0.523438]  [Adversarial loss: 0.299670, acc: 0.984375]\n",
            "1160: [Discriminator loss: 0.615342, acc: 0.433594]  [Adversarial loss: 0.372895, acc: 0.960938]\n",
            "1161: [Discriminator loss: 0.594534, acc: 0.507812]  [Adversarial loss: 0.340529, acc: 0.925781]\n",
            "1162: [Discriminator loss: 0.557009, acc: 0.500000]  [Adversarial loss: 0.320754, acc: 0.957031]\n",
            "1163: [Discriminator loss: 0.547875, acc: 0.500000]  [Adversarial loss: 0.355637, acc: 0.914062]\n",
            "1164: [Discriminator loss: 0.725770, acc: 0.457031]  [Adversarial loss: 0.337271, acc: 0.957031]\n",
            "1165: [Discriminator loss: 0.503977, acc: 0.558594]  [Adversarial loss: 0.302527, acc: 0.937500]\n",
            "1166: [Discriminator loss: 0.584940, acc: 0.480469]  [Adversarial loss: 0.356768, acc: 0.929688]\n",
            "1167: [Discriminator loss: 0.488481, acc: 0.562500]  [Adversarial loss: 0.288325, acc: 0.972656]\n",
            "1168: [Discriminator loss: 0.557079, acc: 0.496094]  [Adversarial loss: 0.317629, acc: 0.964844]\n",
            "1169: [Discriminator loss: 0.569882, acc: 0.523438]  [Adversarial loss: 0.333146, acc: 0.945312]\n",
            "1170: [Discriminator loss: 0.525379, acc: 0.507812]  [Adversarial loss: 0.315497, acc: 0.968750]\n",
            "1171: [Discriminator loss: 0.544758, acc: 0.507812]  [Adversarial loss: 0.346106, acc: 0.925781]\n",
            "1172: [Discriminator loss: 0.583885, acc: 0.527344]  [Adversarial loss: 0.302122, acc: 0.949219]\n",
            "1173: [Discriminator loss: 0.591839, acc: 0.488281]  [Adversarial loss: 0.341612, acc: 0.949219]\n",
            "1174: [Discriminator loss: 0.562035, acc: 0.476562]  [Adversarial loss: 0.340063, acc: 0.941406]\n",
            "1175: [Discriminator loss: 0.591343, acc: 0.496094]  [Adversarial loss: 0.323865, acc: 0.953125]\n",
            "1176: [Discriminator loss: 0.551426, acc: 0.503906]  [Adversarial loss: 0.330918, acc: 0.957031]\n",
            "1177: [Discriminator loss: 0.571636, acc: 0.488281]  [Adversarial loss: 0.325771, acc: 0.957031]\n",
            "1178: [Discriminator loss: 0.596107, acc: 0.515625]  [Adversarial loss: 0.311440, acc: 0.957031]\n",
            "1179: [Discriminator loss: 0.624387, acc: 0.484375]  [Adversarial loss: 0.357785, acc: 0.957031]\n",
            "1180: [Discriminator loss: 0.576573, acc: 0.492188]  [Adversarial loss: 0.335737, acc: 0.964844]\n",
            "1181: [Discriminator loss: 0.586899, acc: 0.488281]  [Adversarial loss: 0.337728, acc: 0.968750]\n",
            "1182: [Discriminator loss: 0.502166, acc: 0.535156]  [Adversarial loss: 0.299213, acc: 0.972656]\n",
            "1183: [Discriminator loss: 0.527923, acc: 0.578125]  [Adversarial loss: 0.274315, acc: 0.953125]\n",
            "1184: [Discriminator loss: 0.543694, acc: 0.507812]  [Adversarial loss: 0.311703, acc: 0.945312]\n",
            "1185: [Discriminator loss: 0.657079, acc: 0.464844]  [Adversarial loss: 0.352896, acc: 0.941406]\n",
            "1186: [Discriminator loss: 0.505053, acc: 0.566406]  [Adversarial loss: 0.277903, acc: 0.972656]\n",
            "1187: [Discriminator loss: 0.569001, acc: 0.449219]  [Adversarial loss: 0.368727, acc: 0.937500]\n",
            "1188: [Discriminator loss: 0.651713, acc: 0.515625]  [Adversarial loss: 0.311331, acc: 0.953125]\n",
            "1189: [Discriminator loss: 0.657454, acc: 0.460938]  [Adversarial loss: 0.334000, acc: 0.953125]\n",
            "1190: [Discriminator loss: 0.550863, acc: 0.503906]  [Adversarial loss: 0.385190, acc: 0.964844]\n",
            "1191: [Discriminator loss: 0.587080, acc: 0.488281]  [Adversarial loss: 0.324252, acc: 0.960938]\n",
            "1192: [Discriminator loss: 0.582970, acc: 0.460938]  [Adversarial loss: 0.347739, acc: 0.957031]\n",
            "1193: [Discriminator loss: 0.540637, acc: 0.519531]  [Adversarial loss: 0.314518, acc: 0.964844]\n",
            "1194: [Discriminator loss: 0.525090, acc: 0.488281]  [Adversarial loss: 0.344922, acc: 0.964844]\n",
            "1195: [Discriminator loss: 0.546506, acc: 0.527344]  [Adversarial loss: 0.316576, acc: 0.953125]\n",
            "1196: [Discriminator loss: 0.597158, acc: 0.472656]  [Adversarial loss: 0.326208, acc: 0.972656]\n",
            "1197: [Discriminator loss: 0.523469, acc: 0.531250]  [Adversarial loss: 0.288233, acc: 0.964844]\n",
            "1198: [Discriminator loss: 0.596581, acc: 0.476562]  [Adversarial loss: 0.348674, acc: 0.945312]\n",
            "1199: [Discriminator loss: 0.550302, acc: 0.539062]  [Adversarial loss: 0.318815, acc: 0.957031]\n",
            "1200: [Discriminator loss: 0.570764, acc: 0.472656]  [Adversarial loss: 0.326545, acc: 0.957031]\n",
            "1201: [Discriminator loss: 0.545425, acc: 0.554688]  [Adversarial loss: 0.292030, acc: 0.949219]\n",
            "1202: [Discriminator loss: 0.580353, acc: 0.527344]  [Adversarial loss: 0.325552, acc: 0.945312]\n",
            "1203: [Discriminator loss: 0.526486, acc: 0.511719]  [Adversarial loss: 0.337733, acc: 0.933594]\n",
            "1204: [Discriminator loss: 0.539918, acc: 0.496094]  [Adversarial loss: 0.318851, acc: 0.960938]\n",
            "1205: [Discriminator loss: 0.546267, acc: 0.503906]  [Adversarial loss: 0.325178, acc: 0.945312]\n",
            "1206: [Discriminator loss: 0.546015, acc: 0.531250]  [Adversarial loss: 0.308682, acc: 0.937500]\n",
            "1207: [Discriminator loss: 0.558953, acc: 0.523438]  [Adversarial loss: 0.325598, acc: 0.949219]\n",
            "1208: [Discriminator loss: 0.603773, acc: 0.492188]  [Adversarial loss: 0.333837, acc: 0.937500]\n",
            "1209: [Discriminator loss: 0.637694, acc: 0.500000]  [Adversarial loss: 0.305173, acc: 0.953125]\n",
            "1210: [Discriminator loss: 0.603910, acc: 0.511719]  [Adversarial loss: 0.316453, acc: 0.960938]\n",
            "1211: [Discriminator loss: 0.585269, acc: 0.511719]  [Adversarial loss: 0.296923, acc: 0.976562]\n",
            "1212: [Discriminator loss: 0.594180, acc: 0.531250]  [Adversarial loss: 0.309751, acc: 0.953125]\n",
            "1213: [Discriminator loss: 0.598179, acc: 0.511719]  [Adversarial loss: 0.343460, acc: 0.917969]\n",
            "1214: [Discriminator loss: 0.557605, acc: 0.531250]  [Adversarial loss: 0.327943, acc: 0.933594]\n",
            "1215: [Discriminator loss: 0.578595, acc: 0.546875]  [Adversarial loss: 0.315631, acc: 0.925781]\n",
            "1216: [Discriminator loss: 0.534522, acc: 0.546875]  [Adversarial loss: 0.321713, acc: 0.945312]\n",
            "1217: [Discriminator loss: 0.604847, acc: 0.496094]  [Adversarial loss: 0.333715, acc: 0.941406]\n",
            "1218: [Discriminator loss: 0.556444, acc: 0.464844]  [Adversarial loss: 0.338604, acc: 0.960938]\n",
            "1219: [Discriminator loss: 0.592587, acc: 0.488281]  [Adversarial loss: 0.345360, acc: 0.945312]\n",
            "1220: [Discriminator loss: 0.585519, acc: 0.500000]  [Adversarial loss: 0.329194, acc: 0.945312]\n",
            "1221: [Discriminator loss: 0.553893, acc: 0.496094]  [Adversarial loss: 0.322956, acc: 0.964844]\n",
            "1222: [Discriminator loss: 0.513606, acc: 0.535156]  [Adversarial loss: 0.311159, acc: 0.972656]\n",
            "1223: [Discriminator loss: 0.571777, acc: 0.515625]  [Adversarial loss: 0.301990, acc: 0.972656]\n",
            "1224: [Discriminator loss: 0.567428, acc: 0.484375]  [Adversarial loss: 0.348772, acc: 0.949219]\n",
            "1225: [Discriminator loss: 0.550061, acc: 0.500000]  [Adversarial loss: 0.310143, acc: 0.964844]\n",
            "1226: [Discriminator loss: 0.593257, acc: 0.496094]  [Adversarial loss: 0.311901, acc: 0.972656]\n",
            "1227: [Discriminator loss: 0.510180, acc: 0.523438]  [Adversarial loss: 0.296300, acc: 0.968750]\n",
            "1228: [Discriminator loss: 0.537896, acc: 0.511719]  [Adversarial loss: 0.296356, acc: 0.972656]\n",
            "1229: [Discriminator loss: 0.454626, acc: 0.574219]  [Adversarial loss: 0.288440, acc: 0.957031]\n",
            "1230: [Discriminator loss: 0.616939, acc: 0.500000]  [Adversarial loss: 0.304685, acc: 0.964844]\n",
            "1231: [Discriminator loss: 0.594499, acc: 0.503906]  [Adversarial loss: 0.310975, acc: 0.960938]\n",
            "1232: [Discriminator loss: 0.638754, acc: 0.468750]  [Adversarial loss: 0.317655, acc: 0.945312]\n",
            "1233: [Discriminator loss: 0.559386, acc: 0.527344]  [Adversarial loss: 0.287660, acc: 0.968750]\n",
            "1234: [Discriminator loss: 0.496454, acc: 0.574219]  [Adversarial loss: 0.283733, acc: 0.953125]\n",
            "1235: [Discriminator loss: 0.549805, acc: 0.515625]  [Adversarial loss: 0.310710, acc: 0.957031]\n",
            "1236: [Discriminator loss: 0.560691, acc: 0.519531]  [Adversarial loss: 0.292765, acc: 0.972656]\n",
            "1237: [Discriminator loss: 0.569097, acc: 0.488281]  [Adversarial loss: 0.340799, acc: 0.945312]\n",
            "1238: [Discriminator loss: 0.539117, acc: 0.546875]  [Adversarial loss: 0.296669, acc: 0.968750]\n",
            "1239: [Discriminator loss: 0.576361, acc: 0.476562]  [Adversarial loss: 0.338233, acc: 0.945312]\n",
            "1240: [Discriminator loss: 0.599770, acc: 0.523438]  [Adversarial loss: 0.307709, acc: 0.933594]\n",
            "1241: [Discriminator loss: 0.602064, acc: 0.468750]  [Adversarial loss: 0.338053, acc: 0.964844]\n",
            "1242: [Discriminator loss: 0.622262, acc: 0.527344]  [Adversarial loss: 0.323843, acc: 0.964844]\n",
            "1243: [Discriminator loss: 0.525658, acc: 0.523438]  [Adversarial loss: 0.313282, acc: 0.953125]\n",
            "1244: [Discriminator loss: 0.507329, acc: 0.554688]  [Adversarial loss: 0.301644, acc: 0.941406]\n",
            "1245: [Discriminator loss: 0.486503, acc: 0.539062]  [Adversarial loss: 0.316409, acc: 0.960938]\n",
            "1246: [Discriminator loss: 0.543949, acc: 0.523438]  [Adversarial loss: 0.292716, acc: 0.976562]\n",
            "1247: [Discriminator loss: 0.551234, acc: 0.496094]  [Adversarial loss: 0.317566, acc: 0.949219]\n",
            "1248: [Discriminator loss: 0.590808, acc: 0.492188]  [Adversarial loss: 0.292353, acc: 0.984375]\n",
            "1249: [Discriminator loss: 0.526848, acc: 0.535156]  [Adversarial loss: 0.311787, acc: 0.945312]\n",
            "1250: [Discriminator loss: 0.508027, acc: 0.515625]  [Adversarial loss: 0.308643, acc: 0.960938]\n",
            "1251: [Discriminator loss: 0.572844, acc: 0.507812]  [Adversarial loss: 0.307090, acc: 0.957031]\n",
            "1252: [Discriminator loss: 0.694523, acc: 0.449219]  [Adversarial loss: 0.337982, acc: 0.960938]\n",
            "1253: [Discriminator loss: 0.519004, acc: 0.515625]  [Adversarial loss: 0.352859, acc: 0.941406]\n",
            "1254: [Discriminator loss: 0.602382, acc: 0.488281]  [Adversarial loss: 0.317358, acc: 0.964844]\n",
            "1255: [Discriminator loss: 0.562437, acc: 0.535156]  [Adversarial loss: 0.295098, acc: 0.953125]\n",
            "1256: [Discriminator loss: 0.544708, acc: 0.515625]  [Adversarial loss: 0.306850, acc: 0.964844]\n",
            "1257: [Discriminator loss: 0.484843, acc: 0.539062]  [Adversarial loss: 0.321374, acc: 0.941406]\n",
            "1258: [Discriminator loss: 0.561841, acc: 0.515625]  [Adversarial loss: 0.305587, acc: 0.980469]\n",
            "1259: [Discriminator loss: 0.519376, acc: 0.574219]  [Adversarial loss: 0.281286, acc: 0.953125]\n",
            "1260: [Discriminator loss: 0.590732, acc: 0.503906]  [Adversarial loss: 0.316140, acc: 0.953125]\n",
            "1261: [Discriminator loss: 0.594532, acc: 0.535156]  [Adversarial loss: 0.313397, acc: 0.945312]\n",
            "1262: [Discriminator loss: 0.554833, acc: 0.519531]  [Adversarial loss: 0.336851, acc: 0.964844]\n",
            "1263: [Discriminator loss: 0.538182, acc: 0.500000]  [Adversarial loss: 0.311455, acc: 0.953125]\n",
            "1264: [Discriminator loss: 0.583965, acc: 0.527344]  [Adversarial loss: 0.314323, acc: 0.949219]\n",
            "1265: [Discriminator loss: 0.564655, acc: 0.515625]  [Adversarial loss: 0.327214, acc: 0.941406]\n",
            "1266: [Discriminator loss: 0.569707, acc: 0.503906]  [Adversarial loss: 0.319617, acc: 0.960938]\n",
            "1267: [Discriminator loss: 0.525519, acc: 0.542969]  [Adversarial loss: 0.304336, acc: 0.960938]\n",
            "1268: [Discriminator loss: 0.433278, acc: 0.566406]  [Adversarial loss: 0.288522, acc: 0.960938]\n",
            "1269: [Discriminator loss: 0.475295, acc: 0.527344]  [Adversarial loss: 0.322127, acc: 0.953125]\n",
            "1270: [Discriminator loss: 0.567897, acc: 0.511719]  [Adversarial loss: 0.323634, acc: 0.941406]\n",
            "1271: [Discriminator loss: 0.589303, acc: 0.527344]  [Adversarial loss: 0.297798, acc: 0.968750]\n",
            "1272: [Discriminator loss: 0.590006, acc: 0.492188]  [Adversarial loss: 0.301962, acc: 0.968750]\n",
            "1273: [Discriminator loss: 0.581509, acc: 0.480469]  [Adversarial loss: 0.331504, acc: 0.957031]\n",
            "1274: [Discriminator loss: 0.580547, acc: 0.523438]  [Adversarial loss: 0.299850, acc: 0.964844]\n",
            "1275: [Discriminator loss: 0.582491, acc: 0.496094]  [Adversarial loss: 0.330364, acc: 0.957031]\n",
            "1276: [Discriminator loss: 0.580575, acc: 0.492188]  [Adversarial loss: 0.330535, acc: 0.957031]\n",
            "1277: [Discriminator loss: 0.520914, acc: 0.558594]  [Adversarial loss: 0.318256, acc: 0.941406]\n",
            "1278: [Discriminator loss: 0.510938, acc: 0.550781]  [Adversarial loss: 0.305386, acc: 0.925781]\n",
            "1279: [Discriminator loss: 0.608981, acc: 0.484375]  [Adversarial loss: 0.346791, acc: 0.937500]\n",
            "1280: [Discriminator loss: 0.589100, acc: 0.496094]  [Adversarial loss: 0.345114, acc: 0.937500]\n",
            "1281: [Discriminator loss: 0.529487, acc: 0.480469]  [Adversarial loss: 0.366530, acc: 0.937500]\n",
            "1282: [Discriminator loss: 0.560839, acc: 0.523438]  [Adversarial loss: 0.322312, acc: 0.949219]\n",
            "1283: [Discriminator loss: 0.585181, acc: 0.500000]  [Adversarial loss: 0.305360, acc: 0.976562]\n",
            "1284: [Discriminator loss: 0.523198, acc: 0.535156]  [Adversarial loss: 0.291775, acc: 0.976562]\n",
            "1285: [Discriminator loss: 0.523869, acc: 0.539062]  [Adversarial loss: 0.307252, acc: 0.945312]\n",
            "1286: [Discriminator loss: 0.550241, acc: 0.523438]  [Adversarial loss: 0.327288, acc: 0.937500]\n",
            "1287: [Discriminator loss: 0.578840, acc: 0.527344]  [Adversarial loss: 0.310391, acc: 0.957031]\n",
            "1288: [Discriminator loss: 0.559672, acc: 0.464844]  [Adversarial loss: 0.349412, acc: 0.980469]\n",
            "1289: [Discriminator loss: 0.545708, acc: 0.503906]  [Adversarial loss: 0.374695, acc: 0.910156]\n",
            "1290: [Discriminator loss: 0.484961, acc: 0.539062]  [Adversarial loss: 0.326086, acc: 0.949219]\n",
            "1291: [Discriminator loss: 0.517709, acc: 0.531250]  [Adversarial loss: 0.306799, acc: 0.953125]\n",
            "1292: [Discriminator loss: 0.535329, acc: 0.542969]  [Adversarial loss: 0.312621, acc: 0.941406]\n",
            "1293: [Discriminator loss: 0.612856, acc: 0.507812]  [Adversarial loss: 0.334126, acc: 0.945312]\n",
            "1294: [Discriminator loss: 0.521624, acc: 0.503906]  [Adversarial loss: 0.335108, acc: 0.960938]\n",
            "1295: [Discriminator loss: 0.570459, acc: 0.476562]  [Adversarial loss: 0.381390, acc: 0.949219]\n",
            "1296: [Discriminator loss: 0.587485, acc: 0.554688]  [Adversarial loss: 0.289254, acc: 0.945312]\n",
            "1297: [Discriminator loss: 0.507384, acc: 0.496094]  [Adversarial loss: 0.345550, acc: 0.953125]\n",
            "1298: [Discriminator loss: 0.524551, acc: 0.519531]  [Adversarial loss: 0.337872, acc: 0.945312]\n",
            "1299: [Discriminator loss: 0.540439, acc: 0.566406]  [Adversarial loss: 0.305789, acc: 0.960938]\n",
            "1300: [Discriminator loss: 0.536784, acc: 0.523438]  [Adversarial loss: 0.307329, acc: 0.957031]\n",
            "1301: [Discriminator loss: 0.560669, acc: 0.546875]  [Adversarial loss: 0.303118, acc: 0.937500]\n",
            "1302: [Discriminator loss: 0.515915, acc: 0.539062]  [Adversarial loss: 0.338931, acc: 0.945312]\n",
            "1303: [Discriminator loss: 0.584685, acc: 0.480469]  [Adversarial loss: 0.363881, acc: 0.921875]\n",
            "1304: [Discriminator loss: 0.520975, acc: 0.511719]  [Adversarial loss: 0.341116, acc: 0.937500]\n",
            "1305: [Discriminator loss: 0.570650, acc: 0.515625]  [Adversarial loss: 0.302515, acc: 0.964844]\n",
            "1306: [Discriminator loss: 0.509988, acc: 0.558594]  [Adversarial loss: 0.294214, acc: 0.945312]\n",
            "1307: [Discriminator loss: 0.635223, acc: 0.464844]  [Adversarial loss: 0.366594, acc: 0.941406]\n",
            "1308: [Discriminator loss: 0.472054, acc: 0.542969]  [Adversarial loss: 0.304373, acc: 0.960938]\n",
            "1309: [Discriminator loss: 0.600042, acc: 0.507812]  [Adversarial loss: 0.315152, acc: 0.957031]\n",
            "1310: [Discriminator loss: 0.556490, acc: 0.488281]  [Adversarial loss: 0.336254, acc: 0.945312]\n",
            "1311: [Discriminator loss: 0.591188, acc: 0.492188]  [Adversarial loss: 0.330280, acc: 0.953125]\n",
            "1312: [Discriminator loss: 0.444551, acc: 0.601562]  [Adversarial loss: 0.275802, acc: 0.941406]\n",
            "1313: [Discriminator loss: 0.615078, acc: 0.507812]  [Adversarial loss: 0.316549, acc: 0.976562]\n",
            "1314: [Discriminator loss: 0.527693, acc: 0.535156]  [Adversarial loss: 0.305873, acc: 0.960938]\n",
            "1315: [Discriminator loss: 0.623387, acc: 0.468750]  [Adversarial loss: 0.333166, acc: 0.953125]\n",
            "1316: [Discriminator loss: 0.542975, acc: 0.515625]  [Adversarial loss: 0.325564, acc: 0.964844]\n",
            "1317: [Discriminator loss: 0.557902, acc: 0.566406]  [Adversarial loss: 0.312815, acc: 0.941406]\n",
            "1318: [Discriminator loss: 0.640563, acc: 0.445312]  [Adversarial loss: 0.379445, acc: 0.921875]\n",
            "1319: [Discriminator loss: 0.549946, acc: 0.515625]  [Adversarial loss: 0.340483, acc: 0.945312]\n",
            "1320: [Discriminator loss: 0.537508, acc: 0.542969]  [Adversarial loss: 0.312780, acc: 0.949219]\n",
            "1321: [Discriminator loss: 0.511390, acc: 0.492188]  [Adversarial loss: 0.323458, acc: 0.980469]\n",
            "1322: [Discriminator loss: 0.558479, acc: 0.511719]  [Adversarial loss: 0.325950, acc: 0.957031]\n",
            "1323: [Discriminator loss: 0.515749, acc: 0.523438]  [Adversarial loss: 0.340391, acc: 0.964844]\n",
            "1324: [Discriminator loss: 0.575877, acc: 0.460938]  [Adversarial loss: 0.391409, acc: 0.921875]\n",
            "1325: [Discriminator loss: 0.499865, acc: 0.566406]  [Adversarial loss: 0.311155, acc: 0.941406]\n",
            "1326: [Discriminator loss: 0.550842, acc: 0.500000]  [Adversarial loss: 0.300222, acc: 0.968750]\n",
            "1327: [Discriminator loss: 0.541183, acc: 0.488281]  [Adversarial loss: 0.346376, acc: 0.953125]\n",
            "1328: [Discriminator loss: 0.561014, acc: 0.464844]  [Adversarial loss: 0.346838, acc: 0.960938]\n",
            "1329: [Discriminator loss: 0.565414, acc: 0.484375]  [Adversarial loss: 0.317854, acc: 0.968750]\n",
            "1330: [Discriminator loss: 0.525966, acc: 0.515625]  [Adversarial loss: 0.330321, acc: 0.949219]\n",
            "1331: [Discriminator loss: 0.546462, acc: 0.539062]  [Adversarial loss: 0.341684, acc: 0.945312]\n",
            "1332: [Discriminator loss: 0.603371, acc: 0.488281]  [Adversarial loss: 0.333263, acc: 0.957031]\n",
            "1333: [Discriminator loss: 0.545333, acc: 0.531250]  [Adversarial loss: 0.297516, acc: 0.964844]\n",
            "1334: [Discriminator loss: 0.548857, acc: 0.488281]  [Adversarial loss: 0.365677, acc: 0.960938]\n",
            "1335: [Discriminator loss: 0.531655, acc: 0.542969]  [Adversarial loss: 0.308513, acc: 0.945312]\n",
            "1336: [Discriminator loss: 0.565201, acc: 0.480469]  [Adversarial loss: 0.342152, acc: 0.964844]\n",
            "1337: [Discriminator loss: 0.500703, acc: 0.527344]  [Adversarial loss: 0.314573, acc: 0.964844]\n",
            "1338: [Discriminator loss: 0.547242, acc: 0.476562]  [Adversarial loss: 0.351101, acc: 0.945312]\n",
            "1339: [Discriminator loss: 0.532190, acc: 0.496094]  [Adversarial loss: 0.333855, acc: 0.953125]\n",
            "1340: [Discriminator loss: 0.553320, acc: 0.496094]  [Adversarial loss: 0.335032, acc: 0.953125]\n",
            "1341: [Discriminator loss: 0.516164, acc: 0.542969]  [Adversarial loss: 0.294380, acc: 0.964844]\n",
            "1342: [Discriminator loss: 0.610411, acc: 0.476562]  [Adversarial loss: 0.345705, acc: 0.964844]\n",
            "1343: [Discriminator loss: 0.596366, acc: 0.480469]  [Adversarial loss: 0.344510, acc: 0.968750]\n",
            "1344: [Discriminator loss: 0.535202, acc: 0.519531]  [Adversarial loss: 0.292729, acc: 0.968750]\n",
            "1345: [Discriminator loss: 0.560896, acc: 0.464844]  [Adversarial loss: 0.357291, acc: 0.941406]\n",
            "1346: [Discriminator loss: 0.592334, acc: 0.460938]  [Adversarial loss: 0.358670, acc: 0.937500]\n",
            "1347: [Discriminator loss: 0.511690, acc: 0.546875]  [Adversarial loss: 0.324901, acc: 0.945312]\n",
            "1348: [Discriminator loss: 0.512919, acc: 0.566406]  [Adversarial loss: 0.290613, acc: 0.957031]\n",
            "1349: [Discriminator loss: 0.505666, acc: 0.527344]  [Adversarial loss: 0.326193, acc: 0.976562]\n",
            "1350: [Discriminator loss: 0.504181, acc: 0.523438]  [Adversarial loss: 0.332630, acc: 0.972656]\n",
            "1351: [Discriminator loss: 0.498932, acc: 0.578125]  [Adversarial loss: 0.300103, acc: 0.960938]\n",
            "1352: [Discriminator loss: 0.551443, acc: 0.449219]  [Adversarial loss: 0.348767, acc: 0.968750]\n",
            "1353: [Discriminator loss: 0.551678, acc: 0.515625]  [Adversarial loss: 0.308099, acc: 0.976562]\n",
            "1354: [Discriminator loss: 0.549888, acc: 0.523438]  [Adversarial loss: 0.306705, acc: 0.945312]\n",
            "1355: [Discriminator loss: 0.528422, acc: 0.507812]  [Adversarial loss: 0.317998, acc: 0.957031]\n",
            "1356: [Discriminator loss: 0.546834, acc: 0.457031]  [Adversarial loss: 0.376128, acc: 0.953125]\n",
            "1357: [Discriminator loss: 0.512337, acc: 0.500000]  [Adversarial loss: 0.328330, acc: 0.957031]\n",
            "1358: [Discriminator loss: 0.556619, acc: 0.519531]  [Adversarial loss: 0.342258, acc: 0.937500]\n",
            "1359: [Discriminator loss: 0.557041, acc: 0.500000]  [Adversarial loss: 0.316472, acc: 0.941406]\n",
            "1360: [Discriminator loss: 0.558477, acc: 0.519531]  [Adversarial loss: 0.325095, acc: 0.941406]\n",
            "1361: [Discriminator loss: 0.483066, acc: 0.527344]  [Adversarial loss: 0.299527, acc: 0.953125]\n",
            "1362: [Discriminator loss: 0.494620, acc: 0.531250]  [Adversarial loss: 0.337715, acc: 0.960938]\n",
            "1363: [Discriminator loss: 0.627116, acc: 0.511719]  [Adversarial loss: 0.311254, acc: 0.960938]\n",
            "1364: [Discriminator loss: 0.597510, acc: 0.496094]  [Adversarial loss: 0.329057, acc: 0.960938]\n",
            "1365: [Discriminator loss: 0.570757, acc: 0.468750]  [Adversarial loss: 0.342604, acc: 0.957031]\n",
            "1366: [Discriminator loss: 0.519494, acc: 0.531250]  [Adversarial loss: 0.301389, acc: 0.957031]\n",
            "1367: [Discriminator loss: 0.537273, acc: 0.527344]  [Adversarial loss: 0.326175, acc: 0.949219]\n",
            "1368: [Discriminator loss: 0.529036, acc: 0.511719]  [Adversarial loss: 0.292223, acc: 0.968750]\n",
            "1369: [Discriminator loss: 0.545619, acc: 0.527344]  [Adversarial loss: 0.316178, acc: 0.960938]\n",
            "1370: [Discriminator loss: 0.528244, acc: 0.539062]  [Adversarial loss: 0.308793, acc: 0.949219]\n",
            "1371: [Discriminator loss: 0.587107, acc: 0.414062]  [Adversarial loss: 0.392936, acc: 0.949219]\n",
            "1372: [Discriminator loss: 0.559838, acc: 0.500000]  [Adversarial loss: 0.331203, acc: 0.945312]\n",
            "1373: [Discriminator loss: 0.505964, acc: 0.546875]  [Adversarial loss: 0.306997, acc: 0.972656]\n",
            "1374: [Discriminator loss: 0.497751, acc: 0.550781]  [Adversarial loss: 0.299750, acc: 0.972656]\n",
            "1375: [Discriminator loss: 0.502075, acc: 0.527344]  [Adversarial loss: 0.310821, acc: 0.949219]\n",
            "1376: [Discriminator loss: 0.575084, acc: 0.460938]  [Adversarial loss: 0.347162, acc: 0.937500]\n",
            "1377: [Discriminator loss: 0.643335, acc: 0.492188]  [Adversarial loss: 0.333557, acc: 0.937500]\n",
            "1378: [Discriminator loss: 0.480957, acc: 0.531250]  [Adversarial loss: 0.305392, acc: 0.941406]\n",
            "1379: [Discriminator loss: 0.549891, acc: 0.527344]  [Adversarial loss: 0.317219, acc: 0.949219]\n",
            "1380: [Discriminator loss: 0.510882, acc: 0.523438]  [Adversarial loss: 0.310876, acc: 0.941406]\n",
            "1381: [Discriminator loss: 0.510898, acc: 0.523438]  [Adversarial loss: 0.344761, acc: 0.945312]\n",
            "1382: [Discriminator loss: 0.468111, acc: 0.578125]  [Adversarial loss: 0.291760, acc: 0.945312]\n",
            "1383: [Discriminator loss: 0.624590, acc: 0.453125]  [Adversarial loss: 0.361291, acc: 0.953125]\n",
            "1384: [Discriminator loss: 0.561963, acc: 0.511719]  [Adversarial loss: 0.321307, acc: 0.953125]\n",
            "1385: [Discriminator loss: 0.497022, acc: 0.519531]  [Adversarial loss: 0.374976, acc: 0.937500]\n",
            "1386: [Discriminator loss: 0.570123, acc: 0.445312]  [Adversarial loss: 0.367566, acc: 0.941406]\n",
            "1387: [Discriminator loss: 0.535109, acc: 0.500000]  [Adversarial loss: 0.323258, acc: 0.960938]\n",
            "1388: [Discriminator loss: 0.511408, acc: 0.542969]  [Adversarial loss: 0.308741, acc: 0.960938]\n",
            "1389: [Discriminator loss: 0.507052, acc: 0.519531]  [Adversarial loss: 0.322811, acc: 0.949219]\n",
            "1390: [Discriminator loss: 0.565945, acc: 0.480469]  [Adversarial loss: 0.340829, acc: 0.953125]\n",
            "1391: [Discriminator loss: 0.665868, acc: 0.441406]  [Adversarial loss: 0.358363, acc: 0.972656]\n",
            "1392: [Discriminator loss: 0.507865, acc: 0.511719]  [Adversarial loss: 0.330038, acc: 0.957031]\n",
            "1393: [Discriminator loss: 0.534570, acc: 0.527344]  [Adversarial loss: 0.300810, acc: 0.957031]\n",
            "1394: [Discriminator loss: 0.494377, acc: 0.554688]  [Adversarial loss: 0.308136, acc: 0.960938]\n",
            "1395: [Discriminator loss: 0.618862, acc: 0.468750]  [Adversarial loss: 0.342258, acc: 0.964844]\n",
            "1396: [Discriminator loss: 0.517980, acc: 0.507812]  [Adversarial loss: 0.327593, acc: 0.964844]\n",
            "1397: [Discriminator loss: 0.547351, acc: 0.480469]  [Adversarial loss: 0.340015, acc: 0.953125]\n",
            "1398: [Discriminator loss: 0.568695, acc: 0.488281]  [Adversarial loss: 0.340185, acc: 0.945312]\n",
            "1399: [Discriminator loss: 0.523299, acc: 0.515625]  [Adversarial loss: 0.325019, acc: 0.972656]\n",
            "1400: [Discriminator loss: 0.509276, acc: 0.496094]  [Adversarial loss: 0.354568, acc: 0.929688]\n",
            "1401: [Discriminator loss: 0.523615, acc: 0.484375]  [Adversarial loss: 0.429203, acc: 0.953125]\n",
            "1402: [Discriminator loss: 0.533089, acc: 0.507812]  [Adversarial loss: 0.334141, acc: 0.964844]\n",
            "1403: [Discriminator loss: 0.529490, acc: 0.503906]  [Adversarial loss: 0.336860, acc: 0.957031]\n",
            "1404: [Discriminator loss: 0.593183, acc: 0.507812]  [Adversarial loss: 0.315720, acc: 0.957031]\n",
            "1405: [Discriminator loss: 0.631615, acc: 0.488281]  [Adversarial loss: 0.360543, acc: 0.941406]\n",
            "1406: [Discriminator loss: 0.567008, acc: 0.492188]  [Adversarial loss: 0.351952, acc: 0.957031]\n",
            "1407: [Discriminator loss: 0.510614, acc: 0.496094]  [Adversarial loss: 0.371894, acc: 0.949219]\n",
            "1408: [Discriminator loss: 0.537661, acc: 0.464844]  [Adversarial loss: 0.346012, acc: 0.960938]\n",
            "1409: [Discriminator loss: 0.523823, acc: 0.527344]  [Adversarial loss: 0.307440, acc: 0.972656]\n",
            "1410: [Discriminator loss: 0.470503, acc: 0.531250]  [Adversarial loss: 0.304108, acc: 0.960938]\n",
            "1411: [Discriminator loss: 0.486136, acc: 0.539062]  [Adversarial loss: 0.327838, acc: 0.941406]\n",
            "1412: [Discriminator loss: 0.487492, acc: 0.527344]  [Adversarial loss: 0.303821, acc: 0.960938]\n",
            "1413: [Discriminator loss: 0.527398, acc: 0.476562]  [Adversarial loss: 0.314652, acc: 0.984375]\n",
            "1414: [Discriminator loss: 0.515231, acc: 0.539062]  [Adversarial loss: 0.316391, acc: 0.968750]\n",
            "1415: [Discriminator loss: 0.502719, acc: 0.523438]  [Adversarial loss: 0.312771, acc: 0.976562]\n",
            "1416: [Discriminator loss: 0.516570, acc: 0.515625]  [Adversarial loss: 0.311228, acc: 0.957031]\n",
            "1417: [Discriminator loss: 0.590956, acc: 0.484375]  [Adversarial loss: 0.322827, acc: 0.976562]\n",
            "1418: [Discriminator loss: 0.466397, acc: 0.562500]  [Adversarial loss: 0.295229, acc: 0.964844]\n",
            "1419: [Discriminator loss: 0.640414, acc: 0.468750]  [Adversarial loss: 0.372450, acc: 0.937500]\n",
            "1420: [Discriminator loss: 0.554136, acc: 0.507812]  [Adversarial loss: 0.363279, acc: 0.941406]\n",
            "1421: [Discriminator loss: 0.535769, acc: 0.527344]  [Adversarial loss: 0.327053, acc: 0.937500]\n",
            "1422: [Discriminator loss: 0.614586, acc: 0.503906]  [Adversarial loss: 0.346086, acc: 0.941406]\n",
            "1423: [Discriminator loss: 0.586932, acc: 0.476562]  [Adversarial loss: 0.347230, acc: 0.933594]\n",
            "1424: [Discriminator loss: 0.512161, acc: 0.539062]  [Adversarial loss: 0.331828, acc: 0.929688]\n",
            "1425: [Discriminator loss: 0.525566, acc: 0.550781]  [Adversarial loss: 0.339083, acc: 0.921875]\n",
            "1426: [Discriminator loss: 0.495045, acc: 0.492188]  [Adversarial loss: 0.357072, acc: 0.937500]\n",
            "1427: [Discriminator loss: 0.538735, acc: 0.503906]  [Adversarial loss: 0.324216, acc: 0.964844]\n",
            "1428: [Discriminator loss: 0.500139, acc: 0.492188]  [Adversarial loss: 0.349562, acc: 0.953125]\n",
            "1429: [Discriminator loss: 0.557118, acc: 0.441406]  [Adversarial loss: 0.360555, acc: 0.949219]\n",
            "1430: [Discriminator loss: 0.563370, acc: 0.453125]  [Adversarial loss: 0.360466, acc: 0.964844]\n",
            "1431: [Discriminator loss: 0.557878, acc: 0.484375]  [Adversarial loss: 0.352725, acc: 0.941406]\n",
            "1432: [Discriminator loss: 0.546857, acc: 0.476562]  [Adversarial loss: 0.356549, acc: 0.968750]\n",
            "1433: [Discriminator loss: 0.488182, acc: 0.558594]  [Adversarial loss: 0.307357, acc: 0.949219]\n",
            "1434: [Discriminator loss: 0.518776, acc: 0.511719]  [Adversarial loss: 0.303969, acc: 0.972656]\n",
            "1435: [Discriminator loss: 0.571590, acc: 0.546875]  [Adversarial loss: 0.288914, acc: 0.953125]\n",
            "1436: [Discriminator loss: 0.574102, acc: 0.480469]  [Adversarial loss: 0.340043, acc: 0.957031]\n",
            "1437: [Discriminator loss: 0.492539, acc: 0.539062]  [Adversarial loss: 0.305428, acc: 0.953125]\n",
            "1438: [Discriminator loss: 0.522815, acc: 0.539062]  [Adversarial loss: 0.405488, acc: 0.941406]\n",
            "1439: [Discriminator loss: 0.515479, acc: 0.519531]  [Adversarial loss: 0.305605, acc: 0.949219]\n",
            "1440: [Discriminator loss: 0.609480, acc: 0.523438]  [Adversarial loss: 0.295445, acc: 0.976562]\n",
            "1441: [Discriminator loss: 0.516417, acc: 0.535156]  [Adversarial loss: 0.327630, acc: 0.953125]\n",
            "1442: [Discriminator loss: 0.540025, acc: 0.539062]  [Adversarial loss: 0.297828, acc: 0.945312]\n",
            "1443: [Discriminator loss: 0.482302, acc: 0.546875]  [Adversarial loss: 0.321070, acc: 0.957031]\n",
            "1444: [Discriminator loss: 0.417023, acc: 0.578125]  [Adversarial loss: 0.291947, acc: 0.964844]\n",
            "1445: [Discriminator loss: 0.554463, acc: 0.496094]  [Adversarial loss: 0.348402, acc: 0.953125]\n",
            "1446: [Discriminator loss: 0.510822, acc: 0.542969]  [Adversarial loss: 0.309584, acc: 0.964844]\n",
            "1447: [Discriminator loss: 0.554258, acc: 0.531250]  [Adversarial loss: 0.331206, acc: 0.945312]\n",
            "1448: [Discriminator loss: 0.529699, acc: 0.457031]  [Adversarial loss: 0.343468, acc: 0.960938]\n",
            "1449: [Discriminator loss: 0.503278, acc: 0.539062]  [Adversarial loss: 0.321953, acc: 0.960938]\n",
            "1450: [Discriminator loss: 0.480770, acc: 0.550781]  [Adversarial loss: 0.294794, acc: 0.960938]\n",
            "1451: [Discriminator loss: 0.427537, acc: 0.605469]  [Adversarial loss: 0.284106, acc: 0.964844]\n",
            "1452: [Discriminator loss: 0.548825, acc: 0.453125]  [Adversarial loss: 0.354289, acc: 0.949219]\n",
            "1453: [Discriminator loss: 0.621908, acc: 0.468750]  [Adversarial loss: 0.347438, acc: 0.937500]\n",
            "1454: [Discriminator loss: 0.538499, acc: 0.527344]  [Adversarial loss: 0.321573, acc: 0.960938]\n",
            "1455: [Discriminator loss: 0.613268, acc: 0.460938]  [Adversarial loss: 0.373013, acc: 0.945312]\n",
            "1456: [Discriminator loss: 0.598247, acc: 0.476562]  [Adversarial loss: 0.343631, acc: 0.957031]\n",
            "1457: [Discriminator loss: 0.573829, acc: 0.527344]  [Adversarial loss: 0.318582, acc: 0.957031]\n",
            "1458: [Discriminator loss: 0.567862, acc: 0.464844]  [Adversarial loss: 0.370079, acc: 0.945312]\n",
            "1459: [Discriminator loss: 0.537284, acc: 0.511719]  [Adversarial loss: 0.341444, acc: 0.949219]\n",
            "1460: [Discriminator loss: 0.477718, acc: 0.562500]  [Adversarial loss: 0.304092, acc: 0.949219]\n",
            "1461: [Discriminator loss: 0.518416, acc: 0.523438]  [Adversarial loss: 0.298711, acc: 0.964844]\n",
            "1462: [Discriminator loss: 0.495724, acc: 0.500000]  [Adversarial loss: 0.331256, acc: 0.964844]\n",
            "1463: [Discriminator loss: 0.490863, acc: 0.496094]  [Adversarial loss: 0.348242, acc: 0.957031]\n",
            "1464: [Discriminator loss: 0.502382, acc: 0.511719]  [Adversarial loss: 0.324772, acc: 0.964844]\n",
            "1465: [Discriminator loss: 0.551434, acc: 0.457031]  [Adversarial loss: 0.348832, acc: 0.968750]\n",
            "1466: [Discriminator loss: 0.533652, acc: 0.472656]  [Adversarial loss: 0.352698, acc: 0.953125]\n",
            "1467: [Discriminator loss: 0.546069, acc: 0.492188]  [Adversarial loss: 0.320747, acc: 0.972656]\n",
            "1468: [Discriminator loss: 0.593336, acc: 0.507812]  [Adversarial loss: 0.327663, acc: 0.953125]\n",
            "1469: [Discriminator loss: 0.507616, acc: 0.523438]  [Adversarial loss: 0.306317, acc: 0.968750]\n",
            "1470: [Discriminator loss: 0.480326, acc: 0.523438]  [Adversarial loss: 0.342450, acc: 0.957031]\n",
            "1471: [Discriminator loss: 0.464920, acc: 0.582031]  [Adversarial loss: 0.260187, acc: 0.968750]\n",
            "1472: [Discriminator loss: 0.512013, acc: 0.496094]  [Adversarial loss: 0.331395, acc: 0.957031]\n",
            "1473: [Discriminator loss: 0.597208, acc: 0.472656]  [Adversarial loss: 0.345372, acc: 0.945312]\n",
            "1474: [Discriminator loss: 0.549572, acc: 0.480469]  [Adversarial loss: 0.355596, acc: 0.945312]\n",
            "1475: [Discriminator loss: 0.568270, acc: 0.484375]  [Adversarial loss: 0.354861, acc: 0.949219]\n",
            "1476: [Discriminator loss: 0.511921, acc: 0.535156]  [Adversarial loss: 0.290927, acc: 0.972656]\n",
            "1477: [Discriminator loss: 0.556028, acc: 0.515625]  [Adversarial loss: 0.346733, acc: 0.960938]\n",
            "1478: [Discriminator loss: 0.524568, acc: 0.515625]  [Adversarial loss: 0.322856, acc: 0.972656]\n",
            "1479: [Discriminator loss: 0.545732, acc: 0.488281]  [Adversarial loss: 0.340854, acc: 0.949219]\n",
            "1480: [Discriminator loss: 0.455957, acc: 0.542969]  [Adversarial loss: 0.340833, acc: 0.945312]\n",
            "1481: [Discriminator loss: 0.578816, acc: 0.480469]  [Adversarial loss: 0.320497, acc: 0.976562]\n",
            "1482: [Discriminator loss: 0.514347, acc: 0.496094]  [Adversarial loss: 0.329698, acc: 0.957031]\n",
            "1483: [Discriminator loss: 0.498700, acc: 0.500000]  [Adversarial loss: 0.332550, acc: 0.968750]\n",
            "1484: [Discriminator loss: 0.552188, acc: 0.484375]  [Adversarial loss: 0.349285, acc: 0.964844]\n",
            "1485: [Discriminator loss: 0.588447, acc: 0.472656]  [Adversarial loss: 0.329691, acc: 0.960938]\n",
            "1486: [Discriminator loss: 0.570173, acc: 0.484375]  [Adversarial loss: 0.344505, acc: 0.980469]\n",
            "1487: [Discriminator loss: 0.490395, acc: 0.539062]  [Adversarial loss: 0.341180, acc: 0.933594]\n",
            "1488: [Discriminator loss: 0.500848, acc: 0.496094]  [Adversarial loss: 0.321089, acc: 0.960938]\n",
            "1489: [Discriminator loss: 0.584920, acc: 0.523438]  [Adversarial loss: 0.322375, acc: 0.957031]\n",
            "1490: [Discriminator loss: 0.591471, acc: 0.527344]  [Adversarial loss: 0.312577, acc: 0.964844]\n",
            "1491: [Discriminator loss: 0.606118, acc: 0.484375]  [Adversarial loss: 0.363029, acc: 0.949219]\n",
            "1492: [Discriminator loss: 0.521302, acc: 0.484375]  [Adversarial loss: 0.343089, acc: 0.933594]\n",
            "1493: [Discriminator loss: 0.599439, acc: 0.464844]  [Adversarial loss: 0.348646, acc: 0.960938]\n",
            "1494: [Discriminator loss: 0.545315, acc: 0.523438]  [Adversarial loss: 0.365625, acc: 0.933594]\n",
            "1495: [Discriminator loss: 0.486386, acc: 0.562500]  [Adversarial loss: 0.293506, acc: 0.960938]\n",
            "1496: [Discriminator loss: 0.534375, acc: 0.519531]  [Adversarial loss: 0.352915, acc: 0.957031]\n",
            "1497: [Discriminator loss: 0.581036, acc: 0.453125]  [Adversarial loss: 0.393834, acc: 0.921875]\n",
            "1498: [Discriminator loss: 0.511012, acc: 0.507812]  [Adversarial loss: 0.350738, acc: 0.941406]\n",
            "1499: [Discriminator loss: 0.502950, acc: 0.546875]  [Adversarial loss: 0.359988, acc: 0.925781]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jy90r3iq9X4"
      },
      "source": [
        "#### Plot Diabetes BBGAN Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "BgnmO7EnrBpK",
        "outputId": "a4b4ecc1-69a9-41c0-90c0-67a55a481a4b"
      },
      "source": [
        "plot_graphs(discriminator_losses, adversarial_losses, \"discriminator_loss\", \"adversarial_loss\", \"Discriminator and adversarial losses\", \"steps\", \"loss\")\n",
        "plot_graphs(discriminator_accuracies, adversarial_accuracies, \"discriminator_accuracy\", \"adversarial_accuracy\", \"Discriminator and adversarial accuracies\", \"steps\", \"accuracy\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McuB88ugtCEZ"
      },
      "source": [
        "#### Generate Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "TAByOAvU3eb-",
        "outputId": "0be813f7-38ec-4e40-b4e8-e21d3d837581"
      },
      "source": [
        "gen_samples, c = diabetes_bbgan.generate_samples(num_samples=1000)\n",
        "descaled_gen_samples = diabetes_scaler.inverse_transform(gen_samples)\n",
        "descaled_gen_samples = pd.DataFrame(descaled_gen_samples)\n",
        "\n",
        "descaled_gen_samples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>67.099998</td>\n",
              "      <td>2.420000</td>\n",
              "      <td>21.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>6.428891e-11</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>1.808012e-09</td>\n",
              "      <td>67.099998</td>\n",
              "      <td>2.420000</td>\n",
              "      <td>21.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>67.099998</td>\n",
              "      <td>2.420000</td>\n",
              "      <td>21.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>67.099998</td>\n",
              "      <td>2.420000</td>\n",
              "      <td>21.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>67.099998</td>\n",
              "      <td>2.420000</td>\n",
              "      <td>21.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>0.004797</td>\n",
              "      <td>1.988624e+02</td>\n",
              "      <td>0.071318</td>\n",
              "      <td>0.157585</td>\n",
              "      <td>8.454453e+02</td>\n",
              "      <td>0.714622</td>\n",
              "      <td>0.092546</td>\n",
              "      <td>80.957558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>67.099998</td>\n",
              "      <td>2.420000</td>\n",
              "      <td>21.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>67.099998</td>\n",
              "      <td>2.420000</td>\n",
              "      <td>21.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>67.099998</td>\n",
              "      <td>2.420000</td>\n",
              "      <td>21.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>2.744398e-19</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>2.737976e-17</td>\n",
              "      <td>67.099998</td>\n",
              "      <td>2.420000</td>\n",
              "      <td>21.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0             1           2  ...          5         6          7\n",
              "0    17.000000  0.000000e+00  122.000000  ...  67.099998  2.420000  21.000000\n",
              "1    17.000000  6.428891e-11  122.000000  ...  67.099998  2.420000  21.000000\n",
              "2    17.000000  0.000000e+00  122.000000  ...  67.099998  2.420000  21.000000\n",
              "3    17.000000  0.000000e+00  122.000000  ...  67.099998  2.420000  21.000000\n",
              "4    17.000000  0.000000e+00  122.000000  ...  67.099998  2.420000  21.000000\n",
              "..         ...           ...         ...  ...        ...       ...        ...\n",
              "995   0.004797  1.988624e+02    0.071318  ...   0.714622  0.092546  80.957558\n",
              "996  17.000000  0.000000e+00  122.000000  ...  67.099998  2.420000  21.000000\n",
              "997  17.000000  0.000000e+00  122.000000  ...  67.099998  2.420000  21.000000\n",
              "998  17.000000  0.000000e+00  122.000000  ...  67.099998  2.420000  21.000000\n",
              "999  17.000000  2.744398e-19  122.000000  ...  67.099998  2.420000  21.000000\n",
              "\n",
              "[1000 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "b5vAhqxPS1tW",
        "outputId": "fa93ad32-6d8f-45f1-b1e4-4e37538cecb1"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "principalComponentsOriginal = pca.fit_transform(diabetes_data)\n",
        "originalPrincipalDf = pd.DataFrame(data = principalComponentsOriginal, columns = ['principal component 1', 'principal component 2'])\n",
        "principalComponents = pca.transform(descaled_gen_samples)\n",
        "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
        "principalDf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>principal component 1</th>\n",
              "      <th>principal component 2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-84.946224</td>\n",
              "      <td>106.784651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-84.946224</td>\n",
              "      <td>106.784651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-84.946224</td>\n",
              "      <td>106.784651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-84.946224</td>\n",
              "      <td>106.784651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-84.946224</td>\n",
              "      <td>106.784651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>765.047447</td>\n",
              "      <td>0.135006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>-84.946224</td>\n",
              "      <td>106.784651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>-84.946224</td>\n",
              "      <td>106.784651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>-84.946224</td>\n",
              "      <td>106.784651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>-84.946224</td>\n",
              "      <td>106.784651</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     principal component 1  principal component 2\n",
              "0               -84.946224             106.784651\n",
              "1               -84.946224             106.784651\n",
              "2               -84.946224             106.784651\n",
              "3               -84.946224             106.784651\n",
              "4               -84.946224             106.784651\n",
              "..                     ...                    ...\n",
              "995             765.047447               0.135006\n",
              "996             -84.946224             106.784651\n",
              "997             -84.946224             106.784651\n",
              "998             -84.946224             106.784651\n",
              "999             -84.946224             106.784651\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmnXfWFrNC9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88cb2045-cc81-415c-d22d-6bd5d07ff374"
      },
      "source": [
        "y = diabetes_rf_clf.predict_proba(gen_samples)[:, 1]\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.01, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.01, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.02, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.01, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.02, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.01, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.01, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.01,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.02, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.02, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.01, 0.01, 0.05, 0.05, 0.05, 0.01, 0.01, 0.05,\n",
              "       0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05,\n",
              "       0.01, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.02,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05,\n",
              "       0.01, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.05,\n",
              "       0.05, 0.05, 0.05, 0.05, 0.05, 0.01, 0.05, 0.05, 0.05, 0.05])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "UvdRAk8BV5-5",
        "outputId": "9ecc6891-bbea-444b-b941-c5e0326d1c2a"
      },
      "source": [
        "plt.hist(y, rwidth=0.8)\n",
        "plt.xlabel(\"Confidence\")\n",
        "plt.ylabel(\"Number of cases\")\n",
        "plt.title(\"Confidence scores distribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-38vrFtTTk_7"
      },
      "source": [
        "diabetes_rf_clf.predict_proba(diabetes_data)[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "091Kk4JV5yhG"
      },
      "source": [
        "###German Credit Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoXk_mDk6Ses"
      },
      "source": [
        "####Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "5a9rG62j6QrL",
        "outputId": "a9012f09-71d4-4976-b499-e895dfb4f28a"
      },
      "source": [
        "german_data_scaled"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.029412</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.050567</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.375</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.313690</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.053571</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.625</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.101574</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.535714</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.419941</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.464286</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.254209</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.607143</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>0.625</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.081765</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.382353</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.198470</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0.625</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.030483</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.339286</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.602941</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.087763</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0.375</td>\n",
              "      <td>0.602941</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.238032</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1         2    3     4         5  ...        16     17   18    19    20\n",
              "0    0.125  0.029412  0.1  0.05  0.050567  ...  0.333333  0.125  0.0  0.25  0.25\n",
              "1    0.375  0.647059  0.3  0.05  0.313690  ...  0.000000  0.125  0.0  0.75  0.25\n",
              "2    0.625  0.117647  0.1  0.15  0.101574  ...  0.000000  0.375  1.0  0.75  0.25\n",
              "3    0.125  0.558824  0.3  0.25  0.419941  ...  0.000000  0.125  1.0  0.75  0.25\n",
              "4    0.125  0.294118  0.5  0.35  0.254209  ...  0.333333  0.125  1.0  0.75  0.25\n",
              "..     ...       ...  ...   ...       ...  ...       ...    ...  ...   ...   ...\n",
              "995  0.625  0.117647  0.3  0.25  0.081765  ...  0.000000  0.375  0.0  0.75  0.25\n",
              "996  0.125  0.382353  0.3  0.45  0.198470  ...  0.000000  0.625  0.0  0.25  0.25\n",
              "997  0.625  0.117647  0.3  0.05  0.030483  ...  0.000000  0.125  0.0  0.75  0.25\n",
              "998  0.125  0.602941  0.3  0.05  0.087763  ...  0.000000  0.125  0.0  0.25  0.25\n",
              "999  0.375  0.602941  0.1  0.45  0.238032  ...  0.000000  0.125  0.0  0.75  0.25\n",
              "\n",
              "[1000 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad7fEuV27xTd"
      },
      "source": [
        "X = german_data_scaled\n",
        "y = german_data_class.astype('int')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlJBITpO8Bcr"
      },
      "source": [
        "####German Credit Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuSHPBqp8CPp",
        "outputId": "ca75e03a-76d8-4c1a-ffee-c824bf4390d4"
      },
      "source": [
        "german_credit_rf_clf = RandomForestClassifier()\n",
        "german_credit_rf_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bamjr0At8LUi",
        "outputId": "9526cfd4-0c6c-449d-c255-ac7d2b198b55"
      },
      "source": [
        "german_credit_confidence = german_credit_rf_clf.predict_proba(X_test)[:, 1]\n",
        "print(german_credit_confidence)\n",
        "\n",
        "print(f\"max confidence score: {max(german_credit_confidence)}\")\n",
        "print(f\"min confidence score: {min(german_credit_confidence)}\")\n",
        "print(f\"avg confidence score: {np.mean(german_credit_confidence)}\")\n",
        "german_credit_pred = german_credit_rf_clf.predict(X_test)\n",
        "print(f\"german credit rf classifier accuracy: {accuracy_score(y_test, german_credit_pred)}\")\n",
        "\n",
        "\n",
        "german_credit_pred = german_credit_rf_clf.predict(X_test)\n",
        "print(f\"accuracy: {accuracy_score(y_test, german_credit_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.37 0.42 0.6  0.2  0.03 0.36 0.1  0.45 0.24 0.32 0.13 0.51 0.1  0.81\n",
            " 0.86 0.35 0.11 0.16 0.34 0.22 0.24 0.36 0.09 0.04 0.06 0.47 0.62 0.41\n",
            " 0.36 0.1  0.39 0.11 0.38 0.25 0.14 0.27 0.54 0.05 0.1  0.63 0.28 0.04\n",
            " 0.26 0.03 0.18 0.27 0.52 0.39 0.37 0.34 0.56 0.11 0.39 0.51 0.23 0.19\n",
            " 0.08 0.68 0.57 0.17 0.35 0.59 0.43 0.07 0.33 0.36 0.02 0.46 0.37 0.16\n",
            " 0.29 0.19 0.15 0.43 0.25 0.49 0.27 0.33 0.16 0.46 0.18 0.12 0.03 0.4\n",
            " 0.19 0.48 0.68 0.72 0.16 0.05 0.39 0.05 0.9  0.45 0.11 0.26 0.11 0.4\n",
            " 0.72 0.29 0.01 0.36 0.13 0.06 0.1  0.38 0.28 0.48 0.16 0.57 0.63 0.18\n",
            " 0.33 0.16 0.33 0.24 0.35 0.16 0.18 0.47 0.25 0.46 0.23 0.2  0.31 0.23\n",
            " 0.41 0.19 0.18 0.13 0.2  0.26 0.31 0.14 0.42 0.4  0.45 0.27 0.27 0.33\n",
            " 0.66 0.45 0.59 0.04 0.34 0.59 0.29 0.29 0.42 0.56 0.07 0.24 0.14 0.29\n",
            " 0.07 0.4  0.4  0.16 0.27 0.28 0.54 0.19 0.52 0.68 0.31 0.31 0.04 0.19\n",
            " 0.21 0.43 0.15 0.23 0.36 0.32 0.03 0.36 0.44 0.15 0.27 0.58 0.13 0.09\n",
            " 0.15 0.1  0.17 0.11 0.44 0.06 0.25 0.08 0.21 0.57 0.5  0.38 0.13 0.06\n",
            " 0.64 0.54 0.6  0.32 0.74 0.16 0.16 0.37 0.47 0.74 0.08 0.44 0.42 0.13\n",
            " 0.19 0.6  0.68 0.35 0.26 0.03 0.12 0.4  0.36 0.3  0.05 0.15 0.22 0.29\n",
            " 0.48 0.04 0.04 0.28 0.05 0.4  0.22 0.   0.53 0.4  0.27 0.49 0.13 0.06\n",
            " 0.2  0.62 0.57 0.06 0.13 0.38 0.41 0.29 0.16 0.25 0.11 0.8  0.29 0.27\n",
            " 0.62 0.06 0.38 0.4  0.12 0.23 0.52 0.29 0.46 0.3  0.19 0.13 0.35 0.25\n",
            " 0.26 0.43 0.69 0.63 0.03 0.27 0.17 0.21 0.19 0.64 0.04 0.19 0.14 0.6\n",
            " 0.36 0.27 0.17 0.17 0.08 0.07 0.67 0.25 0.55 0.27 0.64 0.33 0.67 0.33\n",
            " 0.28 0.23 0.53 0.41 0.38 0.55]\n",
            "max confidence score: 0.9\n",
            "min confidence score: 0.0\n",
            "avg confidence score: 0.30846666666666667\n",
            "german credit rf classifier accuracy: 0.78\n",
            "accuracy: 0.78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "ij47tdbIC5hX",
        "outputId": "47b6afc1-24ec-4499-9ca3-03d1adbcda00"
      },
      "source": [
        "plt.hist(german_credit_confidence, rwidth=0.8)\n",
        "plt.xlabel(\"Confidence\")\n",
        "plt.ylabel(\"Number of cases\")\n",
        "plt.title(\"Confidence scores distribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAbkUlEQVR4nO3deZRlZX3u8e8jzaTM0CKD0CIkihNiazRoHIAEwQtexSlIUBk0lyTmaow44ABEUJdT4oiQSBxQwYAoJkqQhmWCQqOIIhoGmzDTINCtXpHhd//Yu/DQ6arePexTu7q+n7XOqj2dd//qrep6eg/n3akqJEkamodMdwGSJC2PASVJGiQDSpI0SAaUJGmQDChJ0iAZUJKkQTKgNAhJNkzytSR3JTktyUFJvjXF9guSHDbOGtdWSRYl2audfmuSk9Zg279MslM7/Zkkx63Btj+Z5Og11Z6GZ850F6CZJcmfAm8AHgMsBS4F/q6qvrOaTR8IbA1sWVX3tss+v5ptaiVV1Xu6bJdkAfC5qpoyzKpqozVRV5JXAYdV1TNH2n7dmmhbw+URlDpL8gbgw8B7aMJkB+DjwAFroPkdgf8aCSeNSDKj/jM50+rVMBlQ6iTJpsAxwJFV9S9V9auquqeqvlZVb2q3WT/Jh5Pc2L4+nGT9dt1zklyf5I1Jbk1yU5JXt+veDbwDeFl7SujQJK9K8p2R/e+d5KftKcCPAlmmvtckuSLJHUm+mWTHkXWV5HVJrkxyZ5KPJcnI+sPb9y5N8pMku7fLt03ylSSLk/w8yV9N0T/7tu9dmuSGJH8zsu6AJJcmWZLk6iT7jLR/VpJfJLkqyeEj73lXktOTfC7JEuBVSTZNcnLbdzckOS7JOu32Oyc5v+2f25J8aYpaD05ybZLbk7xtmXXvSvK5dnqDdv+3t/12cZKtk/wd8Czgo+3P66Mj/XxkkiuBK0eW7Tyyi62SnNP20/kTP6ck89pt54zUsiDJYUkeC3wSeEa7vzvb9Q86Zdj+HK9q+/OsJNt2/R3QQFWVL18rfAH7APcCc6bY5hjgu8DDgbnAfwLHtuue077/GGBdYF/g18Dm7fp30ZwymmjrVcB32umtaE4nHti+9/+2bR3Wrj8AuAp4LM1p67cD/znSVgFfBzajOepbDOzTrnsJcAPwVJrQ25nmaO4hwCU0wbkesBNwDfAnk3zvNwHPaqc3B3Zvp58G3AXs3ba5HfCYdt0FNEegGwC7tXU9b6Q/7gFe2L5vQ+AM4FPAw9o+vgh4bbv9qcDb2m03AJ45SZ27Ar8E/ghYH/hg25d7LftzAF4LfA14KLAO8BRgk3bdgon+X6afzwG2ADYcWbZzO/2Z9uc4se+PjPyM57Xbzhlp74F9jP4+jKz/DHBcO/084DZg97btfwAu6PI74Gu4L4+g1NWWwG019Sm4g4BjqurWqloMvBs4eGT9Pe36e6rqGzR/KH+/w773BS6vqtOr6h6a04w3j6x/HXB8VV3R1vceYLfRoyjghKq6s6r+GziPJhAADgPeV1UXV+OqqrqWJrDmVtUxVfXbqroG+DTw8klqvAfYNckmVXVHVX2/XX4o8I9VdU5V3V9VN1TVT5M8EtgDeHNV/aaqLgVOAv5spM0Lq+rMqrof2KTth7+u5uj1VuBDI/XcQxOs27btTXZN8EDg61V1QVXdDRwN3D/F97QlTcDcV1WXVNWSSbadcHxV/aKq/t8k688e2ffbaI6KHrmCNrs4iKafv9+2/Za27Xkj20z2O6CBMqDU1e00p2emurawLXDtyPy17bIH2lgm4H4NdLmIvi1w3cRMVdXoPM0f5o+0p27uBH5BczS03cg2o4E2ut9HAlcvZ587AttOtNm2+1aaa2/L82KaALm2PXX1jBW0vy3wi6paOrLs2mVqXvZ7XBe4aaSeT9EcSQH8Lc33fFGSy5O8ZpI6l+3LX9H8bJfns8A3gS+mOWX7viTrTrLt8mqecn1V/ZLmZ7Xt5Jt39qDfvbbt2+n2O6CBMqDU1YXA3TSnnCZzI80f0gk7tMtW1000f+gBaK8djP6v+zqaU12bjbw2rKr/7ND2dcCjJ1n+82Xa3Liq9l1eI+0R2AE0gXEm8OUVtH8jsEWSjUeW7UBzuvGBZpep525gq5F6Nqmqx7X7v7mqDq+qbWlOzX18mWs/E5bty4fSHCUt73u6p6reXVW7An8IvIDfHeFN9hiEFT0eYXTfG9GcDrwR+FW7+KEj2z5iJdp90O9ekofRfF83TPoODZ4BpU6q6i6a6zEfS/LCJA9Nsm6S5yd5X7vZqcDbk8xNslW7/efWwO7PBh6X5EXtEdxf8eA/Xp8E3pLkcdDc0JHkJR3bPgn4myRPSWPn9tTgRcDSJG9O8xmtdZI8PslTl20gyXppPre1aXsKcgm/O212MvDqJHsmeUiS7ZI8pqquo7lGd3x7M8ITaU4HLre/quom4FvAB5Js0rb16CTPbmt4SZLt283voPmDvrxTd6cDL0jyzCTr0VwTXO7fgSTPTfKENDdiLKE55TfR5i001+VW1r4j+z4W+G5VXdeeEr4BeGXb16/hwcF+C7B9+77lOZWmn3dLc2POe4DvVdWiVahRA2FAqbOq+gDNZ6DeTnOR+TrgL2iOGACOAxYClwE/Ar7fLlvd/d5GczPDCTSnbXYB/mNk/RnAe2lORS0Bfgw8v2PbpwF/B3yB5gL+mcAWVXUfzRHDbsDPaS7AnwRsOklTBwOL2v2/juaaCFV1EfBqmutFdwHn87v/6b+C5uaAG2lugHhnVf37FOX+Gc0NGz+hCaHTgW3adU8Fvpfkl8BZwOvb62bLfr+XA0e23+9NbTvXT7K/R7T7WAJc0db+2XbdR4AD09w1+fdT1LysLwDvpDm19xTglSPrDgfeRPMzfhxNgE/4NnA5cHOS25bzff07zfW0r7Tf16OZ/HqhZog0p/MlSRoWj6AkSYNkQEmSBsmAkiQNkgElSRqkGTGg41ZbbVXz5s2b7jIkST245JJLbququcsunxEBNW/ePBYuXDjdZUiSepDk2uUt9xSfJGmQDChJ0iAZUJKkQTKgJEmDZEBJkgbJgJIkDZIBJUkaJANKkjRIBpQkaZBmxEgSa5t5R5091v0tOmG/se5PktYEj6AkSYNkQEmSBsmAkiQNkgElSRqkXm+SSLIIWArcB9xbVfOTbAF8CZgHLAJeWlV39FmHJGnmGccR1HOrareqmt/OHwWcW1W7AOe285IkPch0nOI7ADilnT4FeOE01CBJGri+PwdVwLeSFPCpqjoR2LqqbmrX3wxsvbw3JjkCOAJghx126LlMDYGfD5M0qu+AemZV3ZDk4cA5SX46urKqqg2v/6ENsxMB5s+fv9xtJElrr15P8VXVDe3XW4EzgKcBtyTZBqD9emufNUiSZqbeAirJw5JsPDEN/DHwY+As4JB2s0OAr/ZVgyRp5urzFN/WwBlJJvbzhar6tyQXA19OcihwLfDSHmuQJM1QvQVUVV0DPGk5y28H9uxrv5KktYMjSUiSBsmAkiQNkgElSRokA0qSNEgGlCRpkAwoSdIg9T3UkQbO8e+Wz36Rpp9HUJKkQTKgJEmDZEBJkgbJgJIkDZIBJUkaJANKkjRIBpQkaZAMKEnSIBlQkqRBMqAkSYNkQEmSBsmAkiQNkgElSRokA0qSNEgGlCRpkAwoSdIgGVCSpEEyoCRJg2RASZIGyYCSJA2SASVJGiQDSpI0SAaUJGmQDChJ0iD1HlBJ1knygyRfb+cfleR7Sa5K8qUk6/VdgyRp5hnHEdTrgStG5t8LfKiqdgbuAA4dQw2SpBmm14BKsj2wH3BSOx/gecDp7SanAC/sswZJ0szU9xHUh4G/Be5v57cE7qyqe9v564HtlvfGJEckWZhk4eLFi3suU5I0NL0FVJIXALdW1SWr8v6qOrGq5lfV/Llz567h6iRJQzenx7b3APZPsi+wAbAJ8BFgsyRz2qOo7YEbeqxBkjRD9XYEVVVvqartq2oe8HLg21V1EHAecGC72SHAV/uqQZI0c03H56DeDLwhyVU016ROnoYaJEkD1+cpvgdU1QJgQTt9DfC0cexXkjRzOZKEJGmQDChJ0iAZUJKkQTKgJEmDZEBJkgZphQGVZI8kD2unX5nkg0l27L80SdJs1uUI6hPAr5M8CXgjcDXwz71WJUma9boE1L1VVcABwEer6mPAxv2WJUma7bp8UHdpkrcABwPPSvIQYN1+y5IkzXZdjqBeBtwNvKaqbqYZ4PX9vVYlSZr1VngEVVU3J/kKsEu76DbgjF6r6sG8o84e6/4WnbDfWPcnSWubLnfxHU7zBNxPtYu2A87ssyhJkrqc4juS5tlOSwCq6krg4X0WJUlSl4C6u6p+OzGTZA5Q/ZUkSVK3gDo/yVuBDZPsDZwGfK3fsiRJs12XgDoKWAz8CHgt8A3g7X0WJUlSl7v47gc+DXw6yRbA9u0HdyVJ6k2Xu/gWJNmkDadLaILqQ/2XJkmazbqc4tu0qpYALwL+uar+ANiz37IkSbNdl4Cak2Qb4KXA13uuR5IkoFtAHQN8E7iqqi5OshNwZb9lSZJmuy43SZxGc2v5xPw1wIv7LEqSpBUGVJINgEOBxwEbTCyvqtf0WJckaZbrcorvs8AjgD8BzqcZzXxpn0VJktQloHauqqOBX1XVKcB+wB/0W5YkabbrElD3tF/vTPJ4YFMcLFaS1LMuT9Q9McnmNMMbnQVsBLyj16okSbNel7v4TmonLwB26rccSZIaXYY6ek+SzUbmN09yXL9lSZJmuy7XoJ5fVXdOzFTVHcC+/ZUkSVK3gFonyfoTM0k2BNafYntJklZbl5skPg+cm+Sf2vlXA6es6E3tB3wvoAmzOcDpVfXOJI8CvghsSTM6+sGjT+yVJAk6HEFV1XuB44DHtq9jq+p9Hdq+G3heVT0J2A3YJ8nTgfcCH6qqnYE7aEapkCTpQbocQVFV/wb828o03D7U8Jft7Lrtq4DnAX/aLj8FeBfwiZVpW5K09utyDWqVJVknyaXArcA5wNXAnVV1b7vJ9cB2k7z3iCQLkyxcvHhxn2VKkgao14Cqqvuqajea8fueBjxmJd57YlXNr6r5c+fO7a1GSdIwTRpQSc5tv753dXfS3qZ+HvAMYLMkE6cWtwduWN32JUlrn6mOoLZJ8ofA/kmenGT30deKGk4yd+IDvu2t6XsDV9AE1YHtZocAX129b0GStDaa6iaJdwBH0xzlfHCZdRM3O0xlG+CUJOvQBOGXq+rrSX4CfLEdjeIHwMmrVLkkaa02aUBV1enA6UmOrqpjV7bhqroMePJyll9Dcz1KkqRJdRks9tgk+wN/1C5aUFVf77csSdJs12Ww2OOB1wM/aV+vT/KevguTJM1uXT6oux+wW1XdD5DkFJprR2/tszBJ0uzW9XNQm41Mb9pHIZIkjepyBHU88IMk5wGhuRZ1VK9VSZJmvS43SZyaZAHw1HbRm6vq5l6rkiTNel0Hi70JOKvnWiRJekCvY/FJkrSqDChJ0iBNGVDt4zJ+Oq5iJEmaMGVAVdV9wM+S7DCmeiRJArrdJLE5cHmSi4BfTSysqv17q0qSNOt1Caije69CkqRldPkc1PlJdgR2qap/T/JQYJ3+S5MkzWZdBos9HDgd+FS7aDvgzD6LkiSpy23mRwJ7AEsAqupK4OF9FiVJUpeAuruqfjsxk2QOzRN1JUnqTZeAOj/JW4ENk+wNnAZ8rd+yJEmzXZe7+I4CDgV+BLwW+AZwUp9FSfqdeUedPdb9LTphv7HuT5pMl7v47m8fUvg9mlN7P6sqT/FJknq1woBKsh/wSeBqmudBPSrJa6vqX/suTpI0e3U5xfcB4LlVdRVAkkcDZwMGlCSpN11uklg6EU6ta4ClPdUjSRIwxRFUkhe1kwuTfAP4Ms01qJcAF4+hNknSLDbVKb7/NTJ9C/DsdnoxsGFvFUmSxBQBVVWvHmchkiSN6nIX36OAvwTmjW7v4zak2cfPZGmcutzFdyZwMs3oEff3W44kSY0uAfWbqvr73iuRJGlEl4D6SJJ3At8C7p5YWFXf760qSdKs1yWgngAcDDyP353iq3ZekqRedAmolwA7jT5yo4skjwT+GdiaJtBOrKqPJNkC+BLNTReLgJdW1R0r07Ykae3XZSSJHwObrULb9wJvrKpdgacDRybZlWZ09HOrahfg3HZekqQH6XIEtRnw0yQX8+BrUFPeZl5VNwE3tdNLk1xB87j4A4DntJudAiwA3ryyhUuS1m5dAuqdq7uTJPOAJ9M8smPrNrwAbqY5Bbi89xwBHAGwww47rG4JkqQZpsvzoM5fnR0k2Qj4CvDXVbUkyWjblWS5z5aqqhOBEwHmz5/v86ckaZZZ4TWoJEuTLGlfv0lyX5IlXRpPsi5NOH2+qv6lXXxLkm3a9dsAt65q8ZKktdcKA6qqNq6qTapqE5pBYl8MfHxF70tzqHQycEVVfXBk1VnAIe30IcBXV7pqSdJar8tdfA+oxpnAn3TYfA/az08lubR97QucAOyd5Epgr3ZekqQH6TJY7ItGZh8CzAd+s6L3VdV3aB4Rvzx7dqpOkjRrdbmLb/S5UPfSfLj2gF6qkSSp1eUuPp8LJUkau6ke+f6OKd5XVXVsD/VIkgRMfQT1q+UsexhwKLAlYEBJknoz1SPfPzAxnWRj4PXAq4EvAh+Y7H2SJK0JU16DakcefwNwEM24ebs78rgkaRymugb1fuBFNMMNPaGqfjm2qiRJs95UH9R9I7At8HbgxpHhjpZ2HepIkqRVNdU1qJUaZUKSpDXJEJIkDZIBJUkaJANKkjRIBpQkaZAMKEnSIBlQkqRBMqAkSYNkQEmSBsmAkiQNkgElSRqkLo98l6TBmXfU2WPd36IT9hvr/uQRlCRpoAwoSdIgGVCSpEEyoCRJg2RASZIGyYCSJA2SASVJGiQDSpI0SAaUJGmQDChJ0iAZUJKkQeotoJL8Y5Jbk/x4ZNkWSc5JcmX7dfO+9i9Jmtn6PIL6DLDPMsuOAs6tql2Ac9t5SZL+h94CqqouAH6xzOIDgFPa6VOAF/a1f0nSzDbua1BbV9VN7fTNwNZj3r8kaYaYtpskqqqAmmx9kiOSLEyycPHixWOsTJI0BOMOqFuSbAPQfr11sg2r6sSqml9V8+fOnTu2AiVJwzDugDoLOKSdPgT46pj3L0maIfq8zfxU4ELg95Ncn+RQ4ARg7yRXAnu185Ik/Q9z+mq4ql4xyao9+9qnJGnt4UgSkqRBMqAkSYNkQEmSBsmAkiQNkgElSRokA0qSNEgGlCRpkAwoSdIgGVCSpEEyoCRJg2RASZIGyYCSJA2SASVJGiQDSpI0SAaUJGmQenselCTNFvOOOnus+1t0wn5j3d908QhKkjRIBpQkaZAMKEnSIBlQkqRBMqAkSYNkQEmSBsmAkiQNkp+DkqS1yDg/k9X357E8gpIkDZIBJUkaJANKkjRIBpQkaZAMKEnSIBlQkqRBMqAkSYNkQEmSBmlaAirJPkl+luSqJEdNRw2SpGEbe0AlWQf4GPB8YFfgFUl2HXcdkqRhm44jqKcBV1XVNVX1W+CLwAHTUIckacBSVePdYXIgsE9VHdbOHwz8QVX9xTLbHQEc0c7+PvCzldzVVsBtq1nubGS/rRr7beXZZ6tmbey3Hatq7rILBztYbFWdCJy4qu9PsrCq5q/BkmYF+23V2G8rzz5bNbOp36bjFN8NwCNH5rdvl0mS9IDpCKiLgV2SPCrJesDLgbOmoQ5J0oCN/RRfVd2b5C+AbwLrAP9YVZf3sKtVPj04y9lvq8Z+W3n22aqZNf029pskJEnqwpEkJEmDZEBJkgZpxgfUioZNSrJ+ki+167+XZN74qxyeDv32hiQ/SXJZknOT7DgddQ5N12G6krw4SSWZFbcDT6VLnyV5afv7dnmSL4y7xiHq8G90hyTnJflB++903+mos1dVNWNfNDdZXA3sBKwH/BDYdZlt/g/wyXb65cCXprvu6X517LfnAg9tp//cfuvWb+12GwMXAN8F5k933UPvM2AX4AfA5u38w6e77ul+dey3E4E/b6d3BRZNd91r+jXTj6C6DJt0AHBKO306sGeSjLHGIVphv1XVeVX163b2uzSfV5vtug7TdSzwXuA34yxuoLr02eHAx6rqDoCqunXMNQ5Rl34rYJN2elPgxjHWNxYzPaC2A64bmb++XbbcbarqXuAuYMuxVDdcXfpt1KHAv/Za0cywwn5LsjvwyKo6e5yFDViX37XfA34vyX8k+W6SfcZW3XB16bd3Aa9Mcj3wDeAvx1Pa+Ax2qCMNQ5JXAvOBZ093LUOX5CHAB4FXTXMpM80cmtN8z6E5Ur8gyROq6s5prWr4XgF8pqo+kOQZwGeTPL6q7p/uwtaUmX4E1WXYpAe2STKH5lD49rFUN1ydhptKshfwNmD/qrp7TLUN2Yr6bWPg8cCCJIuApwNnzfIbJbr8rl0PnFVV91TVz4H/ogms2axLvx0KfBmgqi4ENqAZSHatMdMDqsuwSWcBh7TTBwLfrvaq4iy2wn5L8mTgUzTh5DWBxpT9VlV3VdVWVTWvqubRXLvbv6oWTk+5g9Dl3+iZNEdPJNmK5pTfNeMscoC69Nt/A3sCJHksTUAtHmuVPZvRAdVeU5oYNukK4MtVdXmSY5Ls3252MrBlkquANwCz/gm+Hfvt/cBGwGlJLk0y68dL7NhvGtGxz74J3J7kJ8B5wJuqalaf5ejYb28EDk/yQ+BU4FVr23++HepIkjRIM/oISpK09jKgJEmDZEBJkgbJgJIkDZIBJUkaJANK6ijJI5J8McnVSS5J8o0kv7cK7TyrHbX70iTbJTl9ku0WzPIP+WqWM6CkDtoBhs8AFlTVo6vqKcBbgK1XobmDgOOrarequqGqDlyTtUprCwNK6ua5wD1V9cmJBVX1Q+A7Sd6f5MdJfpTkZQBJntMeAZ2e5KdJPp/GYcBLgWPbZfOS/Lh9z4btEdoVSc4ANpzYV5I/TnJhku8nOS3JRu3yRUne3S7/UZLHtMs3SvJP7bLLkrx4qnakITKgpG4eD1yynOUvAnYDngTsBbw/yTbtuicDf03zrJ6dgD2q6iSaIWveVFUHLdPWnwO/rqrHAu8EngIPDP/zdmCvqtodWEgzKsqE29rlnwD+pl12NHBXVT2hqp4IfLtDO9KgOJq5tHqeCZxaVfcBtyQ5H3gqsAS4qKquB0hyKTAP+M4Ubf0R8PcAVXVZksva5U+nCbn/aB9lth5w4cj7/qX9eglNYEITli+f2KCq7kjyghW0Iw2KASV1cznNYMMrY3QE+PtY9X9vAc6pqlesYD8r2seK2pEGxVN8UjffBtZPcsTEgiRPBO4EXpZknSRzaY6CLlrFfVwA/Gnb9uOBJ7bLvwvskWTndt3DOtw9eA5w5Eitm69iO9K0MaCkDtpRov83sFd7m/nlwPHAF4DLgB/ShNjfVtXNq7ibTwAbJbkCOIb2mldVLaZ5COKp7Wm/C4HHrKCt44DN25s3fgg8dxXbkaaNo5lLkgbJIyhJ0iAZUJKkQTKgJEmDZEBJkgbJgJIkDZIBJUkaJANKkjRI/x98bQmbnh+/ggAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkU4PBV38eZl"
      },
      "source": [
        "####German Credit BBGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjdAh9jH8hII",
        "outputId": "9ecb1948-3a5f-4c43-9048-effcde5e2cb3"
      },
      "source": [
        "german_credit_bbgan = IMPLEMENTED_BBGAN(german_credit_rf_clf, german_data_scaled.shape[1], noise_dim=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator summary\n",
            "Model: \"model_15\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_44 (InputLayer)           [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_45 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_46 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 22)           0           input_44[0][0]                   \n",
            "                                                                 input_45[0][0]                   \n",
            "                                                                 input_46[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_44 (Dense)                (None, 256)          5888        concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 256)          0           dense_44[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_45 (Dense)                (None, 256)          65792       dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 256)          0           dense_45[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_46 (Dense)                (None, 128)          32896       dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 128)          0           dense_46[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_47 (Dense)                (None, 1)            129         dropout_39[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 104,705\n",
            "Trainable params: 104,705\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Generator summary\n",
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_50 (InputLayer)           [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_51 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 4)            0           input_50[0][0]                   \n",
            "                                                                 input_51[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_48 (Dense)                (None, 128)          640         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 128)          0           dense_48[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_49 (Dense)                (None, 256)          33024       dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 256)          0           dense_49[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_50 (Dense)                (None, 256)          65792       dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 256)          0           dense_50[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "samples_generator_layer (Dense) (None, 20)           5140        dropout_42[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 104,596\n",
            "Trainable params: 104,596\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Adversarial model summary:\n",
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_47 (InputLayer)           [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_48 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_16 (Functional)           (None, 20)           104596      input_47[0][0]                   \n",
            "                                                                 input_48[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_49 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_15 (Functional)           (None, 1)            104705      model_16[0][0]                   \n",
            "                                                                 input_48[0][0]                   \n",
            "                                                                 input_49[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 209,301\n",
            "Trainable params: 209,301\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqJbXx819HMg",
        "outputId": "3140b902-2f43-493a-b8c1-6458eecf3807"
      },
      "source": [
        "discriminator_losses, discriminator_accuracies, adversarial_losses, adversarial_accuracies = german_credit_bbgan.train(train_steps=2500, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: [Discriminator loss: 0.709661, acc: 0.492188]  [Adversarial loss: 0.724673, acc: 0.457031]\n",
            "1: [Discriminator loss: 0.715547, acc: 0.437500]  [Adversarial loss: 0.701411, acc: 0.492188]\n",
            "2: [Discriminator loss: 0.681834, acc: 0.554688]  [Adversarial loss: 0.698035, acc: 0.507812]\n",
            "3: [Discriminator loss: 0.696622, acc: 0.527344]  [Adversarial loss: 0.683681, acc: 0.570312]\n",
            "4: [Discriminator loss: 0.704944, acc: 0.460938]  [Adversarial loss: 0.699448, acc: 0.480469]\n",
            "5: [Discriminator loss: 0.710770, acc: 0.429688]  [Adversarial loss: 0.698236, acc: 0.500000]\n",
            "6: [Discriminator loss: 0.693365, acc: 0.542969]  [Adversarial loss: 0.697997, acc: 0.503906]\n",
            "7: [Discriminator loss: 0.699878, acc: 0.507812]  [Adversarial loss: 0.693904, acc: 0.535156]\n",
            "8: [Discriminator loss: 0.690954, acc: 0.527344]  [Adversarial loss: 0.695261, acc: 0.515625]\n",
            "9: [Discriminator loss: 0.693766, acc: 0.511719]  [Adversarial loss: 0.684980, acc: 0.566406]\n",
            "10: [Discriminator loss: 0.686379, acc: 0.523438]  [Adversarial loss: 0.696091, acc: 0.511719]\n",
            "11: [Discriminator loss: 0.693514, acc: 0.539062]  [Adversarial loss: 0.691666, acc: 0.542969]\n",
            "12: [Discriminator loss: 0.694751, acc: 0.515625]  [Adversarial loss: 0.690320, acc: 0.542969]\n",
            "13: [Discriminator loss: 0.699908, acc: 0.464844]  [Adversarial loss: 0.704132, acc: 0.437500]\n",
            "14: [Discriminator loss: 0.695195, acc: 0.476562]  [Adversarial loss: 0.704819, acc: 0.417969]\n",
            "15: [Discriminator loss: 0.699425, acc: 0.488281]  [Adversarial loss: 0.708396, acc: 0.445312]\n",
            "16: [Discriminator loss: 0.703827, acc: 0.445312]  [Adversarial loss: 0.699027, acc: 0.511719]\n",
            "17: [Discriminator loss: 0.700707, acc: 0.457031]  [Adversarial loss: 0.691647, acc: 0.503906]\n",
            "18: [Discriminator loss: 0.695534, acc: 0.500000]  [Adversarial loss: 0.696656, acc: 0.484375]\n",
            "19: [Discriminator loss: 0.694239, acc: 0.503906]  [Adversarial loss: 0.694387, acc: 0.515625]\n",
            "20: [Discriminator loss: 0.694127, acc: 0.511719]  [Adversarial loss: 0.696948, acc: 0.488281]\n",
            "21: [Discriminator loss: 0.699685, acc: 0.437500]  [Adversarial loss: 0.702954, acc: 0.449219]\n",
            "22: [Discriminator loss: 0.698817, acc: 0.496094]  [Adversarial loss: 0.690780, acc: 0.511719]\n",
            "23: [Discriminator loss: 0.694202, acc: 0.492188]  [Adversarial loss: 0.694785, acc: 0.507812]\n",
            "24: [Discriminator loss: 0.690347, acc: 0.601562]  [Adversarial loss: 0.698783, acc: 0.488281]\n",
            "25: [Discriminator loss: 0.695023, acc: 0.511719]  [Adversarial loss: 0.696610, acc: 0.500000]\n",
            "26: [Discriminator loss: 0.691077, acc: 0.519531]  [Adversarial loss: 0.693085, acc: 0.531250]\n",
            "27: [Discriminator loss: 0.696509, acc: 0.507812]  [Adversarial loss: 0.693986, acc: 0.496094]\n",
            "28: [Discriminator loss: 0.694573, acc: 0.507812]  [Adversarial loss: 0.692312, acc: 0.554688]\n",
            "29: [Discriminator loss: 0.685401, acc: 0.578125]  [Adversarial loss: 0.700691, acc: 0.457031]\n",
            "30: [Discriminator loss: 0.695959, acc: 0.515625]  [Adversarial loss: 0.696193, acc: 0.476562]\n",
            "31: [Discriminator loss: 0.696822, acc: 0.472656]  [Adversarial loss: 0.700549, acc: 0.453125]\n",
            "32: [Discriminator loss: 0.696207, acc: 0.484375]  [Adversarial loss: 0.696295, acc: 0.507812]\n",
            "33: [Discriminator loss: 0.691578, acc: 0.550781]  [Adversarial loss: 0.694991, acc: 0.515625]\n",
            "34: [Discriminator loss: 0.693049, acc: 0.496094]  [Adversarial loss: 0.692485, acc: 0.531250]\n",
            "35: [Discriminator loss: 0.695708, acc: 0.511719]  [Adversarial loss: 0.696004, acc: 0.496094]\n",
            "36: [Discriminator loss: 0.689162, acc: 0.539062]  [Adversarial loss: 0.696910, acc: 0.472656]\n",
            "37: [Discriminator loss: 0.696589, acc: 0.492188]  [Adversarial loss: 0.699676, acc: 0.492188]\n",
            "38: [Discriminator loss: 0.697707, acc: 0.511719]  [Adversarial loss: 0.691225, acc: 0.492188]\n",
            "39: [Discriminator loss: 0.689885, acc: 0.554688]  [Adversarial loss: 0.695788, acc: 0.488281]\n",
            "40: [Discriminator loss: 0.690486, acc: 0.519531]  [Adversarial loss: 0.694142, acc: 0.492188]\n",
            "41: [Discriminator loss: 0.694596, acc: 0.500000]  [Adversarial loss: 0.697474, acc: 0.449219]\n",
            "42: [Discriminator loss: 0.689047, acc: 0.550781]  [Adversarial loss: 0.692168, acc: 0.527344]\n",
            "43: [Discriminator loss: 0.688661, acc: 0.539062]  [Adversarial loss: 0.695418, acc: 0.468750]\n",
            "44: [Discriminator loss: 0.688961, acc: 0.523438]  [Adversarial loss: 0.696838, acc: 0.457031]\n",
            "45: [Discriminator loss: 0.689178, acc: 0.539062]  [Adversarial loss: 0.695294, acc: 0.496094]\n",
            "46: [Discriminator loss: 0.700396, acc: 0.414062]  [Adversarial loss: 0.700336, acc: 0.472656]\n",
            "47: [Discriminator loss: 0.695709, acc: 0.480469]  [Adversarial loss: 0.697124, acc: 0.437500]\n",
            "48: [Discriminator loss: 0.692853, acc: 0.531250]  [Adversarial loss: 0.693286, acc: 0.496094]\n",
            "49: [Discriminator loss: 0.696027, acc: 0.519531]  [Adversarial loss: 0.693197, acc: 0.503906]\n",
            "50: [Discriminator loss: 0.693686, acc: 0.519531]  [Adversarial loss: 0.695257, acc: 0.484375]\n",
            "51: [Discriminator loss: 0.691558, acc: 0.515625]  [Adversarial loss: 0.693020, acc: 0.480469]\n",
            "52: [Discriminator loss: 0.693517, acc: 0.511719]  [Adversarial loss: 0.692573, acc: 0.492188]\n",
            "53: [Discriminator loss: 0.696177, acc: 0.492188]  [Adversarial loss: 0.696856, acc: 0.472656]\n",
            "54: [Discriminator loss: 0.686397, acc: 0.550781]  [Adversarial loss: 0.692341, acc: 0.507812]\n",
            "55: [Discriminator loss: 0.697184, acc: 0.476562]  [Adversarial loss: 0.697466, acc: 0.460938]\n",
            "56: [Discriminator loss: 0.691811, acc: 0.523438]  [Adversarial loss: 0.690431, acc: 0.566406]\n",
            "57: [Discriminator loss: 0.688071, acc: 0.554688]  [Adversarial loss: 0.694354, acc: 0.503906]\n",
            "58: [Discriminator loss: 0.694223, acc: 0.496094]  [Adversarial loss: 0.693451, acc: 0.488281]\n",
            "59: [Discriminator loss: 0.696518, acc: 0.472656]  [Adversarial loss: 0.695725, acc: 0.511719]\n",
            "60: [Discriminator loss: 0.693451, acc: 0.511719]  [Adversarial loss: 0.693530, acc: 0.492188]\n",
            "61: [Discriminator loss: 0.700221, acc: 0.445312]  [Adversarial loss: 0.697044, acc: 0.476562]\n",
            "62: [Discriminator loss: 0.692119, acc: 0.503906]  [Adversarial loss: 0.692079, acc: 0.453125]\n",
            "63: [Discriminator loss: 0.693685, acc: 0.500000]  [Adversarial loss: 0.692445, acc: 0.542969]\n",
            "64: [Discriminator loss: 0.694700, acc: 0.507812]  [Adversarial loss: 0.693581, acc: 0.503906]\n",
            "65: [Discriminator loss: 0.695884, acc: 0.464844]  [Adversarial loss: 0.694655, acc: 0.492188]\n",
            "66: [Discriminator loss: 0.697038, acc: 0.468750]  [Adversarial loss: 0.694552, acc: 0.480469]\n",
            "67: [Discriminator loss: 0.695343, acc: 0.460938]  [Adversarial loss: 0.696207, acc: 0.453125]\n",
            "68: [Discriminator loss: 0.693267, acc: 0.496094]  [Adversarial loss: 0.693947, acc: 0.484375]\n",
            "69: [Discriminator loss: 0.692407, acc: 0.511719]  [Adversarial loss: 0.694203, acc: 0.496094]\n",
            "70: [Discriminator loss: 0.696120, acc: 0.468750]  [Adversarial loss: 0.691811, acc: 0.523438]\n",
            "71: [Discriminator loss: 0.695044, acc: 0.480469]  [Adversarial loss: 0.694631, acc: 0.496094]\n",
            "72: [Discriminator loss: 0.691085, acc: 0.542969]  [Adversarial loss: 0.694918, acc: 0.527344]\n",
            "73: [Discriminator loss: 0.693675, acc: 0.511719]  [Adversarial loss: 0.694167, acc: 0.492188]\n",
            "74: [Discriminator loss: 0.695060, acc: 0.468750]  [Adversarial loss: 0.690880, acc: 0.515625]\n",
            "75: [Discriminator loss: 0.693510, acc: 0.511719]  [Adversarial loss: 0.693718, acc: 0.511719]\n",
            "76: [Discriminator loss: 0.692772, acc: 0.535156]  [Adversarial loss: 0.693212, acc: 0.507812]\n",
            "77: [Discriminator loss: 0.695135, acc: 0.472656]  [Adversarial loss: 0.693371, acc: 0.531250]\n",
            "78: [Discriminator loss: 0.694608, acc: 0.503906]  [Adversarial loss: 0.693224, acc: 0.496094]\n",
            "79: [Discriminator loss: 0.694292, acc: 0.457031]  [Adversarial loss: 0.695998, acc: 0.464844]\n",
            "80: [Discriminator loss: 0.699917, acc: 0.390625]  [Adversarial loss: 0.689728, acc: 0.558594]\n",
            "81: [Discriminator loss: 0.692027, acc: 0.535156]  [Adversarial loss: 0.690857, acc: 0.554688]\n",
            "82: [Discriminator loss: 0.690998, acc: 0.523438]  [Adversarial loss: 0.692461, acc: 0.484375]\n",
            "83: [Discriminator loss: 0.693796, acc: 0.500000]  [Adversarial loss: 0.694886, acc: 0.468750]\n",
            "84: [Discriminator loss: 0.691363, acc: 0.546875]  [Adversarial loss: 0.691885, acc: 0.531250]\n",
            "85: [Discriminator loss: 0.694390, acc: 0.484375]  [Adversarial loss: 0.694365, acc: 0.480469]\n",
            "86: [Discriminator loss: 0.694194, acc: 0.484375]  [Adversarial loss: 0.695025, acc: 0.472656]\n",
            "87: [Discriminator loss: 0.694591, acc: 0.507812]  [Adversarial loss: 0.692376, acc: 0.515625]\n",
            "88: [Discriminator loss: 0.693678, acc: 0.503906]  [Adversarial loss: 0.693794, acc: 0.500000]\n",
            "89: [Discriminator loss: 0.692963, acc: 0.531250]  [Adversarial loss: 0.695908, acc: 0.433594]\n",
            "90: [Discriminator loss: 0.696479, acc: 0.480469]  [Adversarial loss: 0.695926, acc: 0.445312]\n",
            "91: [Discriminator loss: 0.695052, acc: 0.464844]  [Adversarial loss: 0.691255, acc: 0.535156]\n",
            "92: [Discriminator loss: 0.691311, acc: 0.511719]  [Adversarial loss: 0.694105, acc: 0.515625]\n",
            "93: [Discriminator loss: 0.697163, acc: 0.476562]  [Adversarial loss: 0.693896, acc: 0.484375]\n",
            "94: [Discriminator loss: 0.692276, acc: 0.519531]  [Adversarial loss: 0.693625, acc: 0.511719]\n",
            "95: [Discriminator loss: 0.689755, acc: 0.558594]  [Adversarial loss: 0.692174, acc: 0.523438]\n",
            "96: [Discriminator loss: 0.696393, acc: 0.457031]  [Adversarial loss: 0.691982, acc: 0.519531]\n",
            "97: [Discriminator loss: 0.697838, acc: 0.429688]  [Adversarial loss: 0.694106, acc: 0.468750]\n",
            "98: [Discriminator loss: 0.693196, acc: 0.480469]  [Adversarial loss: 0.698678, acc: 0.433594]\n",
            "99: [Discriminator loss: 0.694776, acc: 0.468750]  [Adversarial loss: 0.691991, acc: 0.531250]\n",
            "100: [Discriminator loss: 0.695855, acc: 0.464844]  [Adversarial loss: 0.694818, acc: 0.464844]\n",
            "101: [Discriminator loss: 0.695520, acc: 0.476562]  [Adversarial loss: 0.693423, acc: 0.507812]\n",
            "102: [Discriminator loss: 0.696148, acc: 0.441406]  [Adversarial loss: 0.693474, acc: 0.503906]\n",
            "103: [Discriminator loss: 0.694065, acc: 0.476562]  [Adversarial loss: 0.693851, acc: 0.484375]\n",
            "104: [Discriminator loss: 0.695182, acc: 0.449219]  [Adversarial loss: 0.690922, acc: 0.539062]\n",
            "105: [Discriminator loss: 0.693465, acc: 0.542969]  [Adversarial loss: 0.692237, acc: 0.531250]\n",
            "106: [Discriminator loss: 0.692295, acc: 0.511719]  [Adversarial loss: 0.693251, acc: 0.468750]\n",
            "107: [Discriminator loss: 0.694917, acc: 0.496094]  [Adversarial loss: 0.690968, acc: 0.558594]\n",
            "108: [Discriminator loss: 0.692905, acc: 0.523438]  [Adversarial loss: 0.692781, acc: 0.484375]\n",
            "109: [Discriminator loss: 0.692762, acc: 0.515625]  [Adversarial loss: 0.691440, acc: 0.539062]\n",
            "110: [Discriminator loss: 0.692237, acc: 0.527344]  [Adversarial loss: 0.692507, acc: 0.519531]\n",
            "111: [Discriminator loss: 0.691084, acc: 0.511719]  [Adversarial loss: 0.694336, acc: 0.503906]\n",
            "112: [Discriminator loss: 0.691897, acc: 0.523438]  [Adversarial loss: 0.695340, acc: 0.453125]\n",
            "113: [Discriminator loss: 0.694576, acc: 0.460938]  [Adversarial loss: 0.698215, acc: 0.453125]\n",
            "114: [Discriminator loss: 0.695855, acc: 0.472656]  [Adversarial loss: 0.692233, acc: 0.503906]\n",
            "115: [Discriminator loss: 0.695733, acc: 0.449219]  [Adversarial loss: 0.694928, acc: 0.472656]\n",
            "116: [Discriminator loss: 0.695618, acc: 0.460938]  [Adversarial loss: 0.694089, acc: 0.472656]\n",
            "117: [Discriminator loss: 0.691294, acc: 0.535156]  [Adversarial loss: 0.693304, acc: 0.476562]\n",
            "118: [Discriminator loss: 0.692286, acc: 0.488281]  [Adversarial loss: 0.691660, acc: 0.535156]\n",
            "119: [Discriminator loss: 0.691044, acc: 0.527344]  [Adversarial loss: 0.693635, acc: 0.488281]\n",
            "120: [Discriminator loss: 0.696788, acc: 0.433594]  [Adversarial loss: 0.692780, acc: 0.527344]\n",
            "121: [Discriminator loss: 0.691168, acc: 0.531250]  [Adversarial loss: 0.698291, acc: 0.394531]\n",
            "122: [Discriminator loss: 0.693479, acc: 0.476562]  [Adversarial loss: 0.695702, acc: 0.480469]\n",
            "123: [Discriminator loss: 0.694013, acc: 0.503906]  [Adversarial loss: 0.691564, acc: 0.519531]\n",
            "124: [Discriminator loss: 0.692331, acc: 0.535156]  [Adversarial loss: 0.692006, acc: 0.531250]\n",
            "125: [Discriminator loss: 0.692636, acc: 0.535156]  [Adversarial loss: 0.695429, acc: 0.464844]\n",
            "126: [Discriminator loss: 0.691155, acc: 0.539062]  [Adversarial loss: 0.695129, acc: 0.468750]\n",
            "127: [Discriminator loss: 0.692880, acc: 0.511719]  [Adversarial loss: 0.694105, acc: 0.472656]\n",
            "128: [Discriminator loss: 0.693954, acc: 0.484375]  [Adversarial loss: 0.694153, acc: 0.492188]\n",
            "129: [Discriminator loss: 0.692829, acc: 0.507812]  [Adversarial loss: 0.691584, acc: 0.519531]\n",
            "130: [Discriminator loss: 0.693487, acc: 0.488281]  [Adversarial loss: 0.692253, acc: 0.527344]\n",
            "131: [Discriminator loss: 0.692532, acc: 0.492188]  [Adversarial loss: 0.688832, acc: 0.578125]\n",
            "132: [Discriminator loss: 0.694912, acc: 0.515625]  [Adversarial loss: 0.691777, acc: 0.480469]\n",
            "133: [Discriminator loss: 0.695131, acc: 0.476562]  [Adversarial loss: 0.693301, acc: 0.480469]\n",
            "134: [Discriminator loss: 0.695078, acc: 0.488281]  [Adversarial loss: 0.693508, acc: 0.496094]\n",
            "135: [Discriminator loss: 0.694276, acc: 0.496094]  [Adversarial loss: 0.692984, acc: 0.488281]\n",
            "136: [Discriminator loss: 0.696175, acc: 0.421875]  [Adversarial loss: 0.693474, acc: 0.519531]\n",
            "137: [Discriminator loss: 0.694439, acc: 0.484375]  [Adversarial loss: 0.696382, acc: 0.468750]\n",
            "138: [Discriminator loss: 0.693723, acc: 0.464844]  [Adversarial loss: 0.692561, acc: 0.500000]\n",
            "139: [Discriminator loss: 0.694593, acc: 0.476562]  [Adversarial loss: 0.693679, acc: 0.492188]\n",
            "140: [Discriminator loss: 0.692704, acc: 0.507812]  [Adversarial loss: 0.692402, acc: 0.515625]\n",
            "141: [Discriminator loss: 0.697409, acc: 0.425781]  [Adversarial loss: 0.693443, acc: 0.496094]\n",
            "142: [Discriminator loss: 0.693778, acc: 0.503906]  [Adversarial loss: 0.691018, acc: 0.546875]\n",
            "143: [Discriminator loss: 0.693449, acc: 0.476562]  [Adversarial loss: 0.693166, acc: 0.492188]\n",
            "144: [Discriminator loss: 0.695574, acc: 0.535156]  [Adversarial loss: 0.692947, acc: 0.535156]\n",
            "145: [Discriminator loss: 0.698353, acc: 0.429688]  [Adversarial loss: 0.693836, acc: 0.500000]\n",
            "146: [Discriminator loss: 0.692822, acc: 0.492188]  [Adversarial loss: 0.697473, acc: 0.468750]\n",
            "147: [Discriminator loss: 0.694979, acc: 0.492188]  [Adversarial loss: 0.692139, acc: 0.507812]\n",
            "148: [Discriminator loss: 0.696746, acc: 0.480469]  [Adversarial loss: 0.691872, acc: 0.511719]\n",
            "149: [Discriminator loss: 0.690994, acc: 0.519531]  [Adversarial loss: 0.692784, acc: 0.500000]\n",
            "150: [Discriminator loss: 0.695154, acc: 0.476562]  [Adversarial loss: 0.693537, acc: 0.492188]\n",
            "151: [Discriminator loss: 0.692798, acc: 0.550781]  [Adversarial loss: 0.695948, acc: 0.464844]\n",
            "152: [Discriminator loss: 0.695753, acc: 0.488281]  [Adversarial loss: 0.691625, acc: 0.511719]\n",
            "153: [Discriminator loss: 0.694246, acc: 0.484375]  [Adversarial loss: 0.688866, acc: 0.597656]\n",
            "154: [Discriminator loss: 0.693985, acc: 0.503906]  [Adversarial loss: 0.691592, acc: 0.542969]\n",
            "155: [Discriminator loss: 0.695526, acc: 0.417969]  [Adversarial loss: 0.691698, acc: 0.488281]\n",
            "156: [Discriminator loss: 0.694884, acc: 0.441406]  [Adversarial loss: 0.692685, acc: 0.496094]\n",
            "157: [Discriminator loss: 0.694485, acc: 0.488281]  [Adversarial loss: 0.693790, acc: 0.464844]\n",
            "158: [Discriminator loss: 0.693767, acc: 0.488281]  [Adversarial loss: 0.693452, acc: 0.460938]\n",
            "159: [Discriminator loss: 0.695834, acc: 0.429688]  [Adversarial loss: 0.692255, acc: 0.515625]\n",
            "160: [Discriminator loss: 0.695058, acc: 0.457031]  [Adversarial loss: 0.692393, acc: 0.507812]\n",
            "161: [Discriminator loss: 0.693962, acc: 0.484375]  [Adversarial loss: 0.693192, acc: 0.453125]\n",
            "162: [Discriminator loss: 0.692066, acc: 0.496094]  [Adversarial loss: 0.690254, acc: 0.542969]\n",
            "163: [Discriminator loss: 0.692500, acc: 0.523438]  [Adversarial loss: 0.692657, acc: 0.503906]\n",
            "164: [Discriminator loss: 0.692780, acc: 0.519531]  [Adversarial loss: 0.690171, acc: 0.542969]\n",
            "165: [Discriminator loss: 0.694810, acc: 0.468750]  [Adversarial loss: 0.691959, acc: 0.546875]\n",
            "166: [Discriminator loss: 0.691869, acc: 0.539062]  [Adversarial loss: 0.693710, acc: 0.500000]\n",
            "167: [Discriminator loss: 0.693711, acc: 0.531250]  [Adversarial loss: 0.692762, acc: 0.523438]\n",
            "168: [Discriminator loss: 0.693820, acc: 0.542969]  [Adversarial loss: 0.689999, acc: 0.550781]\n",
            "169: [Discriminator loss: 0.692524, acc: 0.503906]  [Adversarial loss: 0.688297, acc: 0.546875]\n",
            "170: [Discriminator loss: 0.691284, acc: 0.503906]  [Adversarial loss: 0.688903, acc: 0.535156]\n",
            "171: [Discriminator loss: 0.692717, acc: 0.515625]  [Adversarial loss: 0.688555, acc: 0.585938]\n",
            "172: [Discriminator loss: 0.691958, acc: 0.500000]  [Adversarial loss: 0.689508, acc: 0.496094]\n",
            "173: [Discriminator loss: 0.699333, acc: 0.488281]  [Adversarial loss: 0.683496, acc: 0.589844]\n",
            "174: [Discriminator loss: 0.691143, acc: 0.519531]  [Adversarial loss: 0.689683, acc: 0.519531]\n",
            "175: [Discriminator loss: 0.692952, acc: 0.457031]  [Adversarial loss: 0.682666, acc: 0.621094]\n",
            "176: [Discriminator loss: 0.692862, acc: 0.550781]  [Adversarial loss: 0.688384, acc: 0.500000]\n",
            "177: [Discriminator loss: 0.695936, acc: 0.464844]  [Adversarial loss: 0.680836, acc: 0.605469]\n",
            "178: [Discriminator loss: 0.689772, acc: 0.511719]  [Adversarial loss: 0.681287, acc: 0.609375]\n",
            "179: [Discriminator loss: 0.695390, acc: 0.500000]  [Adversarial loss: 0.685553, acc: 0.609375]\n",
            "180: [Discriminator loss: 0.703490, acc: 0.468750]  [Adversarial loss: 0.674329, acc: 0.613281]\n",
            "181: [Discriminator loss: 0.682554, acc: 0.570312]  [Adversarial loss: 0.684322, acc: 0.562500]\n",
            "182: [Discriminator loss: 0.685873, acc: 0.539062]  [Adversarial loss: 0.684693, acc: 0.535156]\n",
            "183: [Discriminator loss: 0.687303, acc: 0.496094]  [Adversarial loss: 0.676404, acc: 0.609375]\n",
            "184: [Discriminator loss: 0.694717, acc: 0.515625]  [Adversarial loss: 0.678024, acc: 0.617188]\n",
            "185: [Discriminator loss: 0.700499, acc: 0.484375]  [Adversarial loss: 0.673896, acc: 0.664062]\n",
            "186: [Discriminator loss: 0.691455, acc: 0.523438]  [Adversarial loss: 0.667943, acc: 0.648438]\n",
            "187: [Discriminator loss: 0.707691, acc: 0.496094]  [Adversarial loss: 0.656910, acc: 0.667969]\n",
            "188: [Discriminator loss: 0.677533, acc: 0.531250]  [Adversarial loss: 0.672297, acc: 0.617188]\n",
            "189: [Discriminator loss: 0.722592, acc: 0.445312]  [Adversarial loss: 0.651551, acc: 0.652344]\n",
            "190: [Discriminator loss: 0.702874, acc: 0.503906]  [Adversarial loss: 0.654485, acc: 0.632812]\n",
            "191: [Discriminator loss: 0.707464, acc: 0.523438]  [Adversarial loss: 0.680061, acc: 0.582031]\n",
            "192: [Discriminator loss: 0.697197, acc: 0.554688]  [Adversarial loss: 0.661946, acc: 0.636719]\n",
            "193: [Discriminator loss: 0.723744, acc: 0.480469]  [Adversarial loss: 0.658457, acc: 0.648438]\n",
            "194: [Discriminator loss: 0.746478, acc: 0.441406]  [Adversarial loss: 0.667741, acc: 0.609375]\n",
            "195: [Discriminator loss: 0.707995, acc: 0.511719]  [Adversarial loss: 0.671021, acc: 0.621094]\n",
            "196: [Discriminator loss: 0.688349, acc: 0.550781]  [Adversarial loss: 0.676001, acc: 0.617188]\n",
            "197: [Discriminator loss: 0.702275, acc: 0.515625]  [Adversarial loss: 0.664467, acc: 0.667969]\n",
            "198: [Discriminator loss: 0.700044, acc: 0.527344]  [Adversarial loss: 0.674207, acc: 0.625000]\n",
            "199: [Discriminator loss: 0.722606, acc: 0.503906]  [Adversarial loss: 0.659388, acc: 0.664062]\n",
            "200: [Discriminator loss: 0.704536, acc: 0.523438]  [Adversarial loss: 0.689565, acc: 0.582031]\n",
            "201: [Discriminator loss: 0.694126, acc: 0.531250]  [Adversarial loss: 0.672258, acc: 0.644531]\n",
            "202: [Discriminator loss: 0.702896, acc: 0.507812]  [Adversarial loss: 0.667434, acc: 0.632812]\n",
            "203: [Discriminator loss: 0.704644, acc: 0.515625]  [Adversarial loss: 0.670015, acc: 0.640625]\n",
            "204: [Discriminator loss: 0.724004, acc: 0.492188]  [Adversarial loss: 0.663649, acc: 0.664062]\n",
            "205: [Discriminator loss: 0.703985, acc: 0.484375]  [Adversarial loss: 0.658364, acc: 0.671875]\n",
            "206: [Discriminator loss: 0.713821, acc: 0.468750]  [Adversarial loss: 0.650867, acc: 0.722656]\n",
            "207: [Discriminator loss: 0.727180, acc: 0.476562]  [Adversarial loss: 0.661057, acc: 0.660156]\n",
            "208: [Discriminator loss: 0.711879, acc: 0.511719]  [Adversarial loss: 0.653889, acc: 0.691406]\n",
            "209: [Discriminator loss: 0.696503, acc: 0.515625]  [Adversarial loss: 0.657501, acc: 0.671875]\n",
            "210: [Discriminator loss: 0.697177, acc: 0.496094]  [Adversarial loss: 0.664093, acc: 0.625000]\n",
            "211: [Discriminator loss: 0.712908, acc: 0.480469]  [Adversarial loss: 0.660564, acc: 0.695312]\n",
            "212: [Discriminator loss: 0.665202, acc: 0.585938]  [Adversarial loss: 0.663185, acc: 0.675781]\n",
            "213: [Discriminator loss: 0.698921, acc: 0.546875]  [Adversarial loss: 0.666350, acc: 0.632812]\n",
            "214: [Discriminator loss: 0.712764, acc: 0.492188]  [Adversarial loss: 0.667088, acc: 0.621094]\n",
            "215: [Discriminator loss: 0.713179, acc: 0.480469]  [Adversarial loss: 0.645825, acc: 0.734375]\n",
            "216: [Discriminator loss: 0.702910, acc: 0.496094]  [Adversarial loss: 0.660187, acc: 0.679688]\n",
            "217: [Discriminator loss: 0.733804, acc: 0.464844]  [Adversarial loss: 0.643115, acc: 0.726562]\n",
            "218: [Discriminator loss: 0.716763, acc: 0.496094]  [Adversarial loss: 0.666023, acc: 0.636719]\n",
            "219: [Discriminator loss: 0.710039, acc: 0.523438]  [Adversarial loss: 0.656066, acc: 0.644531]\n",
            "220: [Discriminator loss: 0.722355, acc: 0.484375]  [Adversarial loss: 0.656340, acc: 0.671875]\n",
            "221: [Discriminator loss: 0.685749, acc: 0.496094]  [Adversarial loss: 0.652926, acc: 0.718750]\n",
            "222: [Discriminator loss: 0.736234, acc: 0.472656]  [Adversarial loss: 0.646222, acc: 0.699219]\n",
            "223: [Discriminator loss: 0.698498, acc: 0.523438]  [Adversarial loss: 0.649979, acc: 0.699219]\n",
            "224: [Discriminator loss: 0.742083, acc: 0.468750]  [Adversarial loss: 0.646096, acc: 0.726562]\n",
            "225: [Discriminator loss: 0.700432, acc: 0.492188]  [Adversarial loss: 0.649889, acc: 0.679688]\n",
            "226: [Discriminator loss: 0.700188, acc: 0.480469]  [Adversarial loss: 0.662270, acc: 0.664062]\n",
            "227: [Discriminator loss: 0.698429, acc: 0.519531]  [Adversarial loss: 0.640158, acc: 0.726562]\n",
            "228: [Discriminator loss: 0.662174, acc: 0.566406]  [Adversarial loss: 0.667192, acc: 0.613281]\n",
            "229: [Discriminator loss: 0.693097, acc: 0.507812]  [Adversarial loss: 0.642522, acc: 0.707031]\n",
            "230: [Discriminator loss: 0.725492, acc: 0.496094]  [Adversarial loss: 0.642534, acc: 0.644531]\n",
            "231: [Discriminator loss: 0.717454, acc: 0.484375]  [Adversarial loss: 0.623921, acc: 0.730469]\n",
            "232: [Discriminator loss: 0.737017, acc: 0.449219]  [Adversarial loss: 0.639507, acc: 0.679688]\n",
            "233: [Discriminator loss: 0.686918, acc: 0.539062]  [Adversarial loss: 0.654638, acc: 0.656250]\n",
            "234: [Discriminator loss: 0.690477, acc: 0.531250]  [Adversarial loss: 0.646389, acc: 0.660156]\n",
            "235: [Discriminator loss: 0.711471, acc: 0.511719]  [Adversarial loss: 0.633110, acc: 0.671875]\n",
            "236: [Discriminator loss: 0.707116, acc: 0.515625]  [Adversarial loss: 0.647761, acc: 0.648438]\n",
            "237: [Discriminator loss: 0.713282, acc: 0.500000]  [Adversarial loss: 0.624821, acc: 0.695312]\n",
            "238: [Discriminator loss: 0.676394, acc: 0.542969]  [Adversarial loss: 0.637034, acc: 0.679688]\n",
            "239: [Discriminator loss: 0.694500, acc: 0.496094]  [Adversarial loss: 0.627715, acc: 0.703125]\n",
            "240: [Discriminator loss: 0.702132, acc: 0.515625]  [Adversarial loss: 0.605946, acc: 0.718750]\n",
            "241: [Discriminator loss: 0.689611, acc: 0.531250]  [Adversarial loss: 0.659192, acc: 0.632812]\n",
            "242: [Discriminator loss: 0.710665, acc: 0.515625]  [Adversarial loss: 0.635777, acc: 0.667969]\n",
            "243: [Discriminator loss: 0.713135, acc: 0.496094]  [Adversarial loss: 0.639582, acc: 0.679688]\n",
            "244: [Discriminator loss: 0.728148, acc: 0.468750]  [Adversarial loss: 0.634394, acc: 0.652344]\n",
            "245: [Discriminator loss: 0.731638, acc: 0.464844]  [Adversarial loss: 0.624551, acc: 0.687500]\n",
            "246: [Discriminator loss: 0.669469, acc: 0.582031]  [Adversarial loss: 0.648727, acc: 0.636719]\n",
            "247: [Discriminator loss: 0.712310, acc: 0.535156]  [Adversarial loss: 0.610346, acc: 0.718750]\n",
            "248: [Discriminator loss: 0.680298, acc: 0.585938]  [Adversarial loss: 0.627297, acc: 0.675781]\n",
            "249: [Discriminator loss: 0.702435, acc: 0.527344]  [Adversarial loss: 0.630641, acc: 0.644531]\n",
            "250: [Discriminator loss: 0.708201, acc: 0.519531]  [Adversarial loss: 0.625951, acc: 0.675781]\n",
            "251: [Discriminator loss: 0.709252, acc: 0.515625]  [Adversarial loss: 0.607444, acc: 0.722656]\n",
            "252: [Discriminator loss: 0.704296, acc: 0.523438]  [Adversarial loss: 0.649315, acc: 0.675781]\n",
            "253: [Discriminator loss: 0.717924, acc: 0.531250]  [Adversarial loss: 0.605601, acc: 0.726562]\n",
            "254: [Discriminator loss: 0.656799, acc: 0.585938]  [Adversarial loss: 0.606988, acc: 0.738281]\n",
            "255: [Discriminator loss: 0.665161, acc: 0.546875]  [Adversarial loss: 0.635070, acc: 0.664062]\n",
            "256: [Discriminator loss: 0.764660, acc: 0.468750]  [Adversarial loss: 0.633070, acc: 0.710938]\n",
            "257: [Discriminator loss: 0.700774, acc: 0.531250]  [Adversarial loss: 0.653339, acc: 0.656250]\n",
            "258: [Discriminator loss: 0.738895, acc: 0.496094]  [Adversarial loss: 0.627862, acc: 0.703125]\n",
            "259: [Discriminator loss: 0.713122, acc: 0.527344]  [Adversarial loss: 0.635703, acc: 0.671875]\n",
            "260: [Discriminator loss: 0.697891, acc: 0.542969]  [Adversarial loss: 0.631573, acc: 0.699219]\n",
            "261: [Discriminator loss: 0.734448, acc: 0.464844]  [Adversarial loss: 0.617961, acc: 0.679688]\n",
            "262: [Discriminator loss: 0.676801, acc: 0.542969]  [Adversarial loss: 0.634903, acc: 0.683594]\n",
            "263: [Discriminator loss: 0.696265, acc: 0.511719]  [Adversarial loss: 0.628123, acc: 0.718750]\n",
            "264: [Discriminator loss: 0.723163, acc: 0.496094]  [Adversarial loss: 0.625941, acc: 0.679688]\n",
            "265: [Discriminator loss: 0.706480, acc: 0.542969]  [Adversarial loss: 0.624831, acc: 0.687500]\n",
            "266: [Discriminator loss: 0.706082, acc: 0.535156]  [Adversarial loss: 0.629310, acc: 0.683594]\n",
            "267: [Discriminator loss: 0.663034, acc: 0.566406]  [Adversarial loss: 0.638429, acc: 0.648438]\n",
            "268: [Discriminator loss: 0.692142, acc: 0.542969]  [Adversarial loss: 0.629579, acc: 0.648438]\n",
            "269: [Discriminator loss: 0.693789, acc: 0.566406]  [Adversarial loss: 0.630646, acc: 0.679688]\n",
            "270: [Discriminator loss: 0.675279, acc: 0.574219]  [Adversarial loss: 0.631372, acc: 0.664062]\n",
            "271: [Discriminator loss: 0.704794, acc: 0.519531]  [Adversarial loss: 0.620538, acc: 0.687500]\n",
            "272: [Discriminator loss: 0.701940, acc: 0.527344]  [Adversarial loss: 0.638727, acc: 0.656250]\n",
            "273: [Discriminator loss: 0.712676, acc: 0.503906]  [Adversarial loss: 0.623140, acc: 0.683594]\n",
            "274: [Discriminator loss: 0.683369, acc: 0.546875]  [Adversarial loss: 0.617296, acc: 0.699219]\n",
            "275: [Discriminator loss: 0.659964, acc: 0.562500]  [Adversarial loss: 0.632520, acc: 0.675781]\n",
            "276: [Discriminator loss: 0.658346, acc: 0.593750]  [Adversarial loss: 0.613821, acc: 0.714844]\n",
            "277: [Discriminator loss: 0.650952, acc: 0.605469]  [Adversarial loss: 0.618080, acc: 0.699219]\n",
            "278: [Discriminator loss: 0.685780, acc: 0.527344]  [Adversarial loss: 0.617274, acc: 0.691406]\n",
            "279: [Discriminator loss: 0.661826, acc: 0.570312]  [Adversarial loss: 0.632970, acc: 0.691406]\n",
            "280: [Discriminator loss: 0.689299, acc: 0.562500]  [Adversarial loss: 0.616377, acc: 0.648438]\n",
            "281: [Discriminator loss: 0.683472, acc: 0.582031]  [Adversarial loss: 0.629222, acc: 0.667969]\n",
            "282: [Discriminator loss: 0.693253, acc: 0.578125]  [Adversarial loss: 0.616764, acc: 0.648438]\n",
            "283: [Discriminator loss: 0.655699, acc: 0.605469]  [Adversarial loss: 0.616134, acc: 0.679688]\n",
            "284: [Discriminator loss: 0.642714, acc: 0.574219]  [Adversarial loss: 0.610147, acc: 0.679688]\n",
            "285: [Discriminator loss: 0.655717, acc: 0.562500]  [Adversarial loss: 0.628767, acc: 0.667969]\n",
            "286: [Discriminator loss: 0.672689, acc: 0.593750]  [Adversarial loss: 0.638740, acc: 0.628906]\n",
            "287: [Discriminator loss: 0.595808, acc: 0.667969]  [Adversarial loss: 0.650793, acc: 0.636719]\n",
            "288: [Discriminator loss: 0.656284, acc: 0.609375]  [Adversarial loss: 0.599619, acc: 0.699219]\n",
            "289: [Discriminator loss: 0.627564, acc: 0.667969]  [Adversarial loss: 0.632386, acc: 0.640625]\n",
            "290: [Discriminator loss: 0.630242, acc: 0.636719]  [Adversarial loss: 0.641430, acc: 0.605469]\n",
            "291: [Discriminator loss: 0.664250, acc: 0.621094]  [Adversarial loss: 0.646743, acc: 0.609375]\n",
            "292: [Discriminator loss: 0.640216, acc: 0.597656]  [Adversarial loss: 0.646357, acc: 0.644531]\n",
            "293: [Discriminator loss: 0.615922, acc: 0.640625]  [Adversarial loss: 0.624116, acc: 0.652344]\n",
            "294: [Discriminator loss: 0.630990, acc: 0.593750]  [Adversarial loss: 0.650351, acc: 0.593750]\n",
            "295: [Discriminator loss: 0.598278, acc: 0.632812]  [Adversarial loss: 0.599812, acc: 0.695312]\n",
            "296: [Discriminator loss: 0.639007, acc: 0.605469]  [Adversarial loss: 0.640432, acc: 0.621094]\n",
            "297: [Discriminator loss: 0.622529, acc: 0.621094]  [Adversarial loss: 0.631311, acc: 0.621094]\n",
            "298: [Discriminator loss: 0.598063, acc: 0.675781]  [Adversarial loss: 0.607052, acc: 0.667969]\n",
            "299: [Discriminator loss: 0.656966, acc: 0.605469]  [Adversarial loss: 0.632691, acc: 0.613281]\n",
            "300: [Discriminator loss: 0.639748, acc: 0.664062]  [Adversarial loss: 0.600518, acc: 0.675781]\n",
            "301: [Discriminator loss: 0.619449, acc: 0.628906]  [Adversarial loss: 0.617022, acc: 0.632812]\n",
            "302: [Discriminator loss: 0.637673, acc: 0.597656]  [Adversarial loss: 0.570057, acc: 0.707031]\n",
            "303: [Discriminator loss: 0.699346, acc: 0.566406]  [Adversarial loss: 0.595691, acc: 0.656250]\n",
            "304: [Discriminator loss: 0.677966, acc: 0.558594]  [Adversarial loss: 0.611581, acc: 0.667969]\n",
            "305: [Discriminator loss: 0.701253, acc: 0.527344]  [Adversarial loss: 0.577758, acc: 0.679688]\n",
            "306: [Discriminator loss: 0.694952, acc: 0.562500]  [Adversarial loss: 0.590323, acc: 0.699219]\n",
            "307: [Discriminator loss: 0.637685, acc: 0.585938]  [Adversarial loss: 0.595078, acc: 0.660156]\n",
            "308: [Discriminator loss: 0.703058, acc: 0.523438]  [Adversarial loss: 0.587164, acc: 0.652344]\n",
            "309: [Discriminator loss: 0.646578, acc: 0.605469]  [Adversarial loss: 0.618200, acc: 0.625000]\n",
            "310: [Discriminator loss: 0.637913, acc: 0.613281]  [Adversarial loss: 0.593718, acc: 0.664062]\n",
            "311: [Discriminator loss: 0.665367, acc: 0.625000]  [Adversarial loss: 0.592726, acc: 0.640625]\n",
            "312: [Discriminator loss: 0.645585, acc: 0.652344]  [Adversarial loss: 0.613886, acc: 0.625000]\n",
            "313: [Discriminator loss: 0.628272, acc: 0.652344]  [Adversarial loss: 0.621860, acc: 0.593750]\n",
            "314: [Discriminator loss: 0.647996, acc: 0.625000]  [Adversarial loss: 0.585867, acc: 0.664062]\n",
            "315: [Discriminator loss: 0.625110, acc: 0.628906]  [Adversarial loss: 0.590683, acc: 0.691406]\n",
            "316: [Discriminator loss: 0.636776, acc: 0.578125]  [Adversarial loss: 0.592493, acc: 0.667969]\n",
            "317: [Discriminator loss: 0.602050, acc: 0.640625]  [Adversarial loss: 0.573046, acc: 0.675781]\n",
            "318: [Discriminator loss: 0.653382, acc: 0.585938]  [Adversarial loss: 0.577693, acc: 0.667969]\n",
            "319: [Discriminator loss: 0.640383, acc: 0.601562]  [Adversarial loss: 0.573017, acc: 0.679688]\n",
            "320: [Discriminator loss: 0.622283, acc: 0.609375]  [Adversarial loss: 0.590096, acc: 0.644531]\n",
            "321: [Discriminator loss: 0.614083, acc: 0.625000]  [Adversarial loss: 0.563701, acc: 0.679688]\n",
            "322: [Discriminator loss: 0.578838, acc: 0.679688]  [Adversarial loss: 0.567179, acc: 0.710938]\n",
            "323: [Discriminator loss: 0.636634, acc: 0.632812]  [Adversarial loss: 0.571513, acc: 0.687500]\n",
            "324: [Discriminator loss: 0.581277, acc: 0.675781]  [Adversarial loss: 0.567812, acc: 0.640625]\n",
            "325: [Discriminator loss: 0.604180, acc: 0.621094]  [Adversarial loss: 0.557328, acc: 0.699219]\n",
            "326: [Discriminator loss: 0.656325, acc: 0.554688]  [Adversarial loss: 0.561462, acc: 0.683594]\n",
            "327: [Discriminator loss: 0.602604, acc: 0.617188]  [Adversarial loss: 0.578208, acc: 0.707031]\n",
            "328: [Discriminator loss: 0.606881, acc: 0.570312]  [Adversarial loss: 0.563388, acc: 0.695312]\n",
            "329: [Discriminator loss: 0.625989, acc: 0.585938]  [Adversarial loss: 0.535885, acc: 0.714844]\n",
            "330: [Discriminator loss: 0.588396, acc: 0.625000]  [Adversarial loss: 0.550218, acc: 0.710938]\n",
            "331: [Discriminator loss: 0.621419, acc: 0.632812]  [Adversarial loss: 0.552217, acc: 0.714844]\n",
            "332: [Discriminator loss: 0.552764, acc: 0.656250]  [Adversarial loss: 0.557778, acc: 0.679688]\n",
            "333: [Discriminator loss: 0.626409, acc: 0.625000]  [Adversarial loss: 0.588451, acc: 0.667969]\n",
            "334: [Discriminator loss: 0.620272, acc: 0.632812]  [Adversarial loss: 0.598742, acc: 0.625000]\n",
            "335: [Discriminator loss: 0.628221, acc: 0.617188]  [Adversarial loss: 0.558303, acc: 0.675781]\n",
            "336: [Discriminator loss: 0.626684, acc: 0.617188]  [Adversarial loss: 0.531116, acc: 0.742188]\n",
            "337: [Discriminator loss: 0.596674, acc: 0.605469]  [Adversarial loss: 0.570164, acc: 0.707031]\n",
            "338: [Discriminator loss: 0.614162, acc: 0.554688]  [Adversarial loss: 0.568035, acc: 0.687500]\n",
            "339: [Discriminator loss: 0.578217, acc: 0.605469]  [Adversarial loss: 0.556363, acc: 0.687500]\n",
            "340: [Discriminator loss: 0.578520, acc: 0.613281]  [Adversarial loss: 0.556489, acc: 0.691406]\n",
            "341: [Discriminator loss: 0.579559, acc: 0.609375]  [Adversarial loss: 0.548704, acc: 0.722656]\n",
            "342: [Discriminator loss: 0.589972, acc: 0.593750]  [Adversarial loss: 0.563083, acc: 0.718750]\n",
            "343: [Discriminator loss: 0.580373, acc: 0.597656]  [Adversarial loss: 0.543972, acc: 0.742188]\n",
            "344: [Discriminator loss: 0.577377, acc: 0.621094]  [Adversarial loss: 0.557538, acc: 0.769531]\n",
            "345: [Discriminator loss: 0.551994, acc: 0.621094]  [Adversarial loss: 0.522041, acc: 0.777344]\n",
            "346: [Discriminator loss: 0.644597, acc: 0.542969]  [Adversarial loss: 0.555793, acc: 0.753906]\n",
            "347: [Discriminator loss: 0.659704, acc: 0.550781]  [Adversarial loss: 0.550824, acc: 0.730469]\n",
            "348: [Discriminator loss: 0.635176, acc: 0.562500]  [Adversarial loss: 0.517280, acc: 0.777344]\n",
            "349: [Discriminator loss: 0.649165, acc: 0.546875]  [Adversarial loss: 0.530532, acc: 0.761719]\n",
            "350: [Discriminator loss: 0.703879, acc: 0.511719]  [Adversarial loss: 0.559268, acc: 0.699219]\n",
            "351: [Discriminator loss: 0.631810, acc: 0.582031]  [Adversarial loss: 0.524984, acc: 0.777344]\n",
            "352: [Discriminator loss: 0.581642, acc: 0.585938]  [Adversarial loss: 0.543881, acc: 0.746094]\n",
            "353: [Discriminator loss: 0.645618, acc: 0.566406]  [Adversarial loss: 0.560119, acc: 0.718750]\n",
            "354: [Discriminator loss: 0.569253, acc: 0.644531]  [Adversarial loss: 0.534251, acc: 0.730469]\n",
            "355: [Discriminator loss: 0.599751, acc: 0.617188]  [Adversarial loss: 0.515866, acc: 0.765625]\n",
            "356: [Discriminator loss: 0.580164, acc: 0.636719]  [Adversarial loss: 0.503933, acc: 0.808594]\n",
            "357: [Discriminator loss: 0.608754, acc: 0.554688]  [Adversarial loss: 0.541275, acc: 0.757812]\n",
            "358: [Discriminator loss: 0.653904, acc: 0.550781]  [Adversarial loss: 0.542043, acc: 0.714844]\n",
            "359: [Discriminator loss: 0.635248, acc: 0.550781]  [Adversarial loss: 0.508313, acc: 0.785156]\n",
            "360: [Discriminator loss: 0.636448, acc: 0.570312]  [Adversarial loss: 0.505074, acc: 0.777344]\n",
            "361: [Discriminator loss: 0.605633, acc: 0.550781]  [Adversarial loss: 0.527360, acc: 0.773438]\n",
            "362: [Discriminator loss: 0.597561, acc: 0.558594]  [Adversarial loss: 0.535732, acc: 0.742188]\n",
            "363: [Discriminator loss: 0.630962, acc: 0.570312]  [Adversarial loss: 0.546992, acc: 0.683594]\n",
            "364: [Discriminator loss: 0.625629, acc: 0.585938]  [Adversarial loss: 0.537354, acc: 0.746094]\n",
            "365: [Discriminator loss: 0.624575, acc: 0.550781]  [Adversarial loss: 0.528161, acc: 0.734375]\n",
            "366: [Discriminator loss: 0.543839, acc: 0.625000]  [Adversarial loss: 0.552319, acc: 0.726562]\n",
            "367: [Discriminator loss: 0.599378, acc: 0.597656]  [Adversarial loss: 0.520551, acc: 0.742188]\n",
            "368: [Discriminator loss: 0.577131, acc: 0.601562]  [Adversarial loss: 0.513612, acc: 0.777344]\n",
            "369: [Discriminator loss: 0.613819, acc: 0.550781]  [Adversarial loss: 0.577269, acc: 0.722656]\n",
            "370: [Discriminator loss: 0.586606, acc: 0.605469]  [Adversarial loss: 0.510294, acc: 0.781250]\n",
            "371: [Discriminator loss: 0.577433, acc: 0.589844]  [Adversarial loss: 0.499092, acc: 0.792969]\n",
            "372: [Discriminator loss: 0.597744, acc: 0.558594]  [Adversarial loss: 0.498322, acc: 0.789062]\n",
            "373: [Discriminator loss: 0.629999, acc: 0.589844]  [Adversarial loss: 0.502812, acc: 0.792969]\n",
            "374: [Discriminator loss: 0.663903, acc: 0.535156]  [Adversarial loss: 0.551866, acc: 0.746094]\n",
            "375: [Discriminator loss: 0.643367, acc: 0.593750]  [Adversarial loss: 0.509829, acc: 0.781250]\n",
            "376: [Discriminator loss: 0.620424, acc: 0.589844]  [Adversarial loss: 0.504759, acc: 0.757812]\n",
            "377: [Discriminator loss: 0.561953, acc: 0.652344]  [Adversarial loss: 0.553977, acc: 0.734375]\n",
            "378: [Discriminator loss: 0.569011, acc: 0.621094]  [Adversarial loss: 0.515374, acc: 0.734375]\n",
            "379: [Discriminator loss: 0.552943, acc: 0.648438]  [Adversarial loss: 0.520877, acc: 0.738281]\n",
            "380: [Discriminator loss: 0.513633, acc: 0.683594]  [Adversarial loss: 0.504473, acc: 0.753906]\n",
            "381: [Discriminator loss: 0.610474, acc: 0.546875]  [Adversarial loss: 0.499940, acc: 0.765625]\n",
            "382: [Discriminator loss: 0.612558, acc: 0.558594]  [Adversarial loss: 0.531436, acc: 0.765625]\n",
            "383: [Discriminator loss: 0.642724, acc: 0.542969]  [Adversarial loss: 0.532995, acc: 0.773438]\n",
            "384: [Discriminator loss: 0.595470, acc: 0.562500]  [Adversarial loss: 0.528980, acc: 0.730469]\n",
            "385: [Discriminator loss: 0.596601, acc: 0.585938]  [Adversarial loss: 0.518857, acc: 0.757812]\n",
            "386: [Discriminator loss: 0.554874, acc: 0.585938]  [Adversarial loss: 0.534017, acc: 0.789062]\n",
            "387: [Discriminator loss: 0.659419, acc: 0.519531]  [Adversarial loss: 0.542350, acc: 0.730469]\n",
            "388: [Discriminator loss: 0.649471, acc: 0.589844]  [Adversarial loss: 0.493008, acc: 0.789062]\n",
            "389: [Discriminator loss: 0.610373, acc: 0.519531]  [Adversarial loss: 0.527921, acc: 0.761719]\n",
            "390: [Discriminator loss: 0.594967, acc: 0.597656]  [Adversarial loss: 0.518545, acc: 0.781250]\n",
            "391: [Discriminator loss: 0.558445, acc: 0.609375]  [Adversarial loss: 0.477428, acc: 0.796875]\n",
            "392: [Discriminator loss: 0.583607, acc: 0.535156]  [Adversarial loss: 0.489835, acc: 0.796875]\n",
            "393: [Discriminator loss: 0.690510, acc: 0.507812]  [Adversarial loss: 0.480313, acc: 0.812500]\n",
            "394: [Discriminator loss: 0.627947, acc: 0.550781]  [Adversarial loss: 0.509899, acc: 0.800781]\n",
            "395: [Discriminator loss: 0.566870, acc: 0.589844]  [Adversarial loss: 0.468173, acc: 0.753906]\n",
            "396: [Discriminator loss: 0.598394, acc: 0.558594]  [Adversarial loss: 0.495968, acc: 0.792969]\n",
            "397: [Discriminator loss: 0.583361, acc: 0.558594]  [Adversarial loss: 0.518287, acc: 0.777344]\n",
            "398: [Discriminator loss: 0.541515, acc: 0.593750]  [Adversarial loss: 0.488359, acc: 0.816406]\n",
            "399: [Discriminator loss: 0.600836, acc: 0.558594]  [Adversarial loss: 0.491972, acc: 0.757812]\n",
            "400: [Discriminator loss: 0.605363, acc: 0.527344]  [Adversarial loss: 0.522247, acc: 0.753906]\n",
            "401: [Discriminator loss: 0.543412, acc: 0.593750]  [Adversarial loss: 0.468349, acc: 0.843750]\n",
            "402: [Discriminator loss: 0.625375, acc: 0.539062]  [Adversarial loss: 0.420415, acc: 0.839844]\n",
            "403: [Discriminator loss: 0.598321, acc: 0.578125]  [Adversarial loss: 0.446821, acc: 0.800781]\n",
            "404: [Discriminator loss: 0.625124, acc: 0.585938]  [Adversarial loss: 0.462386, acc: 0.808594]\n",
            "405: [Discriminator loss: 0.570737, acc: 0.597656]  [Adversarial loss: 0.487218, acc: 0.816406]\n",
            "406: [Discriminator loss: 0.602640, acc: 0.535156]  [Adversarial loss: 0.477727, acc: 0.808594]\n",
            "407: [Discriminator loss: 0.582479, acc: 0.531250]  [Adversarial loss: 0.506047, acc: 0.769531]\n",
            "408: [Discriminator loss: 0.574591, acc: 0.582031]  [Adversarial loss: 0.418907, acc: 0.820312]\n",
            "409: [Discriminator loss: 0.556576, acc: 0.601562]  [Adversarial loss: 0.500367, acc: 0.792969]\n",
            "410: [Discriminator loss: 0.594714, acc: 0.539062]  [Adversarial loss: 0.490862, acc: 0.792969]\n",
            "411: [Discriminator loss: 0.557707, acc: 0.597656]  [Adversarial loss: 0.466180, acc: 0.792969]\n",
            "412: [Discriminator loss: 0.562860, acc: 0.566406]  [Adversarial loss: 0.478795, acc: 0.804688]\n",
            "413: [Discriminator loss: 0.568195, acc: 0.574219]  [Adversarial loss: 0.468756, acc: 0.812500]\n",
            "414: [Discriminator loss: 0.669842, acc: 0.464844]  [Adversarial loss: 0.496067, acc: 0.796875]\n",
            "415: [Discriminator loss: 0.586114, acc: 0.539062]  [Adversarial loss: 0.447494, acc: 0.828125]\n",
            "416: [Discriminator loss: 0.564517, acc: 0.585938]  [Adversarial loss: 0.396219, acc: 0.886719]\n",
            "417: [Discriminator loss: 0.620056, acc: 0.546875]  [Adversarial loss: 0.461142, acc: 0.855469]\n",
            "418: [Discriminator loss: 0.603617, acc: 0.539062]  [Adversarial loss: 0.445391, acc: 0.832031]\n",
            "419: [Discriminator loss: 0.562668, acc: 0.531250]  [Adversarial loss: 0.449833, acc: 0.812500]\n",
            "420: [Discriminator loss: 0.546300, acc: 0.613281]  [Adversarial loss: 0.454925, acc: 0.796875]\n",
            "421: [Discriminator loss: 0.651056, acc: 0.492188]  [Adversarial loss: 0.517363, acc: 0.792969]\n",
            "422: [Discriminator loss: 0.615983, acc: 0.515625]  [Adversarial loss: 0.468018, acc: 0.820312]\n",
            "423: [Discriminator loss: 0.596801, acc: 0.511719]  [Adversarial loss: 0.536050, acc: 0.746094]\n",
            "424: [Discriminator loss: 0.542239, acc: 0.613281]  [Adversarial loss: 0.461478, acc: 0.800781]\n",
            "425: [Discriminator loss: 0.631385, acc: 0.511719]  [Adversarial loss: 0.456438, acc: 0.796875]\n",
            "426: [Discriminator loss: 0.614746, acc: 0.558594]  [Adversarial loss: 0.470113, acc: 0.824219]\n",
            "427: [Discriminator loss: 0.526738, acc: 0.593750]  [Adversarial loss: 0.469566, acc: 0.781250]\n",
            "428: [Discriminator loss: 0.611025, acc: 0.531250]  [Adversarial loss: 0.458905, acc: 0.812500]\n",
            "429: [Discriminator loss: 0.555247, acc: 0.570312]  [Adversarial loss: 0.384612, acc: 0.886719]\n",
            "430: [Discriminator loss: 0.683528, acc: 0.531250]  [Adversarial loss: 0.451875, acc: 0.820312]\n",
            "431: [Discriminator loss: 0.599418, acc: 0.550781]  [Adversarial loss: 0.526326, acc: 0.792969]\n",
            "432: [Discriminator loss: 0.615680, acc: 0.570312]  [Adversarial loss: 0.521059, acc: 0.730469]\n",
            "433: [Discriminator loss: 0.590771, acc: 0.554688]  [Adversarial loss: 0.437207, acc: 0.804688]\n",
            "434: [Discriminator loss: 0.569271, acc: 0.554688]  [Adversarial loss: 0.443209, acc: 0.824219]\n",
            "435: [Discriminator loss: 0.556310, acc: 0.582031]  [Adversarial loss: 0.440862, acc: 0.800781]\n",
            "436: [Discriminator loss: 0.548831, acc: 0.640625]  [Adversarial loss: 0.400433, acc: 0.859375]\n",
            "437: [Discriminator loss: 0.580487, acc: 0.539062]  [Adversarial loss: 0.488900, acc: 0.832031]\n",
            "438: [Discriminator loss: 0.570562, acc: 0.562500]  [Adversarial loss: 0.412243, acc: 0.882812]\n",
            "439: [Discriminator loss: 0.550174, acc: 0.570312]  [Adversarial loss: 0.454172, acc: 0.839844]\n",
            "440: [Discriminator loss: 0.538981, acc: 0.566406]  [Adversarial loss: 0.449546, acc: 0.855469]\n",
            "441: [Discriminator loss: 0.645523, acc: 0.500000]  [Adversarial loss: 0.441374, acc: 0.843750]\n",
            "442: [Discriminator loss: 0.581559, acc: 0.527344]  [Adversarial loss: 0.432443, acc: 0.843750]\n",
            "443: [Discriminator loss: 0.576206, acc: 0.578125]  [Adversarial loss: 0.423566, acc: 0.863281]\n",
            "444: [Discriminator loss: 0.645966, acc: 0.558594]  [Adversarial loss: 0.448798, acc: 0.828125]\n",
            "445: [Discriminator loss: 0.678543, acc: 0.488281]  [Adversarial loss: 0.436187, acc: 0.875000]\n",
            "446: [Discriminator loss: 0.714543, acc: 0.507812]  [Adversarial loss: 0.431945, acc: 0.871094]\n",
            "447: [Discriminator loss: 0.745892, acc: 0.515625]  [Adversarial loss: 0.427664, acc: 0.878906]\n",
            "448: [Discriminator loss: 0.678450, acc: 0.515625]  [Adversarial loss: 0.443032, acc: 0.894531]\n",
            "449: [Discriminator loss: 0.579107, acc: 0.589844]  [Adversarial loss: 0.436361, acc: 0.867188]\n",
            "450: [Discriminator loss: 0.628373, acc: 0.519531]  [Adversarial loss: 0.435875, acc: 0.843750]\n",
            "451: [Discriminator loss: 0.608874, acc: 0.539062]  [Adversarial loss: 0.456609, acc: 0.843750]\n",
            "452: [Discriminator loss: 0.647191, acc: 0.492188]  [Adversarial loss: 0.424226, acc: 0.851562]\n",
            "453: [Discriminator loss: 0.599959, acc: 0.546875]  [Adversarial loss: 0.465783, acc: 0.816406]\n",
            "454: [Discriminator loss: 0.596876, acc: 0.527344]  [Adversarial loss: 0.471729, acc: 0.820312]\n",
            "455: [Discriminator loss: 0.577893, acc: 0.550781]  [Adversarial loss: 0.501202, acc: 0.789062]\n",
            "456: [Discriminator loss: 0.630984, acc: 0.507812]  [Adversarial loss: 0.505214, acc: 0.851562]\n",
            "457: [Discriminator loss: 0.596668, acc: 0.558594]  [Adversarial loss: 0.441910, acc: 0.839844]\n",
            "458: [Discriminator loss: 0.594038, acc: 0.511719]  [Adversarial loss: 0.496414, acc: 0.828125]\n",
            "459: [Discriminator loss: 0.614197, acc: 0.531250]  [Adversarial loss: 0.448331, acc: 0.847656]\n",
            "460: [Discriminator loss: 0.613235, acc: 0.523438]  [Adversarial loss: 0.458111, acc: 0.863281]\n",
            "461: [Discriminator loss: 0.561530, acc: 0.625000]  [Adversarial loss: 0.454470, acc: 0.785156]\n",
            "462: [Discriminator loss: 0.575930, acc: 0.558594]  [Adversarial loss: 0.535822, acc: 0.765625]\n",
            "463: [Discriminator loss: 0.600960, acc: 0.542969]  [Adversarial loss: 0.532202, acc: 0.746094]\n",
            "464: [Discriminator loss: 0.592645, acc: 0.558594]  [Adversarial loss: 0.512137, acc: 0.750000]\n",
            "465: [Discriminator loss: 0.654956, acc: 0.539062]  [Adversarial loss: 0.505598, acc: 0.777344]\n",
            "466: [Discriminator loss: 0.594290, acc: 0.531250]  [Adversarial loss: 0.535340, acc: 0.761719]\n",
            "467: [Discriminator loss: 0.591146, acc: 0.593750]  [Adversarial loss: 0.550184, acc: 0.738281]\n",
            "468: [Discriminator loss: 0.634410, acc: 0.519531]  [Adversarial loss: 0.489129, acc: 0.816406]\n",
            "469: [Discriminator loss: 0.525673, acc: 0.648438]  [Adversarial loss: 0.486209, acc: 0.808594]\n",
            "470: [Discriminator loss: 0.620828, acc: 0.550781]  [Adversarial loss: 0.534240, acc: 0.746094]\n",
            "471: [Discriminator loss: 0.544093, acc: 0.566406]  [Adversarial loss: 0.475547, acc: 0.839844]\n",
            "472: [Discriminator loss: 0.594094, acc: 0.582031]  [Adversarial loss: 0.480037, acc: 0.792969]\n",
            "473: [Discriminator loss: 0.577145, acc: 0.542969]  [Adversarial loss: 0.482975, acc: 0.800781]\n",
            "474: [Discriminator loss: 0.572497, acc: 0.546875]  [Adversarial loss: 0.527356, acc: 0.792969]\n",
            "475: [Discriminator loss: 0.578005, acc: 0.566406]  [Adversarial loss: 0.472801, acc: 0.792969]\n",
            "476: [Discriminator loss: 0.582819, acc: 0.585938]  [Adversarial loss: 0.471051, acc: 0.800781]\n",
            "477: [Discriminator loss: 0.614487, acc: 0.515625]  [Adversarial loss: 0.527769, acc: 0.792969]\n",
            "478: [Discriminator loss: 0.613189, acc: 0.535156]  [Adversarial loss: 0.553412, acc: 0.820312]\n",
            "479: [Discriminator loss: 0.597526, acc: 0.554688]  [Adversarial loss: 0.482697, acc: 0.824219]\n",
            "480: [Discriminator loss: 0.601953, acc: 0.539062]  [Adversarial loss: 0.472041, acc: 0.812500]\n",
            "481: [Discriminator loss: 0.670195, acc: 0.511719]  [Adversarial loss: 0.485983, acc: 0.871094]\n",
            "482: [Discriminator loss: 0.596095, acc: 0.562500]  [Adversarial loss: 0.455541, acc: 0.851562]\n",
            "483: [Discriminator loss: 0.615445, acc: 0.542969]  [Adversarial loss: 0.473097, acc: 0.835938]\n",
            "484: [Discriminator loss: 0.612958, acc: 0.527344]  [Adversarial loss: 0.497074, acc: 0.828125]\n",
            "485: [Discriminator loss: 0.659743, acc: 0.523438]  [Adversarial loss: 0.464602, acc: 0.863281]\n",
            "486: [Discriminator loss: 0.646911, acc: 0.507812]  [Adversarial loss: 0.465598, acc: 0.847656]\n",
            "487: [Discriminator loss: 0.635641, acc: 0.492188]  [Adversarial loss: 0.494163, acc: 0.824219]\n",
            "488: [Discriminator loss: 0.581152, acc: 0.542969]  [Adversarial loss: 0.452517, acc: 0.859375]\n",
            "489: [Discriminator loss: 0.604731, acc: 0.539062]  [Adversarial loss: 0.479191, acc: 0.855469]\n",
            "490: [Discriminator loss: 0.609798, acc: 0.496094]  [Adversarial loss: 0.469518, acc: 0.843750]\n",
            "491: [Discriminator loss: 0.639028, acc: 0.503906]  [Adversarial loss: 0.471360, acc: 0.867188]\n",
            "492: [Discriminator loss: 0.664296, acc: 0.527344]  [Adversarial loss: 0.513184, acc: 0.808594]\n",
            "493: [Discriminator loss: 0.706518, acc: 0.535156]  [Adversarial loss: 0.540397, acc: 0.785156]\n",
            "494: [Discriminator loss: 0.566727, acc: 0.582031]  [Adversarial loss: 0.475120, acc: 0.855469]\n",
            "495: [Discriminator loss: 0.741960, acc: 0.500000]  [Adversarial loss: 0.499759, acc: 0.863281]\n",
            "496: [Discriminator loss: 0.624975, acc: 0.578125]  [Adversarial loss: 0.470575, acc: 0.828125]\n",
            "497: [Discriminator loss: 0.660671, acc: 0.511719]  [Adversarial loss: 0.496283, acc: 0.855469]\n",
            "498: [Discriminator loss: 0.650819, acc: 0.500000]  [Adversarial loss: 0.458586, acc: 0.875000]\n",
            "499: [Discriminator loss: 0.621391, acc: 0.507812]  [Adversarial loss: 0.513661, acc: 0.824219]\n",
            "500: [Discriminator loss: 0.588227, acc: 0.546875]  [Adversarial loss: 0.487136, acc: 0.800781]\n",
            "501: [Discriminator loss: 0.590015, acc: 0.542969]  [Adversarial loss: 0.483551, acc: 0.847656]\n",
            "502: [Discriminator loss: 0.628304, acc: 0.503906]  [Adversarial loss: 0.466463, acc: 0.855469]\n",
            "503: [Discriminator loss: 0.585602, acc: 0.527344]  [Adversarial loss: 0.456418, acc: 0.847656]\n",
            "504: [Discriminator loss: 0.577256, acc: 0.578125]  [Adversarial loss: 0.459192, acc: 0.843750]\n",
            "505: [Discriminator loss: 0.647230, acc: 0.539062]  [Adversarial loss: 0.437853, acc: 0.867188]\n",
            "506: [Discriminator loss: 0.585574, acc: 0.578125]  [Adversarial loss: 0.434088, acc: 0.832031]\n",
            "507: [Discriminator loss: 0.606230, acc: 0.542969]  [Adversarial loss: 0.467194, acc: 0.835938]\n",
            "508: [Discriminator loss: 0.613439, acc: 0.539062]  [Adversarial loss: 0.486042, acc: 0.824219]\n",
            "509: [Discriminator loss: 0.671760, acc: 0.511719]  [Adversarial loss: 0.451281, acc: 0.851562]\n",
            "510: [Discriminator loss: 0.617002, acc: 0.542969]  [Adversarial loss: 0.472334, acc: 0.832031]\n",
            "511: [Discriminator loss: 0.697478, acc: 0.492188]  [Adversarial loss: 0.453738, acc: 0.847656]\n",
            "512: [Discriminator loss: 0.640235, acc: 0.523438]  [Adversarial loss: 0.453002, acc: 0.851562]\n",
            "513: [Discriminator loss: 0.632811, acc: 0.515625]  [Adversarial loss: 0.445795, acc: 0.882812]\n",
            "514: [Discriminator loss: 0.692547, acc: 0.515625]  [Adversarial loss: 0.440030, acc: 0.863281]\n",
            "515: [Discriminator loss: 0.667211, acc: 0.484375]  [Adversarial loss: 0.477429, acc: 0.847656]\n",
            "516: [Discriminator loss: 0.606682, acc: 0.539062]  [Adversarial loss: 0.463003, acc: 0.824219]\n",
            "517: [Discriminator loss: 0.616809, acc: 0.566406]  [Adversarial loss: 0.461883, acc: 0.835938]\n",
            "518: [Discriminator loss: 0.642713, acc: 0.550781]  [Adversarial loss: 0.441321, acc: 0.824219]\n",
            "519: [Discriminator loss: 0.572952, acc: 0.554688]  [Adversarial loss: 0.433315, acc: 0.859375]\n",
            "520: [Discriminator loss: 0.633157, acc: 0.535156]  [Adversarial loss: 0.472243, acc: 0.839844]\n",
            "521: [Discriminator loss: 0.622966, acc: 0.531250]  [Adversarial loss: 0.467832, acc: 0.804688]\n",
            "522: [Discriminator loss: 0.596734, acc: 0.519531]  [Adversarial loss: 0.441770, acc: 0.832031]\n",
            "523: [Discriminator loss: 0.629067, acc: 0.550781]  [Adversarial loss: 0.475300, acc: 0.808594]\n",
            "524: [Discriminator loss: 0.620611, acc: 0.535156]  [Adversarial loss: 0.444169, acc: 0.855469]\n",
            "525: [Discriminator loss: 0.580476, acc: 0.542969]  [Adversarial loss: 0.460790, acc: 0.847656]\n",
            "526: [Discriminator loss: 0.577544, acc: 0.562500]  [Adversarial loss: 0.433667, acc: 0.832031]\n",
            "527: [Discriminator loss: 0.573681, acc: 0.546875]  [Adversarial loss: 0.450474, acc: 0.835938]\n",
            "528: [Discriminator loss: 0.641921, acc: 0.476562]  [Adversarial loss: 0.425902, acc: 0.878906]\n",
            "529: [Discriminator loss: 0.619005, acc: 0.562500]  [Adversarial loss: 0.406818, acc: 0.867188]\n",
            "530: [Discriminator loss: 0.612465, acc: 0.558594]  [Adversarial loss: 0.443739, acc: 0.835938]\n",
            "531: [Discriminator loss: 0.636689, acc: 0.554688]  [Adversarial loss: 0.440740, acc: 0.859375]\n",
            "532: [Discriminator loss: 0.608415, acc: 0.566406]  [Adversarial loss: 0.440639, acc: 0.851562]\n",
            "533: [Discriminator loss: 0.637346, acc: 0.539062]  [Adversarial loss: 0.414263, acc: 0.906250]\n",
            "534: [Discriminator loss: 0.677290, acc: 0.519531]  [Adversarial loss: 0.416068, acc: 0.875000]\n",
            "535: [Discriminator loss: 0.644686, acc: 0.542969]  [Adversarial loss: 0.411950, acc: 0.894531]\n",
            "536: [Discriminator loss: 0.611126, acc: 0.570312]  [Adversarial loss: 0.443580, acc: 0.835938]\n",
            "537: [Discriminator loss: 0.630919, acc: 0.550781]  [Adversarial loss: 0.384689, acc: 0.867188]\n",
            "538: [Discriminator loss: 0.679854, acc: 0.492188]  [Adversarial loss: 0.481321, acc: 0.863281]\n",
            "539: [Discriminator loss: 0.643509, acc: 0.500000]  [Adversarial loss: 0.479506, acc: 0.851562]\n",
            "540: [Discriminator loss: 0.571812, acc: 0.562500]  [Adversarial loss: 0.441577, acc: 0.832031]\n",
            "541: [Discriminator loss: 0.570732, acc: 0.601562]  [Adversarial loss: 0.434858, acc: 0.871094]\n",
            "542: [Discriminator loss: 0.601363, acc: 0.531250]  [Adversarial loss: 0.456931, acc: 0.863281]\n",
            "543: [Discriminator loss: 0.652843, acc: 0.496094]  [Adversarial loss: 0.499441, acc: 0.847656]\n",
            "544: [Discriminator loss: 0.632813, acc: 0.542969]  [Adversarial loss: 0.475050, acc: 0.859375]\n",
            "545: [Discriminator loss: 0.657462, acc: 0.500000]  [Adversarial loss: 0.439572, acc: 0.902344]\n",
            "546: [Discriminator loss: 0.662600, acc: 0.480469]  [Adversarial loss: 0.458993, acc: 0.832031]\n",
            "547: [Discriminator loss: 0.596100, acc: 0.527344]  [Adversarial loss: 0.438643, acc: 0.878906]\n",
            "548: [Discriminator loss: 0.638368, acc: 0.527344]  [Adversarial loss: 0.467682, acc: 0.863281]\n",
            "549: [Discriminator loss: 0.615055, acc: 0.527344]  [Adversarial loss: 0.423296, acc: 0.878906]\n",
            "550: [Discriminator loss: 0.695358, acc: 0.531250]  [Adversarial loss: 0.443739, acc: 0.886719]\n",
            "551: [Discriminator loss: 0.617588, acc: 0.523438]  [Adversarial loss: 0.443100, acc: 0.875000]\n",
            "552: [Discriminator loss: 0.590087, acc: 0.562500]  [Adversarial loss: 0.391543, acc: 0.863281]\n",
            "553: [Discriminator loss: 0.641437, acc: 0.496094]  [Adversarial loss: 0.437391, acc: 0.878906]\n",
            "554: [Discriminator loss: 0.611497, acc: 0.542969]  [Adversarial loss: 0.412516, acc: 0.871094]\n",
            "555: [Discriminator loss: 0.646059, acc: 0.531250]  [Adversarial loss: 0.397887, acc: 0.886719]\n",
            "556: [Discriminator loss: 0.716330, acc: 0.480469]  [Adversarial loss: 0.475118, acc: 0.839844]\n",
            "557: [Discriminator loss: 0.636930, acc: 0.523438]  [Adversarial loss: 0.499704, acc: 0.816406]\n",
            "558: [Discriminator loss: 0.580569, acc: 0.566406]  [Adversarial loss: 0.444095, acc: 0.878906]\n",
            "559: [Discriminator loss: 0.626045, acc: 0.531250]  [Adversarial loss: 0.466117, acc: 0.816406]\n",
            "560: [Discriminator loss: 0.615151, acc: 0.527344]  [Adversarial loss: 0.462030, acc: 0.875000]\n",
            "561: [Discriminator loss: 0.718235, acc: 0.417969]  [Adversarial loss: 0.460349, acc: 0.882812]\n",
            "562: [Discriminator loss: 0.628139, acc: 0.480469]  [Adversarial loss: 0.485300, acc: 0.839844]\n",
            "563: [Discriminator loss: 0.651688, acc: 0.476562]  [Adversarial loss: 0.447693, acc: 0.882812]\n",
            "564: [Discriminator loss: 0.643899, acc: 0.500000]  [Adversarial loss: 0.434023, acc: 0.886719]\n",
            "565: [Discriminator loss: 0.586233, acc: 0.558594]  [Adversarial loss: 0.433217, acc: 0.890625]\n",
            "566: [Discriminator loss: 0.634483, acc: 0.539062]  [Adversarial loss: 0.463963, acc: 0.832031]\n",
            "567: [Discriminator loss: 0.628311, acc: 0.515625]  [Adversarial loss: 0.461959, acc: 0.843750]\n",
            "568: [Discriminator loss: 0.641913, acc: 0.484375]  [Adversarial loss: 0.440951, acc: 0.878906]\n",
            "569: [Discriminator loss: 0.542736, acc: 0.601562]  [Adversarial loss: 0.434767, acc: 0.878906]\n",
            "570: [Discriminator loss: 0.585711, acc: 0.570312]  [Adversarial loss: 0.489485, acc: 0.863281]\n",
            "571: [Discriminator loss: 0.605516, acc: 0.546875]  [Adversarial loss: 0.508428, acc: 0.773438]\n",
            "572: [Discriminator loss: 0.662003, acc: 0.484375]  [Adversarial loss: 0.491182, acc: 0.789062]\n",
            "573: [Discriminator loss: 0.582864, acc: 0.539062]  [Adversarial loss: 0.501914, acc: 0.824219]\n",
            "574: [Discriminator loss: 0.575496, acc: 0.554688]  [Adversarial loss: 0.443933, acc: 0.863281]\n",
            "575: [Discriminator loss: 0.625439, acc: 0.550781]  [Adversarial loss: 0.451228, acc: 0.855469]\n",
            "576: [Discriminator loss: 0.555800, acc: 0.605469]  [Adversarial loss: 0.430339, acc: 0.843750]\n",
            "577: [Discriminator loss: 0.552164, acc: 0.570312]  [Adversarial loss: 0.437398, acc: 0.851562]\n",
            "578: [Discriminator loss: 0.555484, acc: 0.566406]  [Adversarial loss: 0.451388, acc: 0.847656]\n",
            "579: [Discriminator loss: 0.563723, acc: 0.636719]  [Adversarial loss: 0.396107, acc: 0.824219]\n",
            "580: [Discriminator loss: 0.552824, acc: 0.582031]  [Adversarial loss: 0.444797, acc: 0.824219]\n",
            "581: [Discriminator loss: 0.554618, acc: 0.570312]  [Adversarial loss: 0.495898, acc: 0.812500]\n",
            "582: [Discriminator loss: 0.641605, acc: 0.507812]  [Adversarial loss: 0.470257, acc: 0.812500]\n",
            "583: [Discriminator loss: 0.599339, acc: 0.546875]  [Adversarial loss: 0.457999, acc: 0.847656]\n",
            "584: [Discriminator loss: 0.569606, acc: 0.593750]  [Adversarial loss: 0.398240, acc: 0.867188]\n",
            "585: [Discriminator loss: 0.544683, acc: 0.605469]  [Adversarial loss: 0.425723, acc: 0.851562]\n",
            "586: [Discriminator loss: 0.591359, acc: 0.562500]  [Adversarial loss: 0.509327, acc: 0.800781]\n",
            "587: [Discriminator loss: 0.629479, acc: 0.562500]  [Adversarial loss: 0.435536, acc: 0.843750]\n",
            "588: [Discriminator loss: 0.639862, acc: 0.507812]  [Adversarial loss: 0.422029, acc: 0.863281]\n",
            "589: [Discriminator loss: 0.606700, acc: 0.539062]  [Adversarial loss: 0.462391, acc: 0.851562]\n",
            "590: [Discriminator loss: 0.561577, acc: 0.582031]  [Adversarial loss: 0.382837, acc: 0.863281]\n",
            "591: [Discriminator loss: 0.611523, acc: 0.574219]  [Adversarial loss: 0.449301, acc: 0.859375]\n",
            "592: [Discriminator loss: 0.631271, acc: 0.574219]  [Adversarial loss: 0.452697, acc: 0.816406]\n",
            "593: [Discriminator loss: 0.597064, acc: 0.578125]  [Adversarial loss: 0.426829, acc: 0.839844]\n",
            "594: [Discriminator loss: 0.644169, acc: 0.527344]  [Adversarial loss: 0.480826, acc: 0.808594]\n",
            "595: [Discriminator loss: 0.659654, acc: 0.535156]  [Adversarial loss: 0.466825, acc: 0.812500]\n",
            "596: [Discriminator loss: 0.617221, acc: 0.558594]  [Adversarial loss: 0.436291, acc: 0.843750]\n",
            "597: [Discriminator loss: 0.622391, acc: 0.550781]  [Adversarial loss: 0.428678, acc: 0.855469]\n",
            "598: [Discriminator loss: 0.681521, acc: 0.488281]  [Adversarial loss: 0.438723, acc: 0.871094]\n",
            "599: [Discriminator loss: 0.626343, acc: 0.531250]  [Adversarial loss: 0.440513, acc: 0.835938]\n",
            "600: [Discriminator loss: 0.653562, acc: 0.550781]  [Adversarial loss: 0.470692, acc: 0.855469]\n",
            "601: [Discriminator loss: 0.584020, acc: 0.589844]  [Adversarial loss: 0.411151, acc: 0.839844]\n",
            "602: [Discriminator loss: 0.550543, acc: 0.609375]  [Adversarial loss: 0.402900, acc: 0.847656]\n",
            "603: [Discriminator loss: 0.580573, acc: 0.585938]  [Adversarial loss: 0.453835, acc: 0.816406]\n",
            "604: [Discriminator loss: 0.608102, acc: 0.542969]  [Adversarial loss: 0.449087, acc: 0.832031]\n",
            "605: [Discriminator loss: 0.607950, acc: 0.527344]  [Adversarial loss: 0.440685, acc: 0.882812]\n",
            "606: [Discriminator loss: 0.598228, acc: 0.519531]  [Adversarial loss: 0.479677, acc: 0.859375]\n",
            "607: [Discriminator loss: 0.604237, acc: 0.511719]  [Adversarial loss: 0.467147, acc: 0.839844]\n",
            "608: [Discriminator loss: 0.640167, acc: 0.515625]  [Adversarial loss: 0.470562, acc: 0.832031]\n",
            "609: [Discriminator loss: 0.549468, acc: 0.570312]  [Adversarial loss: 0.432334, acc: 0.847656]\n",
            "610: [Discriminator loss: 0.620637, acc: 0.519531]  [Adversarial loss: 0.437554, acc: 0.839844]\n",
            "611: [Discriminator loss: 0.610679, acc: 0.535156]  [Adversarial loss: 0.433465, acc: 0.835938]\n",
            "612: [Discriminator loss: 0.558309, acc: 0.578125]  [Adversarial loss: 0.423790, acc: 0.832031]\n",
            "613: [Discriminator loss: 0.626793, acc: 0.492188]  [Adversarial loss: 0.419290, acc: 0.839844]\n",
            "614: [Discriminator loss: 0.595392, acc: 0.554688]  [Adversarial loss: 0.400771, acc: 0.859375]\n",
            "615: [Discriminator loss: 0.632898, acc: 0.539062]  [Adversarial loss: 0.427615, acc: 0.859375]\n",
            "616: [Discriminator loss: 0.599923, acc: 0.578125]  [Adversarial loss: 0.465123, acc: 0.824219]\n",
            "617: [Discriminator loss: 0.606567, acc: 0.539062]  [Adversarial loss: 0.396320, acc: 0.855469]\n",
            "618: [Discriminator loss: 0.568374, acc: 0.566406]  [Adversarial loss: 0.445186, acc: 0.835938]\n",
            "619: [Discriminator loss: 0.583848, acc: 0.542969]  [Adversarial loss: 0.439038, acc: 0.839844]\n",
            "620: [Discriminator loss: 0.583937, acc: 0.570312]  [Adversarial loss: 0.396366, acc: 0.851562]\n",
            "621: [Discriminator loss: 0.620083, acc: 0.511719]  [Adversarial loss: 0.431807, acc: 0.824219]\n",
            "622: [Discriminator loss: 0.583796, acc: 0.589844]  [Adversarial loss: 0.416876, acc: 0.839844]\n",
            "623: [Discriminator loss: 0.665663, acc: 0.484375]  [Adversarial loss: 0.430550, acc: 0.859375]\n",
            "624: [Discriminator loss: 0.577389, acc: 0.582031]  [Adversarial loss: 0.429503, acc: 0.824219]\n",
            "625: [Discriminator loss: 0.556161, acc: 0.562500]  [Adversarial loss: 0.413781, acc: 0.863281]\n",
            "626: [Discriminator loss: 0.541208, acc: 0.597656]  [Adversarial loss: 0.403458, acc: 0.851562]\n",
            "627: [Discriminator loss: 0.545749, acc: 0.597656]  [Adversarial loss: 0.353611, acc: 0.898438]\n",
            "628: [Discriminator loss: 0.576995, acc: 0.597656]  [Adversarial loss: 0.371489, acc: 0.886719]\n",
            "629: [Discriminator loss: 0.569063, acc: 0.582031]  [Adversarial loss: 0.397872, acc: 0.867188]\n",
            "630: [Discriminator loss: 0.560264, acc: 0.566406]  [Adversarial loss: 0.382224, acc: 0.843750]\n",
            "631: [Discriminator loss: 0.626553, acc: 0.519531]  [Adversarial loss: 0.411651, acc: 0.859375]\n",
            "632: [Discriminator loss: 0.672855, acc: 0.562500]  [Adversarial loss: 0.433679, acc: 0.847656]\n",
            "633: [Discriminator loss: 0.654141, acc: 0.500000]  [Adversarial loss: 0.474335, acc: 0.855469]\n",
            "634: [Discriminator loss: 0.614967, acc: 0.554688]  [Adversarial loss: 0.413226, acc: 0.871094]\n",
            "635: [Discriminator loss: 0.589712, acc: 0.562500]  [Adversarial loss: 0.434570, acc: 0.855469]\n",
            "636: [Discriminator loss: 0.638279, acc: 0.550781]  [Adversarial loss: 0.436950, acc: 0.859375]\n",
            "637: [Discriminator loss: 0.621170, acc: 0.527344]  [Adversarial loss: 0.396918, acc: 0.875000]\n",
            "638: [Discriminator loss: 0.586477, acc: 0.542969]  [Adversarial loss: 0.446164, acc: 0.839844]\n",
            "639: [Discriminator loss: 0.624532, acc: 0.507812]  [Adversarial loss: 0.412523, acc: 0.871094]\n",
            "640: [Discriminator loss: 0.582804, acc: 0.515625]  [Adversarial loss: 0.414751, acc: 0.875000]\n",
            "641: [Discriminator loss: 0.566868, acc: 0.570312]  [Adversarial loss: 0.414345, acc: 0.886719]\n",
            "642: [Discriminator loss: 0.594412, acc: 0.535156]  [Adversarial loss: 0.389571, acc: 0.882812]\n",
            "643: [Discriminator loss: 0.607329, acc: 0.546875]  [Adversarial loss: 0.426204, acc: 0.859375]\n",
            "644: [Discriminator loss: 0.663413, acc: 0.523438]  [Adversarial loss: 0.406397, acc: 0.863281]\n",
            "645: [Discriminator loss: 0.708078, acc: 0.480469]  [Adversarial loss: 0.450762, acc: 0.875000]\n",
            "646: [Discriminator loss: 0.660080, acc: 0.492188]  [Adversarial loss: 0.442241, acc: 0.863281]\n",
            "647: [Discriminator loss: 0.616566, acc: 0.500000]  [Adversarial loss: 0.439225, acc: 0.839844]\n",
            "648: [Discriminator loss: 0.557987, acc: 0.582031]  [Adversarial loss: 0.384915, acc: 0.875000]\n",
            "649: [Discriminator loss: 0.577295, acc: 0.562500]  [Adversarial loss: 0.448292, acc: 0.843750]\n",
            "650: [Discriminator loss: 0.581104, acc: 0.574219]  [Adversarial loss: 0.440993, acc: 0.828125]\n",
            "651: [Discriminator loss: 0.575997, acc: 0.554688]  [Adversarial loss: 0.406899, acc: 0.878906]\n",
            "652: [Discriminator loss: 0.672801, acc: 0.546875]  [Adversarial loss: 0.416453, acc: 0.820312]\n",
            "653: [Discriminator loss: 0.613921, acc: 0.527344]  [Adversarial loss: 0.406034, acc: 0.847656]\n",
            "654: [Discriminator loss: 0.668196, acc: 0.484375]  [Adversarial loss: 0.449934, acc: 0.839844]\n",
            "655: [Discriminator loss: 0.623344, acc: 0.535156]  [Adversarial loss: 0.422056, acc: 0.843750]\n",
            "656: [Discriminator loss: 0.647228, acc: 0.527344]  [Adversarial loss: 0.384813, acc: 0.867188]\n",
            "657: [Discriminator loss: 0.584282, acc: 0.574219]  [Adversarial loss: 0.377346, acc: 0.894531]\n",
            "658: [Discriminator loss: 0.563626, acc: 0.546875]  [Adversarial loss: 0.357239, acc: 0.875000]\n",
            "659: [Discriminator loss: 0.592012, acc: 0.566406]  [Adversarial loss: 0.472847, acc: 0.863281]\n",
            "660: [Discriminator loss: 0.698600, acc: 0.515625]  [Adversarial loss: 0.432223, acc: 0.835938]\n",
            "661: [Discriminator loss: 0.600143, acc: 0.539062]  [Adversarial loss: 0.402161, acc: 0.882812]\n",
            "662: [Discriminator loss: 0.626231, acc: 0.527344]  [Adversarial loss: 0.428685, acc: 0.867188]\n",
            "663: [Discriminator loss: 0.621126, acc: 0.539062]  [Adversarial loss: 0.452985, acc: 0.835938]\n",
            "664: [Discriminator loss: 0.588496, acc: 0.554688]  [Adversarial loss: 0.404587, acc: 0.855469]\n",
            "665: [Discriminator loss: 0.568351, acc: 0.570312]  [Adversarial loss: 0.374955, acc: 0.835938]\n",
            "666: [Discriminator loss: 0.562868, acc: 0.589844]  [Adversarial loss: 0.419699, acc: 0.894531]\n",
            "667: [Discriminator loss: 0.552399, acc: 0.570312]  [Adversarial loss: 0.433838, acc: 0.832031]\n",
            "668: [Discriminator loss: 0.611298, acc: 0.558594]  [Adversarial loss: 0.424631, acc: 0.843750]\n",
            "669: [Discriminator loss: 0.612323, acc: 0.523438]  [Adversarial loss: 0.463635, acc: 0.871094]\n",
            "670: [Discriminator loss: 0.621351, acc: 0.562500]  [Adversarial loss: 0.373421, acc: 0.882812]\n",
            "671: [Discriminator loss: 0.633335, acc: 0.519531]  [Adversarial loss: 0.369543, acc: 0.863281]\n",
            "672: [Discriminator loss: 0.605009, acc: 0.554688]  [Adversarial loss: 0.385686, acc: 0.894531]\n",
            "673: [Discriminator loss: 0.599030, acc: 0.539062]  [Adversarial loss: 0.412938, acc: 0.847656]\n",
            "674: [Discriminator loss: 0.548772, acc: 0.609375]  [Adversarial loss: 0.351908, acc: 0.851562]\n",
            "675: [Discriminator loss: 0.589750, acc: 0.578125]  [Adversarial loss: 0.393769, acc: 0.871094]\n",
            "676: [Discriminator loss: 0.641075, acc: 0.539062]  [Adversarial loss: 0.370983, acc: 0.878906]\n",
            "677: [Discriminator loss: 0.599893, acc: 0.574219]  [Adversarial loss: 0.392879, acc: 0.878906]\n",
            "678: [Discriminator loss: 0.624889, acc: 0.542969]  [Adversarial loss: 0.434177, acc: 0.863281]\n",
            "679: [Discriminator loss: 0.587925, acc: 0.593750]  [Adversarial loss: 0.445441, acc: 0.796875]\n",
            "680: [Discriminator loss: 0.604658, acc: 0.535156]  [Adversarial loss: 0.487580, acc: 0.820312]\n",
            "681: [Discriminator loss: 0.626731, acc: 0.511719]  [Adversarial loss: 0.389588, acc: 0.894531]\n",
            "682: [Discriminator loss: 0.582088, acc: 0.566406]  [Adversarial loss: 0.391858, acc: 0.855469]\n",
            "683: [Discriminator loss: 0.608818, acc: 0.535156]  [Adversarial loss: 0.389493, acc: 0.886719]\n",
            "684: [Discriminator loss: 0.571438, acc: 0.566406]  [Adversarial loss: 0.443868, acc: 0.828125]\n",
            "685: [Discriminator loss: 0.608048, acc: 0.535156]  [Adversarial loss: 0.425276, acc: 0.828125]\n",
            "686: [Discriminator loss: 0.634288, acc: 0.550781]  [Adversarial loss: 0.391592, acc: 0.894531]\n",
            "687: [Discriminator loss: 0.623268, acc: 0.550781]  [Adversarial loss: 0.375075, acc: 0.882812]\n",
            "688: [Discriminator loss: 0.612171, acc: 0.503906]  [Adversarial loss: 0.419206, acc: 0.867188]\n",
            "689: [Discriminator loss: 0.610179, acc: 0.523438]  [Adversarial loss: 0.400505, acc: 0.882812]\n",
            "690: [Discriminator loss: 0.630174, acc: 0.515625]  [Adversarial loss: 0.400148, acc: 0.867188]\n",
            "691: [Discriminator loss: 0.607541, acc: 0.535156]  [Adversarial loss: 0.385093, acc: 0.875000]\n",
            "692: [Discriminator loss: 0.596152, acc: 0.535156]  [Adversarial loss: 0.347628, acc: 0.890625]\n",
            "693: [Discriminator loss: 0.620050, acc: 0.515625]  [Adversarial loss: 0.414436, acc: 0.863281]\n",
            "694: [Discriminator loss: 0.591467, acc: 0.558594]  [Adversarial loss: 0.360315, acc: 0.878906]\n",
            "695: [Discriminator loss: 0.575421, acc: 0.531250]  [Adversarial loss: 0.352499, acc: 0.914062]\n",
            "696: [Discriminator loss: 0.648346, acc: 0.515625]  [Adversarial loss: 0.482577, acc: 0.835938]\n",
            "697: [Discriminator loss: 0.524961, acc: 0.582031]  [Adversarial loss: 0.424942, acc: 0.839844]\n",
            "698: [Discriminator loss: 0.588913, acc: 0.593750]  [Adversarial loss: 0.356819, acc: 0.906250]\n",
            "699: [Discriminator loss: 0.720595, acc: 0.519531]  [Adversarial loss: 0.409796, acc: 0.851562]\n",
            "700: [Discriminator loss: 0.612086, acc: 0.539062]  [Adversarial loss: 0.389124, acc: 0.847656]\n",
            "701: [Discriminator loss: 0.647685, acc: 0.523438]  [Adversarial loss: 0.388410, acc: 0.886719]\n",
            "702: [Discriminator loss: 0.536521, acc: 0.597656]  [Adversarial loss: 0.367331, acc: 0.894531]\n",
            "703: [Discriminator loss: 0.526779, acc: 0.601562]  [Adversarial loss: 0.429988, acc: 0.878906]\n",
            "704: [Discriminator loss: 0.534377, acc: 0.582031]  [Adversarial loss: 0.371780, acc: 0.910156]\n",
            "705: [Discriminator loss: 0.743133, acc: 0.460938]  [Adversarial loss: 0.451409, acc: 0.855469]\n",
            "706: [Discriminator loss: 0.563749, acc: 0.593750]  [Adversarial loss: 0.402614, acc: 0.859375]\n",
            "707: [Discriminator loss: 0.602823, acc: 0.535156]  [Adversarial loss: 0.343028, acc: 0.906250]\n",
            "708: [Discriminator loss: 0.627540, acc: 0.531250]  [Adversarial loss: 0.370594, acc: 0.898438]\n",
            "709: [Discriminator loss: 0.639688, acc: 0.539062]  [Adversarial loss: 0.363235, acc: 0.902344]\n",
            "710: [Discriminator loss: 0.594229, acc: 0.597656]  [Adversarial loss: 0.387461, acc: 0.878906]\n",
            "711: [Discriminator loss: 0.635508, acc: 0.519531]  [Adversarial loss: 0.363623, acc: 0.878906]\n",
            "712: [Discriminator loss: 0.566245, acc: 0.578125]  [Adversarial loss: 0.407064, acc: 0.906250]\n",
            "713: [Discriminator loss: 0.615187, acc: 0.515625]  [Adversarial loss: 0.403667, acc: 0.878906]\n",
            "714: [Discriminator loss: 0.602692, acc: 0.566406]  [Adversarial loss: 0.387706, acc: 0.878906]\n",
            "715: [Discriminator loss: 0.550388, acc: 0.613281]  [Adversarial loss: 0.405792, acc: 0.871094]\n",
            "716: [Discriminator loss: 0.604244, acc: 0.562500]  [Adversarial loss: 0.347494, acc: 0.894531]\n",
            "717: [Discriminator loss: 0.647597, acc: 0.546875]  [Adversarial loss: 0.392921, acc: 0.859375]\n",
            "718: [Discriminator loss: 0.652510, acc: 0.535156]  [Adversarial loss: 0.403839, acc: 0.863281]\n",
            "719: [Discriminator loss: 0.647908, acc: 0.535156]  [Adversarial loss: 0.390500, acc: 0.867188]\n",
            "720: [Discriminator loss: 0.585364, acc: 0.585938]  [Adversarial loss: 0.387118, acc: 0.847656]\n",
            "721: [Discriminator loss: 0.574479, acc: 0.546875]  [Adversarial loss: 0.402937, acc: 0.863281]\n",
            "722: [Discriminator loss: 0.599023, acc: 0.535156]  [Adversarial loss: 0.406846, acc: 0.867188]\n",
            "723: [Discriminator loss: 0.571607, acc: 0.621094]  [Adversarial loss: 0.375580, acc: 0.847656]\n",
            "724: [Discriminator loss: 0.593897, acc: 0.531250]  [Adversarial loss: 0.400301, acc: 0.875000]\n",
            "725: [Discriminator loss: 0.586428, acc: 0.523438]  [Adversarial loss: 0.422214, acc: 0.847656]\n",
            "726: [Discriminator loss: 0.647946, acc: 0.539062]  [Adversarial loss: 0.431762, acc: 0.832031]\n",
            "727: [Discriminator loss: 0.577972, acc: 0.597656]  [Adversarial loss: 0.382005, acc: 0.851562]\n",
            "728: [Discriminator loss: 0.634384, acc: 0.574219]  [Adversarial loss: 0.385896, acc: 0.847656]\n",
            "729: [Discriminator loss: 0.594632, acc: 0.535156]  [Adversarial loss: 0.421136, acc: 0.843750]\n",
            "730: [Discriminator loss: 0.615382, acc: 0.519531]  [Adversarial loss: 0.448831, acc: 0.847656]\n",
            "731: [Discriminator loss: 0.644678, acc: 0.507812]  [Adversarial loss: 0.431548, acc: 0.843750]\n",
            "732: [Discriminator loss: 0.583747, acc: 0.570312]  [Adversarial loss: 0.428168, acc: 0.867188]\n",
            "733: [Discriminator loss: 0.633559, acc: 0.605469]  [Adversarial loss: 0.387438, acc: 0.843750]\n",
            "734: [Discriminator loss: 0.598269, acc: 0.546875]  [Adversarial loss: 0.409696, acc: 0.851562]\n",
            "735: [Discriminator loss: 0.615046, acc: 0.546875]  [Adversarial loss: 0.393651, acc: 0.843750]\n",
            "736: [Discriminator loss: 0.638973, acc: 0.527344]  [Adversarial loss: 0.475047, acc: 0.824219]\n",
            "737: [Discriminator loss: 0.642096, acc: 0.554688]  [Adversarial loss: 0.485553, acc: 0.765625]\n",
            "738: [Discriminator loss: 0.547265, acc: 0.578125]  [Adversarial loss: 0.433315, acc: 0.832031]\n",
            "739: [Discriminator loss: 0.594456, acc: 0.570312]  [Adversarial loss: 0.518001, acc: 0.746094]\n",
            "740: [Discriminator loss: 0.575973, acc: 0.570312]  [Adversarial loss: 0.457498, acc: 0.796875]\n",
            "741: [Discriminator loss: 0.580263, acc: 0.585938]  [Adversarial loss: 0.433869, acc: 0.832031]\n",
            "742: [Discriminator loss: 0.561636, acc: 0.609375]  [Adversarial loss: 0.445709, acc: 0.816406]\n",
            "743: [Discriminator loss: 0.610682, acc: 0.574219]  [Adversarial loss: 0.426840, acc: 0.820312]\n",
            "744: [Discriminator loss: 0.620656, acc: 0.585938]  [Adversarial loss: 0.463107, acc: 0.792969]\n",
            "745: [Discriminator loss: 0.590949, acc: 0.578125]  [Adversarial loss: 0.467351, acc: 0.761719]\n",
            "746: [Discriminator loss: 0.620143, acc: 0.582031]  [Adversarial loss: 0.443441, acc: 0.828125]\n",
            "747: [Discriminator loss: 0.589249, acc: 0.593750]  [Adversarial loss: 0.419500, acc: 0.851562]\n",
            "748: [Discriminator loss: 0.648649, acc: 0.566406]  [Adversarial loss: 0.460333, acc: 0.792969]\n",
            "749: [Discriminator loss: 0.529616, acc: 0.613281]  [Adversarial loss: 0.425433, acc: 0.820312]\n",
            "750: [Discriminator loss: 0.624561, acc: 0.550781]  [Adversarial loss: 0.451777, acc: 0.789062]\n",
            "751: [Discriminator loss: 0.612627, acc: 0.503906]  [Adversarial loss: 0.460223, acc: 0.785156]\n",
            "752: [Discriminator loss: 0.591379, acc: 0.566406]  [Adversarial loss: 0.422899, acc: 0.808594]\n",
            "753: [Discriminator loss: 0.615452, acc: 0.546875]  [Adversarial loss: 0.410546, acc: 0.835938]\n",
            "754: [Discriminator loss: 0.683277, acc: 0.523438]  [Adversarial loss: 0.425242, acc: 0.839844]\n",
            "755: [Discriminator loss: 0.630331, acc: 0.558594]  [Adversarial loss: 0.420379, acc: 0.832031]\n",
            "756: [Discriminator loss: 0.625499, acc: 0.507812]  [Adversarial loss: 0.469789, acc: 0.792969]\n",
            "757: [Discriminator loss: 0.638865, acc: 0.539062]  [Adversarial loss: 0.452119, acc: 0.832031]\n",
            "758: [Discriminator loss: 0.610236, acc: 0.550781]  [Adversarial loss: 0.462051, acc: 0.804688]\n",
            "759: [Discriminator loss: 0.563710, acc: 0.617188]  [Adversarial loss: 0.467053, acc: 0.839844]\n",
            "760: [Discriminator loss: 0.569583, acc: 0.578125]  [Adversarial loss: 0.463809, acc: 0.796875]\n",
            "761: [Discriminator loss: 0.523053, acc: 0.613281]  [Adversarial loss: 0.426602, acc: 0.781250]\n",
            "762: [Discriminator loss: 0.584665, acc: 0.574219]  [Adversarial loss: 0.401969, acc: 0.835938]\n",
            "763: [Discriminator loss: 0.617661, acc: 0.554688]  [Adversarial loss: 0.441867, acc: 0.808594]\n",
            "764: [Discriminator loss: 0.606719, acc: 0.601562]  [Adversarial loss: 0.457522, acc: 0.808594]\n",
            "765: [Discriminator loss: 0.538661, acc: 0.601562]  [Adversarial loss: 0.431852, acc: 0.816406]\n",
            "766: [Discriminator loss: 0.539225, acc: 0.601562]  [Adversarial loss: 0.463097, acc: 0.804688]\n",
            "767: [Discriminator loss: 0.645432, acc: 0.542969]  [Adversarial loss: 0.435500, acc: 0.820312]\n",
            "768: [Discriminator loss: 0.612980, acc: 0.539062]  [Adversarial loss: 0.473675, acc: 0.804688]\n",
            "769: [Discriminator loss: 0.570128, acc: 0.566406]  [Adversarial loss: 0.450084, acc: 0.804688]\n",
            "770: [Discriminator loss: 0.630080, acc: 0.515625]  [Adversarial loss: 0.456441, acc: 0.843750]\n",
            "771: [Discriminator loss: 0.546937, acc: 0.609375]  [Adversarial loss: 0.411298, acc: 0.855469]\n",
            "772: [Discriminator loss: 0.487226, acc: 0.660156]  [Adversarial loss: 0.396950, acc: 0.867188]\n",
            "773: [Discriminator loss: 0.505771, acc: 0.597656]  [Adversarial loss: 0.399999, acc: 0.855469]\n",
            "774: [Discriminator loss: 0.591788, acc: 0.589844]  [Adversarial loss: 0.406077, acc: 0.820312]\n",
            "775: [Discriminator loss: 0.566320, acc: 0.589844]  [Adversarial loss: 0.437768, acc: 0.820312]\n",
            "776: [Discriminator loss: 0.628899, acc: 0.523438]  [Adversarial loss: 0.418210, acc: 0.808594]\n",
            "777: [Discriminator loss: 0.557599, acc: 0.574219]  [Adversarial loss: 0.439304, acc: 0.847656]\n",
            "778: [Discriminator loss: 0.608243, acc: 0.597656]  [Adversarial loss: 0.368583, acc: 0.863281]\n",
            "779: [Discriminator loss: 0.562483, acc: 0.554688]  [Adversarial loss: 0.442690, acc: 0.835938]\n",
            "780: [Discriminator loss: 0.603681, acc: 0.550781]  [Adversarial loss: 0.426641, acc: 0.863281]\n",
            "781: [Discriminator loss: 0.632427, acc: 0.531250]  [Adversarial loss: 0.411504, acc: 0.875000]\n",
            "782: [Discriminator loss: 0.594132, acc: 0.578125]  [Adversarial loss: 0.410111, acc: 0.824219]\n",
            "783: [Discriminator loss: 0.676102, acc: 0.550781]  [Adversarial loss: 0.442257, acc: 0.820312]\n",
            "784: [Discriminator loss: 0.574616, acc: 0.585938]  [Adversarial loss: 0.381531, acc: 0.855469]\n",
            "785: [Discriminator loss: 0.674382, acc: 0.523438]  [Adversarial loss: 0.444929, acc: 0.816406]\n",
            "786: [Discriminator loss: 0.556458, acc: 0.605469]  [Adversarial loss: 0.451348, acc: 0.808594]\n",
            "787: [Discriminator loss: 0.569396, acc: 0.558594]  [Adversarial loss: 0.419090, acc: 0.851562]\n",
            "788: [Discriminator loss: 0.558640, acc: 0.585938]  [Adversarial loss: 0.415926, acc: 0.824219]\n",
            "789: [Discriminator loss: 0.593512, acc: 0.562500]  [Adversarial loss: 0.403345, acc: 0.832031]\n",
            "790: [Discriminator loss: 0.584763, acc: 0.578125]  [Adversarial loss: 0.450352, acc: 0.835938]\n",
            "791: [Discriminator loss: 0.563595, acc: 0.582031]  [Adversarial loss: 0.398658, acc: 0.828125]\n",
            "792: [Discriminator loss: 0.516889, acc: 0.660156]  [Adversarial loss: 0.399743, acc: 0.863281]\n",
            "793: [Discriminator loss: 0.588094, acc: 0.554688]  [Adversarial loss: 0.436646, acc: 0.808594]\n",
            "794: [Discriminator loss: 0.575516, acc: 0.574219]  [Adversarial loss: 0.443206, acc: 0.804688]\n",
            "795: [Discriminator loss: 0.601594, acc: 0.554688]  [Adversarial loss: 0.373578, acc: 0.871094]\n",
            "796: [Discriminator loss: 0.588965, acc: 0.582031]  [Adversarial loss: 0.384298, acc: 0.871094]\n",
            "797: [Discriminator loss: 0.511736, acc: 0.554688]  [Adversarial loss: 0.423620, acc: 0.855469]\n",
            "798: [Discriminator loss: 0.571907, acc: 0.578125]  [Adversarial loss: 0.407823, acc: 0.855469]\n",
            "799: [Discriminator loss: 0.539470, acc: 0.574219]  [Adversarial loss: 0.389439, acc: 0.847656]\n",
            "800: [Discriminator loss: 0.625900, acc: 0.554688]  [Adversarial loss: 0.357624, acc: 0.898438]\n",
            "801: [Discriminator loss: 0.558247, acc: 0.578125]  [Adversarial loss: 0.429758, acc: 0.843750]\n",
            "802: [Discriminator loss: 0.642038, acc: 0.570312]  [Adversarial loss: 0.385292, acc: 0.867188]\n",
            "803: [Discriminator loss: 0.556560, acc: 0.550781]  [Adversarial loss: 0.397521, acc: 0.859375]\n",
            "804: [Discriminator loss: 0.618949, acc: 0.523438]  [Adversarial loss: 0.371938, acc: 0.867188]\n",
            "805: [Discriminator loss: 0.538515, acc: 0.574219]  [Adversarial loss: 0.411537, acc: 0.859375]\n",
            "806: [Discriminator loss: 0.570562, acc: 0.558594]  [Adversarial loss: 0.393965, acc: 0.835938]\n",
            "807: [Discriminator loss: 0.531625, acc: 0.597656]  [Adversarial loss: 0.370211, acc: 0.878906]\n",
            "808: [Discriminator loss: 0.549424, acc: 0.570312]  [Adversarial loss: 0.381331, acc: 0.863281]\n",
            "809: [Discriminator loss: 0.593340, acc: 0.539062]  [Adversarial loss: 0.429244, acc: 0.835938]\n",
            "810: [Discriminator loss: 0.576483, acc: 0.542969]  [Adversarial loss: 0.432531, acc: 0.863281]\n",
            "811: [Discriminator loss: 0.577701, acc: 0.574219]  [Adversarial loss: 0.393249, acc: 0.867188]\n",
            "812: [Discriminator loss: 0.577247, acc: 0.554688]  [Adversarial loss: 0.426765, acc: 0.832031]\n",
            "813: [Discriminator loss: 0.563779, acc: 0.578125]  [Adversarial loss: 0.374634, acc: 0.867188]\n",
            "814: [Discriminator loss: 0.610700, acc: 0.593750]  [Adversarial loss: 0.383080, acc: 0.875000]\n",
            "815: [Discriminator loss: 0.585224, acc: 0.515625]  [Adversarial loss: 0.401080, acc: 0.890625]\n",
            "816: [Discriminator loss: 0.588443, acc: 0.554688]  [Adversarial loss: 0.406198, acc: 0.832031]\n",
            "817: [Discriminator loss: 0.595580, acc: 0.535156]  [Adversarial loss: 0.401036, acc: 0.863281]\n",
            "818: [Discriminator loss: 0.535457, acc: 0.578125]  [Adversarial loss: 0.348147, acc: 0.867188]\n",
            "819: [Discriminator loss: 0.540549, acc: 0.574219]  [Adversarial loss: 0.354120, acc: 0.851562]\n",
            "820: [Discriminator loss: 0.667685, acc: 0.515625]  [Adversarial loss: 0.429880, acc: 0.839844]\n",
            "821: [Discriminator loss: 0.597617, acc: 0.578125]  [Adversarial loss: 0.387340, acc: 0.878906]\n",
            "822: [Discriminator loss: 0.600325, acc: 0.562500]  [Adversarial loss: 0.419402, acc: 0.832031]\n",
            "823: [Discriminator loss: 0.642752, acc: 0.519531]  [Adversarial loss: 0.387413, acc: 0.871094]\n",
            "824: [Discriminator loss: 0.561577, acc: 0.593750]  [Adversarial loss: 0.366919, acc: 0.898438]\n",
            "825: [Discriminator loss: 0.514309, acc: 0.625000]  [Adversarial loss: 0.372999, acc: 0.855469]\n",
            "826: [Discriminator loss: 0.587066, acc: 0.566406]  [Adversarial loss: 0.403327, acc: 0.859375]\n",
            "827: [Discriminator loss: 0.544691, acc: 0.585938]  [Adversarial loss: 0.369339, acc: 0.875000]\n",
            "828: [Discriminator loss: 0.641765, acc: 0.503906]  [Adversarial loss: 0.493921, acc: 0.832031]\n",
            "829: [Discriminator loss: 0.621343, acc: 0.531250]  [Adversarial loss: 0.402127, acc: 0.851562]\n",
            "830: [Discriminator loss: 0.670876, acc: 0.519531]  [Adversarial loss: 0.474600, acc: 0.835938]\n",
            "831: [Discriminator loss: 0.517056, acc: 0.617188]  [Adversarial loss: 0.369080, acc: 0.890625]\n",
            "832: [Discriminator loss: 0.553405, acc: 0.558594]  [Adversarial loss: 0.424740, acc: 0.808594]\n",
            "833: [Discriminator loss: 0.614614, acc: 0.515625]  [Adversarial loss: 0.453267, acc: 0.843750]\n",
            "834: [Discriminator loss: 0.609390, acc: 0.527344]  [Adversarial loss: 0.360330, acc: 0.863281]\n",
            "835: [Discriminator loss: 0.582168, acc: 0.558594]  [Adversarial loss: 0.400887, acc: 0.851562]\n",
            "836: [Discriminator loss: 0.564445, acc: 0.574219]  [Adversarial loss: 0.373202, acc: 0.839844]\n",
            "837: [Discriminator loss: 0.651922, acc: 0.531250]  [Adversarial loss: 0.422097, acc: 0.843750]\n",
            "838: [Discriminator loss: 0.551536, acc: 0.570312]  [Adversarial loss: 0.373392, acc: 0.871094]\n",
            "839: [Discriminator loss: 0.501514, acc: 0.597656]  [Adversarial loss: 0.393232, acc: 0.871094]\n",
            "840: [Discriminator loss: 0.532264, acc: 0.566406]  [Adversarial loss: 0.363330, acc: 0.863281]\n",
            "841: [Discriminator loss: 0.534341, acc: 0.589844]  [Adversarial loss: 0.404489, acc: 0.839844]\n",
            "842: [Discriminator loss: 0.602124, acc: 0.550781]  [Adversarial loss: 0.390217, acc: 0.855469]\n",
            "843: [Discriminator loss: 0.549828, acc: 0.566406]  [Adversarial loss: 0.367205, acc: 0.894531]\n",
            "844: [Discriminator loss: 0.541432, acc: 0.578125]  [Adversarial loss: 0.368134, acc: 0.898438]\n",
            "845: [Discriminator loss: 0.555316, acc: 0.574219]  [Adversarial loss: 0.382087, acc: 0.890625]\n",
            "846: [Discriminator loss: 0.602652, acc: 0.542969]  [Adversarial loss: 0.406480, acc: 0.894531]\n",
            "847: [Discriminator loss: 0.568418, acc: 0.542969]  [Adversarial loss: 0.350227, acc: 0.882812]\n",
            "848: [Discriminator loss: 0.584459, acc: 0.542969]  [Adversarial loss: 0.392486, acc: 0.875000]\n",
            "849: [Discriminator loss: 0.559196, acc: 0.574219]  [Adversarial loss: 0.317277, acc: 0.902344]\n",
            "850: [Discriminator loss: 0.539270, acc: 0.585938]  [Adversarial loss: 0.361377, acc: 0.882812]\n",
            "851: [Discriminator loss: 0.463970, acc: 0.648438]  [Adversarial loss: 0.343154, acc: 0.890625]\n",
            "852: [Discriminator loss: 0.572690, acc: 0.507812]  [Adversarial loss: 0.389386, acc: 0.875000]\n",
            "853: [Discriminator loss: 0.594859, acc: 0.515625]  [Adversarial loss: 0.437928, acc: 0.871094]\n",
            "854: [Discriminator loss: 0.572948, acc: 0.550781]  [Adversarial loss: 0.393542, acc: 0.855469]\n",
            "855: [Discriminator loss: 0.655066, acc: 0.519531]  [Adversarial loss: 0.389230, acc: 0.851562]\n",
            "856: [Discriminator loss: 0.554557, acc: 0.562500]  [Adversarial loss: 0.411830, acc: 0.859375]\n",
            "857: [Discriminator loss: 0.535698, acc: 0.589844]  [Adversarial loss: 0.311621, acc: 0.917969]\n",
            "858: [Discriminator loss: 0.612206, acc: 0.535156]  [Adversarial loss: 0.372341, acc: 0.855469]\n",
            "859: [Discriminator loss: 0.584183, acc: 0.535156]  [Adversarial loss: 0.368306, acc: 0.878906]\n",
            "860: [Discriminator loss: 0.645801, acc: 0.515625]  [Adversarial loss: 0.375121, acc: 0.902344]\n",
            "861: [Discriminator loss: 0.597077, acc: 0.542969]  [Adversarial loss: 0.350173, acc: 0.882812]\n",
            "862: [Discriminator loss: 0.618203, acc: 0.601562]  [Adversarial loss: 0.394227, acc: 0.886719]\n",
            "863: [Discriminator loss: 0.639389, acc: 0.558594]  [Adversarial loss: 0.441277, acc: 0.847656]\n",
            "864: [Discriminator loss: 0.577738, acc: 0.593750]  [Adversarial loss: 0.342397, acc: 0.886719]\n",
            "865: [Discriminator loss: 0.578766, acc: 0.542969]  [Adversarial loss: 0.362132, acc: 0.890625]\n",
            "866: [Discriminator loss: 0.614049, acc: 0.531250]  [Adversarial loss: 0.365672, acc: 0.894531]\n",
            "867: [Discriminator loss: 0.593593, acc: 0.500000]  [Adversarial loss: 0.406230, acc: 0.843750]\n",
            "868: [Discriminator loss: 0.517045, acc: 0.578125]  [Adversarial loss: 0.353157, acc: 0.871094]\n",
            "869: [Discriminator loss: 0.606365, acc: 0.554688]  [Adversarial loss: 0.366195, acc: 0.867188]\n",
            "870: [Discriminator loss: 0.524831, acc: 0.597656]  [Adversarial loss: 0.367876, acc: 0.867188]\n",
            "871: [Discriminator loss: 0.559043, acc: 0.554688]  [Adversarial loss: 0.374550, acc: 0.863281]\n",
            "872: [Discriminator loss: 0.517071, acc: 0.597656]  [Adversarial loss: 0.390125, acc: 0.835938]\n",
            "873: [Discriminator loss: 0.568451, acc: 0.562500]  [Adversarial loss: 0.377093, acc: 0.832031]\n",
            "874: [Discriminator loss: 0.585168, acc: 0.578125]  [Adversarial loss: 0.417840, acc: 0.863281]\n",
            "875: [Discriminator loss: 0.512925, acc: 0.605469]  [Adversarial loss: 0.287025, acc: 0.917969]\n",
            "876: [Discriminator loss: 0.582036, acc: 0.546875]  [Adversarial loss: 0.363201, acc: 0.859375]\n",
            "877: [Discriminator loss: 0.566586, acc: 0.570312]  [Adversarial loss: 0.348925, acc: 0.867188]\n",
            "878: [Discriminator loss: 0.524663, acc: 0.585938]  [Adversarial loss: 0.386155, acc: 0.863281]\n",
            "879: [Discriminator loss: 0.565743, acc: 0.578125]  [Adversarial loss: 0.346809, acc: 0.917969]\n",
            "880: [Discriminator loss: 0.565826, acc: 0.570312]  [Adversarial loss: 0.340631, acc: 0.875000]\n",
            "881: [Discriminator loss: 0.537107, acc: 0.570312]  [Adversarial loss: 0.368336, acc: 0.882812]\n",
            "882: [Discriminator loss: 0.544743, acc: 0.566406]  [Adversarial loss: 0.388662, acc: 0.859375]\n",
            "883: [Discriminator loss: 0.626068, acc: 0.546875]  [Adversarial loss: 0.376938, acc: 0.839844]\n",
            "884: [Discriminator loss: 0.605786, acc: 0.566406]  [Adversarial loss: 0.379467, acc: 0.804688]\n",
            "885: [Discriminator loss: 0.492213, acc: 0.613281]  [Adversarial loss: 0.323625, acc: 0.863281]\n",
            "886: [Discriminator loss: 0.495265, acc: 0.617188]  [Adversarial loss: 0.344072, acc: 0.863281]\n",
            "887: [Discriminator loss: 0.582404, acc: 0.578125]  [Adversarial loss: 0.377660, acc: 0.863281]\n",
            "888: [Discriminator loss: 0.438528, acc: 0.667969]  [Adversarial loss: 0.352571, acc: 0.871094]\n",
            "889: [Discriminator loss: 0.486078, acc: 0.640625]  [Adversarial loss: 0.356929, acc: 0.894531]\n",
            "890: [Discriminator loss: 0.577980, acc: 0.574219]  [Adversarial loss: 0.399798, acc: 0.867188]\n",
            "891: [Discriminator loss: 0.502827, acc: 0.597656]  [Adversarial loss: 0.359412, acc: 0.859375]\n",
            "892: [Discriminator loss: 0.500173, acc: 0.605469]  [Adversarial loss: 0.359897, acc: 0.859375]\n",
            "893: [Discriminator loss: 0.539622, acc: 0.609375]  [Adversarial loss: 0.350414, acc: 0.890625]\n",
            "894: [Discriminator loss: 0.666368, acc: 0.566406]  [Adversarial loss: 0.350014, acc: 0.871094]\n",
            "895: [Discriminator loss: 0.590326, acc: 0.609375]  [Adversarial loss: 0.356810, acc: 0.894531]\n",
            "896: [Discriminator loss: 0.524414, acc: 0.582031]  [Adversarial loss: 0.348027, acc: 0.898438]\n",
            "897: [Discriminator loss: 0.584890, acc: 0.601562]  [Adversarial loss: 0.355339, acc: 0.882812]\n",
            "898: [Discriminator loss: 0.560046, acc: 0.585938]  [Adversarial loss: 0.359471, acc: 0.867188]\n",
            "899: [Discriminator loss: 0.598073, acc: 0.562500]  [Adversarial loss: 0.397957, acc: 0.859375]\n",
            "900: [Discriminator loss: 0.620259, acc: 0.519531]  [Adversarial loss: 0.388099, acc: 0.863281]\n",
            "901: [Discriminator loss: 0.627885, acc: 0.562500]  [Adversarial loss: 0.361139, acc: 0.875000]\n",
            "902: [Discriminator loss: 0.613281, acc: 0.558594]  [Adversarial loss: 0.381964, acc: 0.878906]\n",
            "903: [Discriminator loss: 0.639166, acc: 0.515625]  [Adversarial loss: 0.405296, acc: 0.839844]\n",
            "904: [Discriminator loss: 0.670018, acc: 0.535156]  [Adversarial loss: 0.459879, acc: 0.863281]\n",
            "905: [Discriminator loss: 0.610041, acc: 0.519531]  [Adversarial loss: 0.433681, acc: 0.847656]\n",
            "906: [Discriminator loss: 0.627641, acc: 0.519531]  [Adversarial loss: 0.460213, acc: 0.796875]\n",
            "907: [Discriminator loss: 0.615695, acc: 0.539062]  [Adversarial loss: 0.496672, acc: 0.777344]\n",
            "908: [Discriminator loss: 0.627128, acc: 0.535156]  [Adversarial loss: 0.422349, acc: 0.804688]\n",
            "909: [Discriminator loss: 0.546027, acc: 0.578125]  [Adversarial loss: 0.461840, acc: 0.765625]\n",
            "910: [Discriminator loss: 0.593866, acc: 0.574219]  [Adversarial loss: 0.448051, acc: 0.804688]\n",
            "911: [Discriminator loss: 0.625264, acc: 0.566406]  [Adversarial loss: 0.449854, acc: 0.792969]\n",
            "912: [Discriminator loss: 0.608509, acc: 0.542969]  [Adversarial loss: 0.471771, acc: 0.808594]\n",
            "913: [Discriminator loss: 0.591750, acc: 0.589844]  [Adversarial loss: 0.457925, acc: 0.832031]\n",
            "914: [Discriminator loss: 0.563537, acc: 0.597656]  [Adversarial loss: 0.437127, acc: 0.843750]\n",
            "915: [Discriminator loss: 0.576019, acc: 0.574219]  [Adversarial loss: 0.411634, acc: 0.867188]\n",
            "916: [Discriminator loss: 0.610707, acc: 0.566406]  [Adversarial loss: 0.437435, acc: 0.828125]\n",
            "917: [Discriminator loss: 0.623678, acc: 0.539062]  [Adversarial loss: 0.424037, acc: 0.855469]\n",
            "918: [Discriminator loss: 0.530598, acc: 0.621094]  [Adversarial loss: 0.367592, acc: 0.855469]\n",
            "919: [Discriminator loss: 0.558532, acc: 0.593750]  [Adversarial loss: 0.420321, acc: 0.847656]\n",
            "920: [Discriminator loss: 0.668507, acc: 0.558594]  [Adversarial loss: 0.449686, acc: 0.812500]\n",
            "921: [Discriminator loss: 0.722639, acc: 0.464844]  [Adversarial loss: 0.468271, acc: 0.816406]\n",
            "922: [Discriminator loss: 0.659354, acc: 0.539062]  [Adversarial loss: 0.416392, acc: 0.800781]\n",
            "923: [Discriminator loss: 0.675721, acc: 0.562500]  [Adversarial loss: 0.406406, acc: 0.859375]\n",
            "924: [Discriminator loss: 0.678992, acc: 0.554688]  [Adversarial loss: 0.394550, acc: 0.820312]\n",
            "925: [Discriminator loss: 0.584934, acc: 0.613281]  [Adversarial loss: 0.391177, acc: 0.859375]\n",
            "926: [Discriminator loss: 0.570063, acc: 0.582031]  [Adversarial loss: 0.366870, acc: 0.855469]\n",
            "927: [Discriminator loss: 0.539183, acc: 0.613281]  [Adversarial loss: 0.404219, acc: 0.859375]\n",
            "928: [Discriminator loss: 0.573363, acc: 0.613281]  [Adversarial loss: 0.360553, acc: 0.878906]\n",
            "929: [Discriminator loss: 0.642596, acc: 0.558594]  [Adversarial loss: 0.439810, acc: 0.828125]\n",
            "930: [Discriminator loss: 0.608243, acc: 0.609375]  [Adversarial loss: 0.412336, acc: 0.828125]\n",
            "931: [Discriminator loss: 0.684612, acc: 0.507812]  [Adversarial loss: 0.458048, acc: 0.804688]\n",
            "932: [Discriminator loss: 0.627538, acc: 0.605469]  [Adversarial loss: 0.408798, acc: 0.851562]\n",
            "933: [Discriminator loss: 0.657847, acc: 0.542969]  [Adversarial loss: 0.423571, acc: 0.835938]\n",
            "934: [Discriminator loss: 0.629338, acc: 0.574219]  [Adversarial loss: 0.441097, acc: 0.824219]\n",
            "935: [Discriminator loss: 0.661065, acc: 0.582031]  [Adversarial loss: 0.374814, acc: 0.835938]\n",
            "936: [Discriminator loss: 0.632443, acc: 0.601562]  [Adversarial loss: 0.429914, acc: 0.839844]\n",
            "937: [Discriminator loss: 0.629145, acc: 0.582031]  [Adversarial loss: 0.432827, acc: 0.859375]\n",
            "938: [Discriminator loss: 0.588342, acc: 0.609375]  [Adversarial loss: 0.376389, acc: 0.816406]\n",
            "939: [Discriminator loss: 0.586320, acc: 0.609375]  [Adversarial loss: 0.423795, acc: 0.824219]\n",
            "940: [Discriminator loss: 0.602630, acc: 0.601562]  [Adversarial loss: 0.383054, acc: 0.835938]\n",
            "941: [Discriminator loss: 0.596315, acc: 0.597656]  [Adversarial loss: 0.391593, acc: 0.843750]\n",
            "942: [Discriminator loss: 0.620204, acc: 0.562500]  [Adversarial loss: 0.394381, acc: 0.839844]\n",
            "943: [Discriminator loss: 0.643657, acc: 0.550781]  [Adversarial loss: 0.417252, acc: 0.808594]\n",
            "944: [Discriminator loss: 0.661765, acc: 0.535156]  [Adversarial loss: 0.389890, acc: 0.839844]\n",
            "945: [Discriminator loss: 0.563809, acc: 0.617188]  [Adversarial loss: 0.378654, acc: 0.828125]\n",
            "946: [Discriminator loss: 0.615026, acc: 0.578125]  [Adversarial loss: 0.344285, acc: 0.882812]\n",
            "947: [Discriminator loss: 0.633934, acc: 0.574219]  [Adversarial loss: 0.422171, acc: 0.820312]\n",
            "948: [Discriminator loss: 0.588436, acc: 0.601562]  [Adversarial loss: 0.486854, acc: 0.808594]\n",
            "949: [Discriminator loss: 0.658001, acc: 0.582031]  [Adversarial loss: 0.403693, acc: 0.843750]\n",
            "950: [Discriminator loss: 0.620585, acc: 0.585938]  [Adversarial loss: 0.351696, acc: 0.871094]\n",
            "951: [Discriminator loss: 0.592417, acc: 0.570312]  [Adversarial loss: 0.371052, acc: 0.843750]\n",
            "952: [Discriminator loss: 0.630273, acc: 0.558594]  [Adversarial loss: 0.397442, acc: 0.855469]\n",
            "953: [Discriminator loss: 0.616680, acc: 0.566406]  [Adversarial loss: 0.368236, acc: 0.847656]\n",
            "954: [Discriminator loss: 0.584632, acc: 0.601562]  [Adversarial loss: 0.411904, acc: 0.863281]\n",
            "955: [Discriminator loss: 0.603298, acc: 0.605469]  [Adversarial loss: 0.359054, acc: 0.859375]\n",
            "956: [Discriminator loss: 0.580224, acc: 0.597656]  [Adversarial loss: 0.378744, acc: 0.828125]\n",
            "957: [Discriminator loss: 0.583407, acc: 0.625000]  [Adversarial loss: 0.408140, acc: 0.808594]\n",
            "958: [Discriminator loss: 0.594474, acc: 0.585938]  [Adversarial loss: 0.377833, acc: 0.882812]\n",
            "959: [Discriminator loss: 0.571943, acc: 0.570312]  [Adversarial loss: 0.431872, acc: 0.820312]\n",
            "960: [Discriminator loss: 0.567253, acc: 0.597656]  [Adversarial loss: 0.414709, acc: 0.824219]\n",
            "961: [Discriminator loss: 0.624000, acc: 0.597656]  [Adversarial loss: 0.423919, acc: 0.816406]\n",
            "962: [Discriminator loss: 0.615855, acc: 0.578125]  [Adversarial loss: 0.383011, acc: 0.839844]\n",
            "963: [Discriminator loss: 0.586123, acc: 0.605469]  [Adversarial loss: 0.400085, acc: 0.851562]\n",
            "964: [Discriminator loss: 0.602505, acc: 0.558594]  [Adversarial loss: 0.408321, acc: 0.839844]\n",
            "965: [Discriminator loss: 0.616238, acc: 0.554688]  [Adversarial loss: 0.394020, acc: 0.847656]\n",
            "966: [Discriminator loss: 0.674321, acc: 0.539062]  [Adversarial loss: 0.446740, acc: 0.851562]\n",
            "967: [Discriminator loss: 0.631769, acc: 0.570312]  [Adversarial loss: 0.389274, acc: 0.867188]\n",
            "968: [Discriminator loss: 0.655825, acc: 0.535156]  [Adversarial loss: 0.424115, acc: 0.863281]\n",
            "969: [Discriminator loss: 0.664185, acc: 0.531250]  [Adversarial loss: 0.465714, acc: 0.824219]\n",
            "970: [Discriminator loss: 0.690230, acc: 0.527344]  [Adversarial loss: 0.383489, acc: 0.878906]\n",
            "971: [Discriminator loss: 0.648888, acc: 0.527344]  [Adversarial loss: 0.425925, acc: 0.800781]\n",
            "972: [Discriminator loss: 0.634688, acc: 0.535156]  [Adversarial loss: 0.424279, acc: 0.843750]\n",
            "973: [Discriminator loss: 0.640539, acc: 0.542969]  [Adversarial loss: 0.414061, acc: 0.847656]\n",
            "974: [Discriminator loss: 0.579091, acc: 0.613281]  [Adversarial loss: 0.407078, acc: 0.839844]\n",
            "975: [Discriminator loss: 0.586458, acc: 0.578125]  [Adversarial loss: 0.376282, acc: 0.886719]\n",
            "976: [Discriminator loss: 0.659411, acc: 0.562500]  [Adversarial loss: 0.429139, acc: 0.871094]\n",
            "977: [Discriminator loss: 0.649046, acc: 0.570312]  [Adversarial loss: 0.394192, acc: 0.882812]\n",
            "978: [Discriminator loss: 0.712283, acc: 0.542969]  [Adversarial loss: 0.388487, acc: 0.863281]\n",
            "979: [Discriminator loss: 0.605646, acc: 0.570312]  [Adversarial loss: 0.386655, acc: 0.875000]\n",
            "980: [Discriminator loss: 0.602336, acc: 0.539062]  [Adversarial loss: 0.401330, acc: 0.835938]\n",
            "981: [Discriminator loss: 0.585410, acc: 0.582031]  [Adversarial loss: 0.401744, acc: 0.832031]\n",
            "982: [Discriminator loss: 0.576252, acc: 0.617188]  [Adversarial loss: 0.403501, acc: 0.843750]\n",
            "983: [Discriminator loss: 0.559202, acc: 0.597656]  [Adversarial loss: 0.327592, acc: 0.886719]\n",
            "984: [Discriminator loss: 0.644940, acc: 0.566406]  [Adversarial loss: 0.387929, acc: 0.871094]\n",
            "985: [Discriminator loss: 0.602634, acc: 0.582031]  [Adversarial loss: 0.395598, acc: 0.843750]\n",
            "986: [Discriminator loss: 0.658389, acc: 0.566406]  [Adversarial loss: 0.380384, acc: 0.851562]\n",
            "987: [Discriminator loss: 0.651286, acc: 0.585938]  [Adversarial loss: 0.386718, acc: 0.847656]\n",
            "988: [Discriminator loss: 0.551103, acc: 0.664062]  [Adversarial loss: 0.379283, acc: 0.835938]\n",
            "989: [Discriminator loss: 0.613754, acc: 0.613281]  [Adversarial loss: 0.449960, acc: 0.796875]\n",
            "990: [Discriminator loss: 0.672028, acc: 0.550781]  [Adversarial loss: 0.381725, acc: 0.863281]\n",
            "991: [Discriminator loss: 0.698059, acc: 0.515625]  [Adversarial loss: 0.422153, acc: 0.781250]\n",
            "992: [Discriminator loss: 0.593632, acc: 0.589844]  [Adversarial loss: 0.422110, acc: 0.835938]\n",
            "993: [Discriminator loss: 0.680622, acc: 0.578125]  [Adversarial loss: 0.418849, acc: 0.785156]\n",
            "994: [Discriminator loss: 0.597140, acc: 0.582031]  [Adversarial loss: 0.366630, acc: 0.851562]\n",
            "995: [Discriminator loss: 0.590762, acc: 0.597656]  [Adversarial loss: 0.357253, acc: 0.882812]\n",
            "996: [Discriminator loss: 0.627984, acc: 0.562500]  [Adversarial loss: 0.370204, acc: 0.832031]\n",
            "997: [Discriminator loss: 0.541056, acc: 0.601562]  [Adversarial loss: 0.381412, acc: 0.855469]\n",
            "998: [Discriminator loss: 0.617533, acc: 0.582031]  [Adversarial loss: 0.394750, acc: 0.863281]\n",
            "999: [Discriminator loss: 0.633600, acc: 0.550781]  [Adversarial loss: 0.385166, acc: 0.843750]\n",
            "1000: [Discriminator loss: 0.644513, acc: 0.574219]  [Adversarial loss: 0.407127, acc: 0.808594]\n",
            "1001: [Discriminator loss: 0.595232, acc: 0.605469]  [Adversarial loss: 0.408935, acc: 0.808594]\n",
            "1002: [Discriminator loss: 0.587260, acc: 0.589844]  [Adversarial loss: 0.373284, acc: 0.843750]\n",
            "1003: [Discriminator loss: 0.627312, acc: 0.574219]  [Adversarial loss: 0.398673, acc: 0.859375]\n",
            "1004: [Discriminator loss: 0.606205, acc: 0.582031]  [Adversarial loss: 0.398636, acc: 0.851562]\n",
            "1005: [Discriminator loss: 0.576286, acc: 0.617188]  [Adversarial loss: 0.368951, acc: 0.832031]\n",
            "1006: [Discriminator loss: 0.615440, acc: 0.601562]  [Adversarial loss: 0.376953, acc: 0.851562]\n",
            "1007: [Discriminator loss: 0.570377, acc: 0.628906]  [Adversarial loss: 0.380736, acc: 0.843750]\n",
            "1008: [Discriminator loss: 0.613667, acc: 0.558594]  [Adversarial loss: 0.441304, acc: 0.812500]\n",
            "1009: [Discriminator loss: 0.587314, acc: 0.578125]  [Adversarial loss: 0.355573, acc: 0.875000]\n",
            "1010: [Discriminator loss: 0.621732, acc: 0.570312]  [Adversarial loss: 0.378150, acc: 0.851562]\n",
            "1011: [Discriminator loss: 0.613855, acc: 0.582031]  [Adversarial loss: 0.397302, acc: 0.816406]\n",
            "1012: [Discriminator loss: 0.606616, acc: 0.566406]  [Adversarial loss: 0.358120, acc: 0.839844]\n",
            "1013: [Discriminator loss: 0.674880, acc: 0.535156]  [Adversarial loss: 0.441715, acc: 0.800781]\n",
            "1014: [Discriminator loss: 0.609705, acc: 0.578125]  [Adversarial loss: 0.412930, acc: 0.832031]\n",
            "1015: [Discriminator loss: 0.710982, acc: 0.531250]  [Adversarial loss: 0.461965, acc: 0.808594]\n",
            "1016: [Discriminator loss: 0.621447, acc: 0.566406]  [Adversarial loss: 0.393278, acc: 0.847656]\n",
            "1017: [Discriminator loss: 0.652868, acc: 0.515625]  [Adversarial loss: 0.430381, acc: 0.816406]\n",
            "1018: [Discriminator loss: 0.617730, acc: 0.597656]  [Adversarial loss: 0.371267, acc: 0.890625]\n",
            "1019: [Discriminator loss: 0.620449, acc: 0.625000]  [Adversarial loss: 0.384764, acc: 0.835938]\n",
            "1020: [Discriminator loss: 0.622686, acc: 0.589844]  [Adversarial loss: 0.407719, acc: 0.835938]\n",
            "1021: [Discriminator loss: 0.592097, acc: 0.578125]  [Adversarial loss: 0.428043, acc: 0.835938]\n",
            "1022: [Discriminator loss: 0.702092, acc: 0.546875]  [Adversarial loss: 0.427142, acc: 0.808594]\n",
            "1023: [Discriminator loss: 0.625314, acc: 0.589844]  [Adversarial loss: 0.415777, acc: 0.835938]\n",
            "1024: [Discriminator loss: 0.553411, acc: 0.613281]  [Adversarial loss: 0.438628, acc: 0.812500]\n",
            "1025: [Discriminator loss: 0.591352, acc: 0.605469]  [Adversarial loss: 0.422248, acc: 0.835938]\n",
            "1026: [Discriminator loss: 0.602907, acc: 0.609375]  [Adversarial loss: 0.419029, acc: 0.820312]\n",
            "1027: [Discriminator loss: 0.607942, acc: 0.585938]  [Adversarial loss: 0.402780, acc: 0.792969]\n",
            "1028: [Discriminator loss: 0.573058, acc: 0.566406]  [Adversarial loss: 0.432914, acc: 0.832031]\n",
            "1029: [Discriminator loss: 0.700038, acc: 0.511719]  [Adversarial loss: 0.431370, acc: 0.828125]\n",
            "1030: [Discriminator loss: 0.581850, acc: 0.617188]  [Adversarial loss: 0.417810, acc: 0.808594]\n",
            "1031: [Discriminator loss: 0.537161, acc: 0.636719]  [Adversarial loss: 0.487353, acc: 0.785156]\n",
            "1032: [Discriminator loss: 0.590052, acc: 0.574219]  [Adversarial loss: 0.444715, acc: 0.800781]\n",
            "1033: [Discriminator loss: 0.518654, acc: 0.632812]  [Adversarial loss: 0.363276, acc: 0.851562]\n",
            "1034: [Discriminator loss: 0.574911, acc: 0.617188]  [Adversarial loss: 0.385529, acc: 0.824219]\n",
            "1035: [Discriminator loss: 0.513056, acc: 0.648438]  [Adversarial loss: 0.440175, acc: 0.812500]\n",
            "1036: [Discriminator loss: 0.576452, acc: 0.605469]  [Adversarial loss: 0.469294, acc: 0.777344]\n",
            "1037: [Discriminator loss: 0.665034, acc: 0.578125]  [Adversarial loss: 0.443255, acc: 0.820312]\n",
            "1038: [Discriminator loss: 0.615560, acc: 0.582031]  [Adversarial loss: 0.358710, acc: 0.894531]\n",
            "1039: [Discriminator loss: 0.605718, acc: 0.562500]  [Adversarial loss: 0.374339, acc: 0.843750]\n",
            "1040: [Discriminator loss: 0.637289, acc: 0.585938]  [Adversarial loss: 0.429235, acc: 0.820312]\n",
            "1041: [Discriminator loss: 0.706052, acc: 0.546875]  [Adversarial loss: 0.418202, acc: 0.820312]\n",
            "1042: [Discriminator loss: 0.581080, acc: 0.609375]  [Adversarial loss: 0.363715, acc: 0.843750]\n",
            "1043: [Discriminator loss: 0.573358, acc: 0.628906]  [Adversarial loss: 0.438491, acc: 0.820312]\n",
            "1044: [Discriminator loss: 0.623329, acc: 0.617188]  [Adversarial loss: 0.390186, acc: 0.832031]\n",
            "1045: [Discriminator loss: 0.644113, acc: 0.585938]  [Adversarial loss: 0.439511, acc: 0.820312]\n",
            "1046: [Discriminator loss: 0.605586, acc: 0.597656]  [Adversarial loss: 0.418097, acc: 0.812500]\n",
            "1047: [Discriminator loss: 0.642403, acc: 0.566406]  [Adversarial loss: 0.423794, acc: 0.812500]\n",
            "1048: [Discriminator loss: 0.572809, acc: 0.554688]  [Adversarial loss: 0.411120, acc: 0.835938]\n",
            "1049: [Discriminator loss: 0.540419, acc: 0.617188]  [Adversarial loss: 0.392402, acc: 0.824219]\n",
            "1050: [Discriminator loss: 0.564254, acc: 0.621094]  [Adversarial loss: 0.444871, acc: 0.832031]\n",
            "1051: [Discriminator loss: 0.624812, acc: 0.570312]  [Adversarial loss: 0.402479, acc: 0.839844]\n",
            "1052: [Discriminator loss: 0.675968, acc: 0.546875]  [Adversarial loss: 0.443447, acc: 0.800781]\n",
            "1053: [Discriminator loss: 0.543521, acc: 0.597656]  [Adversarial loss: 0.394723, acc: 0.843750]\n",
            "1054: [Discriminator loss: 0.558125, acc: 0.628906]  [Adversarial loss: 0.387686, acc: 0.867188]\n",
            "1055: [Discriminator loss: 0.615324, acc: 0.566406]  [Adversarial loss: 0.439642, acc: 0.804688]\n",
            "1056: [Discriminator loss: 0.634107, acc: 0.566406]  [Adversarial loss: 0.437346, acc: 0.800781]\n",
            "1057: [Discriminator loss: 0.605519, acc: 0.515625]  [Adversarial loss: 0.426095, acc: 0.789062]\n",
            "1058: [Discriminator loss: 0.630201, acc: 0.570312]  [Adversarial loss: 0.430111, acc: 0.820312]\n",
            "1059: [Discriminator loss: 0.567729, acc: 0.589844]  [Adversarial loss: 0.387776, acc: 0.835938]\n",
            "1060: [Discriminator loss: 0.639475, acc: 0.593750]  [Adversarial loss: 0.410065, acc: 0.792969]\n",
            "1061: [Discriminator loss: 0.605728, acc: 0.589844]  [Adversarial loss: 0.421064, acc: 0.785156]\n",
            "1062: [Discriminator loss: 0.580128, acc: 0.605469]  [Adversarial loss: 0.413735, acc: 0.828125]\n",
            "1063: [Discriminator loss: 0.549581, acc: 0.621094]  [Adversarial loss: 0.338075, acc: 0.867188]\n",
            "1064: [Discriminator loss: 0.676106, acc: 0.570312]  [Adversarial loss: 0.442839, acc: 0.800781]\n",
            "1065: [Discriminator loss: 0.584705, acc: 0.609375]  [Adversarial loss: 0.405336, acc: 0.843750]\n",
            "1066: [Discriminator loss: 0.603007, acc: 0.601562]  [Adversarial loss: 0.364049, acc: 0.875000]\n",
            "1067: [Discriminator loss: 0.668982, acc: 0.554688]  [Adversarial loss: 0.400910, acc: 0.824219]\n",
            "1068: [Discriminator loss: 0.568728, acc: 0.570312]  [Adversarial loss: 0.367898, acc: 0.875000]\n",
            "1069: [Discriminator loss: 0.692565, acc: 0.535156]  [Adversarial loss: 0.375763, acc: 0.867188]\n",
            "1070: [Discriminator loss: 0.643798, acc: 0.597656]  [Adversarial loss: 0.349430, acc: 0.847656]\n",
            "1071: [Discriminator loss: 0.635617, acc: 0.605469]  [Adversarial loss: 0.357198, acc: 0.882812]\n",
            "1072: [Discriminator loss: 0.699156, acc: 0.554688]  [Adversarial loss: 0.439617, acc: 0.828125]\n",
            "1073: [Discriminator loss: 0.612072, acc: 0.585938]  [Adversarial loss: 0.327208, acc: 0.863281]\n",
            "1074: [Discriminator loss: 0.555267, acc: 0.601562]  [Adversarial loss: 0.390636, acc: 0.828125]\n",
            "1075: [Discriminator loss: 0.641600, acc: 0.574219]  [Adversarial loss: 0.409614, acc: 0.804688]\n",
            "1076: [Discriminator loss: 0.618610, acc: 0.566406]  [Adversarial loss: 0.372525, acc: 0.863281]\n",
            "1077: [Discriminator loss: 0.680109, acc: 0.523438]  [Adversarial loss: 0.417777, acc: 0.828125]\n",
            "1078: [Discriminator loss: 0.613688, acc: 0.589844]  [Adversarial loss: 0.417243, acc: 0.796875]\n",
            "1079: [Discriminator loss: 0.575335, acc: 0.570312]  [Adversarial loss: 0.347929, acc: 0.843750]\n",
            "1080: [Discriminator loss: 0.621217, acc: 0.574219]  [Adversarial loss: 0.378171, acc: 0.839844]\n",
            "1081: [Discriminator loss: 0.702842, acc: 0.558594]  [Adversarial loss: 0.365488, acc: 0.839844]\n",
            "1082: [Discriminator loss: 0.589335, acc: 0.621094]  [Adversarial loss: 0.407451, acc: 0.824219]\n",
            "1083: [Discriminator loss: 0.662592, acc: 0.550781]  [Adversarial loss: 0.387699, acc: 0.839844]\n",
            "1084: [Discriminator loss: 0.635866, acc: 0.574219]  [Adversarial loss: 0.429423, acc: 0.847656]\n",
            "1085: [Discriminator loss: 0.621119, acc: 0.597656]  [Adversarial loss: 0.400670, acc: 0.820312]\n",
            "1086: [Discriminator loss: 0.621981, acc: 0.562500]  [Adversarial loss: 0.397397, acc: 0.847656]\n",
            "1087: [Discriminator loss: 0.648064, acc: 0.574219]  [Adversarial loss: 0.374044, acc: 0.828125]\n",
            "1088: [Discriminator loss: 0.545202, acc: 0.582031]  [Adversarial loss: 0.386849, acc: 0.851562]\n",
            "1089: [Discriminator loss: 0.617594, acc: 0.507812]  [Adversarial loss: 0.372636, acc: 0.847656]\n",
            "1090: [Discriminator loss: 0.566538, acc: 0.617188]  [Adversarial loss: 0.401221, acc: 0.789062]\n",
            "1091: [Discriminator loss: 0.583149, acc: 0.582031]  [Adversarial loss: 0.389966, acc: 0.851562]\n",
            "1092: [Discriminator loss: 0.594921, acc: 0.597656]  [Adversarial loss: 0.397979, acc: 0.816406]\n",
            "1093: [Discriminator loss: 0.639985, acc: 0.554688]  [Adversarial loss: 0.379286, acc: 0.871094]\n",
            "1094: [Discriminator loss: 0.575975, acc: 0.597656]  [Adversarial loss: 0.366309, acc: 0.867188]\n",
            "1095: [Discriminator loss: 0.564755, acc: 0.593750]  [Adversarial loss: 0.348817, acc: 0.882812]\n",
            "1096: [Discriminator loss: 0.561542, acc: 0.613281]  [Adversarial loss: 0.385693, acc: 0.835938]\n",
            "1097: [Discriminator loss: 0.584491, acc: 0.589844]  [Adversarial loss: 0.382854, acc: 0.828125]\n",
            "1098: [Discriminator loss: 0.609023, acc: 0.589844]  [Adversarial loss: 0.385065, acc: 0.828125]\n",
            "1099: [Discriminator loss: 0.615312, acc: 0.511719]  [Adversarial loss: 0.379519, acc: 0.824219]\n",
            "1100: [Discriminator loss: 0.588014, acc: 0.562500]  [Adversarial loss: 0.381785, acc: 0.863281]\n",
            "1101: [Discriminator loss: 0.551134, acc: 0.632812]  [Adversarial loss: 0.396959, acc: 0.820312]\n",
            "1102: [Discriminator loss: 0.653275, acc: 0.570312]  [Adversarial loss: 0.374778, acc: 0.859375]\n",
            "1103: [Discriminator loss: 0.646117, acc: 0.570312]  [Adversarial loss: 0.394499, acc: 0.851562]\n",
            "1104: [Discriminator loss: 0.623700, acc: 0.566406]  [Adversarial loss: 0.394887, acc: 0.812500]\n",
            "1105: [Discriminator loss: 0.629013, acc: 0.578125]  [Adversarial loss: 0.386131, acc: 0.855469]\n",
            "1106: [Discriminator loss: 0.665634, acc: 0.535156]  [Adversarial loss: 0.398870, acc: 0.812500]\n",
            "1107: [Discriminator loss: 0.561127, acc: 0.621094]  [Adversarial loss: 0.422285, acc: 0.835938]\n",
            "1108: [Discriminator loss: 0.543124, acc: 0.644531]  [Adversarial loss: 0.367586, acc: 0.859375]\n",
            "1109: [Discriminator loss: 0.629380, acc: 0.582031]  [Adversarial loss: 0.379381, acc: 0.824219]\n",
            "1110: [Discriminator loss: 0.615390, acc: 0.609375]  [Adversarial loss: 0.365484, acc: 0.855469]\n",
            "1111: [Discriminator loss: 0.657035, acc: 0.597656]  [Adversarial loss: 0.398071, acc: 0.824219]\n",
            "1112: [Discriminator loss: 0.601701, acc: 0.613281]  [Adversarial loss: 0.376118, acc: 0.828125]\n",
            "1113: [Discriminator loss: 0.631264, acc: 0.582031]  [Adversarial loss: 0.357528, acc: 0.847656]\n",
            "1114: [Discriminator loss: 0.591783, acc: 0.597656]  [Adversarial loss: 0.399833, acc: 0.816406]\n",
            "1115: [Discriminator loss: 0.636816, acc: 0.554688]  [Adversarial loss: 0.427986, acc: 0.800781]\n",
            "1116: [Discriminator loss: 0.653233, acc: 0.578125]  [Adversarial loss: 0.437893, acc: 0.812500]\n",
            "1117: [Discriminator loss: 0.627356, acc: 0.574219]  [Adversarial loss: 0.410411, acc: 0.832031]\n",
            "1118: [Discriminator loss: 0.568551, acc: 0.589844]  [Adversarial loss: 0.436147, acc: 0.828125]\n",
            "1119: [Discriminator loss: 0.615289, acc: 0.542969]  [Adversarial loss: 0.397683, acc: 0.816406]\n",
            "1120: [Discriminator loss: 0.629614, acc: 0.585938]  [Adversarial loss: 0.405815, acc: 0.843750]\n",
            "1121: [Discriminator loss: 0.663357, acc: 0.566406]  [Adversarial loss: 0.424780, acc: 0.832031]\n",
            "1122: [Discriminator loss: 0.629184, acc: 0.585938]  [Adversarial loss: 0.409148, acc: 0.863281]\n",
            "1123: [Discriminator loss: 0.643203, acc: 0.582031]  [Adversarial loss: 0.406618, acc: 0.859375]\n",
            "1124: [Discriminator loss: 0.566517, acc: 0.644531]  [Adversarial loss: 0.383691, acc: 0.824219]\n",
            "1125: [Discriminator loss: 0.589976, acc: 0.593750]  [Adversarial loss: 0.386590, acc: 0.839844]\n",
            "1126: [Discriminator loss: 0.619759, acc: 0.593750]  [Adversarial loss: 0.385790, acc: 0.851562]\n",
            "1127: [Discriminator loss: 0.695163, acc: 0.542969]  [Adversarial loss: 0.426084, acc: 0.812500]\n",
            "1128: [Discriminator loss: 0.600531, acc: 0.609375]  [Adversarial loss: 0.440646, acc: 0.789062]\n",
            "1129: [Discriminator loss: 0.562055, acc: 0.605469]  [Adversarial loss: 0.428566, acc: 0.843750]\n",
            "1130: [Discriminator loss: 0.620655, acc: 0.593750]  [Adversarial loss: 0.380709, acc: 0.828125]\n",
            "1131: [Discriminator loss: 0.651385, acc: 0.570312]  [Adversarial loss: 0.412361, acc: 0.808594]\n",
            "1132: [Discriminator loss: 0.655855, acc: 0.527344]  [Adversarial loss: 0.405399, acc: 0.832031]\n",
            "1133: [Discriminator loss: 0.572215, acc: 0.640625]  [Adversarial loss: 0.368855, acc: 0.824219]\n",
            "1134: [Discriminator loss: 0.637819, acc: 0.578125]  [Adversarial loss: 0.371415, acc: 0.855469]\n",
            "1135: [Discriminator loss: 0.591062, acc: 0.640625]  [Adversarial loss: 0.395802, acc: 0.824219]\n",
            "1136: [Discriminator loss: 0.606846, acc: 0.601562]  [Adversarial loss: 0.370899, acc: 0.828125]\n",
            "1137: [Discriminator loss: 0.591396, acc: 0.585938]  [Adversarial loss: 0.415742, acc: 0.804688]\n",
            "1138: [Discriminator loss: 0.563253, acc: 0.617188]  [Adversarial loss: 0.318484, acc: 0.878906]\n",
            "1139: [Discriminator loss: 0.634663, acc: 0.570312]  [Adversarial loss: 0.396310, acc: 0.855469]\n",
            "1140: [Discriminator loss: 0.638374, acc: 0.562500]  [Adversarial loss: 0.386236, acc: 0.832031]\n",
            "1141: [Discriminator loss: 0.654396, acc: 0.578125]  [Adversarial loss: 0.401689, acc: 0.859375]\n",
            "1142: [Discriminator loss: 0.692159, acc: 0.574219]  [Adversarial loss: 0.446452, acc: 0.792969]\n",
            "1143: [Discriminator loss: 0.701111, acc: 0.519531]  [Adversarial loss: 0.421678, acc: 0.828125]\n",
            "1144: [Discriminator loss: 0.603396, acc: 0.570312]  [Adversarial loss: 0.388665, acc: 0.820312]\n",
            "1145: [Discriminator loss: 0.570892, acc: 0.605469]  [Adversarial loss: 0.403107, acc: 0.867188]\n",
            "1146: [Discriminator loss: 0.640247, acc: 0.546875]  [Adversarial loss: 0.381439, acc: 0.859375]\n",
            "1147: [Discriminator loss: 0.544070, acc: 0.636719]  [Adversarial loss: 0.411670, acc: 0.796875]\n",
            "1148: [Discriminator loss: 0.581411, acc: 0.597656]  [Adversarial loss: 0.399155, acc: 0.847656]\n",
            "1149: [Discriminator loss: 0.591582, acc: 0.601562]  [Adversarial loss: 0.372530, acc: 0.843750]\n",
            "1150: [Discriminator loss: 0.507360, acc: 0.636719]  [Adversarial loss: 0.354798, acc: 0.824219]\n",
            "1151: [Discriminator loss: 0.600281, acc: 0.585938]  [Adversarial loss: 0.413500, acc: 0.824219]\n",
            "1152: [Discriminator loss: 0.643484, acc: 0.546875]  [Adversarial loss: 0.393947, acc: 0.839844]\n",
            "1153: [Discriminator loss: 0.588684, acc: 0.617188]  [Adversarial loss: 0.401985, acc: 0.824219]\n",
            "1154: [Discriminator loss: 0.591825, acc: 0.578125]  [Adversarial loss: 0.373111, acc: 0.820312]\n",
            "1155: [Discriminator loss: 0.640712, acc: 0.578125]  [Adversarial loss: 0.405820, acc: 0.816406]\n",
            "1156: [Discriminator loss: 0.602701, acc: 0.625000]  [Adversarial loss: 0.404640, acc: 0.820312]\n",
            "1157: [Discriminator loss: 0.639452, acc: 0.527344]  [Adversarial loss: 0.428276, acc: 0.808594]\n",
            "1158: [Discriminator loss: 0.631622, acc: 0.570312]  [Adversarial loss: 0.409957, acc: 0.843750]\n",
            "1159: [Discriminator loss: 0.599978, acc: 0.585938]  [Adversarial loss: 0.380002, acc: 0.847656]\n",
            "1160: [Discriminator loss: 0.607188, acc: 0.601562]  [Adversarial loss: 0.377219, acc: 0.828125]\n",
            "1161: [Discriminator loss: 0.567819, acc: 0.648438]  [Adversarial loss: 0.326644, acc: 0.851562]\n",
            "1162: [Discriminator loss: 0.596170, acc: 0.578125]  [Adversarial loss: 0.387428, acc: 0.808594]\n",
            "1163: [Discriminator loss: 0.700800, acc: 0.566406]  [Adversarial loss: 0.376715, acc: 0.843750]\n",
            "1164: [Discriminator loss: 0.770410, acc: 0.519531]  [Adversarial loss: 0.409167, acc: 0.832031]\n",
            "1165: [Discriminator loss: 0.621992, acc: 0.601562]  [Adversarial loss: 0.388625, acc: 0.855469]\n",
            "1166: [Discriminator loss: 0.666928, acc: 0.589844]  [Adversarial loss: 0.427709, acc: 0.808594]\n",
            "1167: [Discriminator loss: 0.594200, acc: 0.632812]  [Adversarial loss: 0.368589, acc: 0.832031]\n",
            "1168: [Discriminator loss: 0.628230, acc: 0.582031]  [Adversarial loss: 0.376442, acc: 0.828125]\n",
            "1169: [Discriminator loss: 0.538682, acc: 0.652344]  [Adversarial loss: 0.375418, acc: 0.859375]\n",
            "1170: [Discriminator loss: 0.587002, acc: 0.621094]  [Adversarial loss: 0.356873, acc: 0.859375]\n",
            "1171: [Discriminator loss: 0.570613, acc: 0.597656]  [Adversarial loss: 0.348378, acc: 0.820312]\n",
            "1172: [Discriminator loss: 0.572673, acc: 0.593750]  [Adversarial loss: 0.340056, acc: 0.835938]\n",
            "1173: [Discriminator loss: 0.578261, acc: 0.574219]  [Adversarial loss: 0.333937, acc: 0.878906]\n",
            "1174: [Discriminator loss: 0.513585, acc: 0.648438]  [Adversarial loss: 0.377153, acc: 0.835938]\n",
            "1175: [Discriminator loss: 0.667964, acc: 0.546875]  [Adversarial loss: 0.361773, acc: 0.859375]\n",
            "1176: [Discriminator loss: 0.628881, acc: 0.582031]  [Adversarial loss: 0.400485, acc: 0.863281]\n",
            "1177: [Discriminator loss: 0.632199, acc: 0.585938]  [Adversarial loss: 0.382275, acc: 0.847656]\n",
            "1178: [Discriminator loss: 0.632349, acc: 0.558594]  [Adversarial loss: 0.418580, acc: 0.832031]\n",
            "1179: [Discriminator loss: 0.586289, acc: 0.601562]  [Adversarial loss: 0.398084, acc: 0.878906]\n",
            "1180: [Discriminator loss: 0.577481, acc: 0.640625]  [Adversarial loss: 0.408181, acc: 0.847656]\n",
            "1181: [Discriminator loss: 0.604368, acc: 0.574219]  [Adversarial loss: 0.399686, acc: 0.828125]\n",
            "1182: [Discriminator loss: 0.600899, acc: 0.597656]  [Adversarial loss: 0.408025, acc: 0.828125]\n",
            "1183: [Discriminator loss: 0.604172, acc: 0.597656]  [Adversarial loss: 0.395206, acc: 0.808594]\n",
            "1184: [Discriminator loss: 0.649791, acc: 0.558594]  [Adversarial loss: 0.469123, acc: 0.757812]\n",
            "1185: [Discriminator loss: 0.573895, acc: 0.648438]  [Adversarial loss: 0.395281, acc: 0.820312]\n",
            "1186: [Discriminator loss: 0.573747, acc: 0.605469]  [Adversarial loss: 0.355489, acc: 0.855469]\n",
            "1187: [Discriminator loss: 0.605657, acc: 0.574219]  [Adversarial loss: 0.416316, acc: 0.847656]\n",
            "1188: [Discriminator loss: 0.588224, acc: 0.621094]  [Adversarial loss: 0.323842, acc: 0.835938]\n",
            "1189: [Discriminator loss: 0.617511, acc: 0.605469]  [Adversarial loss: 0.381977, acc: 0.832031]\n",
            "1190: [Discriminator loss: 0.609331, acc: 0.578125]  [Adversarial loss: 0.405937, acc: 0.839844]\n",
            "1191: [Discriminator loss: 0.595806, acc: 0.546875]  [Adversarial loss: 0.383736, acc: 0.832031]\n",
            "1192: [Discriminator loss: 0.593938, acc: 0.589844]  [Adversarial loss: 0.361669, acc: 0.835938]\n",
            "1193: [Discriminator loss: 0.648806, acc: 0.535156]  [Adversarial loss: 0.362164, acc: 0.863281]\n",
            "1194: [Discriminator loss: 0.631554, acc: 0.585938]  [Adversarial loss: 0.405483, acc: 0.808594]\n",
            "1195: [Discriminator loss: 0.671046, acc: 0.578125]  [Adversarial loss: 0.398311, acc: 0.800781]\n",
            "1196: [Discriminator loss: 0.614513, acc: 0.562500]  [Adversarial loss: 0.388788, acc: 0.824219]\n",
            "1197: [Discriminator loss: 0.637733, acc: 0.593750]  [Adversarial loss: 0.417202, acc: 0.847656]\n",
            "1198: [Discriminator loss: 0.606460, acc: 0.593750]  [Adversarial loss: 0.419090, acc: 0.824219]\n",
            "1199: [Discriminator loss: 0.616848, acc: 0.550781]  [Adversarial loss: 0.464178, acc: 0.816406]\n",
            "1200: [Discriminator loss: 0.618855, acc: 0.542969]  [Adversarial loss: 0.373901, acc: 0.839844]\n",
            "1201: [Discriminator loss: 0.601194, acc: 0.593750]  [Adversarial loss: 0.348580, acc: 0.882812]\n",
            "1202: [Discriminator loss: 0.603256, acc: 0.585938]  [Adversarial loss: 0.388685, acc: 0.808594]\n",
            "1203: [Discriminator loss: 0.610720, acc: 0.570312]  [Adversarial loss: 0.421188, acc: 0.773438]\n",
            "1204: [Discriminator loss: 0.732874, acc: 0.496094]  [Adversarial loss: 0.406938, acc: 0.828125]\n",
            "1205: [Discriminator loss: 0.713786, acc: 0.562500]  [Adversarial loss: 0.375071, acc: 0.847656]\n",
            "1206: [Discriminator loss: 0.570353, acc: 0.621094]  [Adversarial loss: 0.385272, acc: 0.851562]\n",
            "1207: [Discriminator loss: 0.625991, acc: 0.546875]  [Adversarial loss: 0.484583, acc: 0.804688]\n",
            "1208: [Discriminator loss: 0.602073, acc: 0.589844]  [Adversarial loss: 0.360523, acc: 0.855469]\n",
            "1209: [Discriminator loss: 0.623907, acc: 0.582031]  [Adversarial loss: 0.400731, acc: 0.824219]\n",
            "1210: [Discriminator loss: 0.646983, acc: 0.562500]  [Adversarial loss: 0.424426, acc: 0.808594]\n",
            "1211: [Discriminator loss: 0.695686, acc: 0.531250]  [Adversarial loss: 0.430955, acc: 0.804688]\n",
            "1212: [Discriminator loss: 0.551989, acc: 0.640625]  [Adversarial loss: 0.375409, acc: 0.882812]\n",
            "1213: [Discriminator loss: 0.522501, acc: 0.652344]  [Adversarial loss: 0.369266, acc: 0.843750]\n",
            "1214: [Discriminator loss: 0.607805, acc: 0.550781]  [Adversarial loss: 0.375983, acc: 0.839844]\n",
            "1215: [Discriminator loss: 0.576407, acc: 0.605469]  [Adversarial loss: 0.375155, acc: 0.835938]\n",
            "1216: [Discriminator loss: 0.612988, acc: 0.582031]  [Adversarial loss: 0.356022, acc: 0.851562]\n",
            "1217: [Discriminator loss: 0.600895, acc: 0.621094]  [Adversarial loss: 0.370401, acc: 0.839844]\n",
            "1218: [Discriminator loss: 0.587706, acc: 0.640625]  [Adversarial loss: 0.357477, acc: 0.863281]\n",
            "1219: [Discriminator loss: 0.652705, acc: 0.542969]  [Adversarial loss: 0.390764, acc: 0.859375]\n",
            "1220: [Discriminator loss: 0.634352, acc: 0.566406]  [Adversarial loss: 0.336010, acc: 0.875000]\n",
            "1221: [Discriminator loss: 0.659454, acc: 0.558594]  [Adversarial loss: 0.396277, acc: 0.824219]\n",
            "1222: [Discriminator loss: 0.620202, acc: 0.605469]  [Adversarial loss: 0.345017, acc: 0.859375]\n",
            "1223: [Discriminator loss: 0.608963, acc: 0.597656]  [Adversarial loss: 0.354056, acc: 0.812500]\n",
            "1224: [Discriminator loss: 0.700498, acc: 0.597656]  [Adversarial loss: 0.374576, acc: 0.820312]\n",
            "1225: [Discriminator loss: 0.594693, acc: 0.578125]  [Adversarial loss: 0.392552, acc: 0.824219]\n",
            "1226: [Discriminator loss: 0.615716, acc: 0.566406]  [Adversarial loss: 0.399924, acc: 0.808594]\n",
            "1227: [Discriminator loss: 0.566397, acc: 0.628906]  [Adversarial loss: 0.329354, acc: 0.902344]\n",
            "1228: [Discriminator loss: 0.621864, acc: 0.558594]  [Adversarial loss: 0.368623, acc: 0.800781]\n",
            "1229: [Discriminator loss: 0.661630, acc: 0.589844]  [Adversarial loss: 0.383456, acc: 0.851562]\n",
            "1230: [Discriminator loss: 0.604634, acc: 0.578125]  [Adversarial loss: 0.349447, acc: 0.851562]\n",
            "1231: [Discriminator loss: 0.675785, acc: 0.527344]  [Adversarial loss: 0.375766, acc: 0.808594]\n",
            "1232: [Discriminator loss: 0.600999, acc: 0.617188]  [Adversarial loss: 0.397607, acc: 0.820312]\n",
            "1233: [Discriminator loss: 0.572293, acc: 0.601562]  [Adversarial loss: 0.407258, acc: 0.832031]\n",
            "1234: [Discriminator loss: 0.573631, acc: 0.636719]  [Adversarial loss: 0.342237, acc: 0.882812]\n",
            "1235: [Discriminator loss: 0.637819, acc: 0.566406]  [Adversarial loss: 0.377513, acc: 0.828125]\n",
            "1236: [Discriminator loss: 0.513437, acc: 0.648438]  [Adversarial loss: 0.414731, acc: 0.800781]\n",
            "1237: [Discriminator loss: 0.607354, acc: 0.539062]  [Adversarial loss: 0.419074, acc: 0.839844]\n",
            "1238: [Discriminator loss: 0.560705, acc: 0.601562]  [Adversarial loss: 0.387698, acc: 0.824219]\n",
            "1239: [Discriminator loss: 0.639582, acc: 0.601562]  [Adversarial loss: 0.440269, acc: 0.800781]\n",
            "1240: [Discriminator loss: 0.550335, acc: 0.605469]  [Adversarial loss: 0.378514, acc: 0.835938]\n",
            "1241: [Discriminator loss: 0.599596, acc: 0.597656]  [Adversarial loss: 0.421830, acc: 0.824219]\n",
            "1242: [Discriminator loss: 0.610464, acc: 0.566406]  [Adversarial loss: 0.396569, acc: 0.777344]\n",
            "1243: [Discriminator loss: 0.608315, acc: 0.593750]  [Adversarial loss: 0.397919, acc: 0.835938]\n",
            "1244: [Discriminator loss: 0.658560, acc: 0.550781]  [Adversarial loss: 0.400182, acc: 0.804688]\n",
            "1245: [Discriminator loss: 0.614525, acc: 0.593750]  [Adversarial loss: 0.418570, acc: 0.789062]\n",
            "1246: [Discriminator loss: 0.529289, acc: 0.660156]  [Adversarial loss: 0.362859, acc: 0.828125]\n",
            "1247: [Discriminator loss: 0.580156, acc: 0.609375]  [Adversarial loss: 0.499454, acc: 0.773438]\n",
            "1248: [Discriminator loss: 0.568029, acc: 0.605469]  [Adversarial loss: 0.375812, acc: 0.820312]\n",
            "1249: [Discriminator loss: 0.549443, acc: 0.640625]  [Adversarial loss: 0.366544, acc: 0.820312]\n",
            "1250: [Discriminator loss: 0.632361, acc: 0.546875]  [Adversarial loss: 0.399252, acc: 0.839844]\n",
            "1251: [Discriminator loss: 0.586117, acc: 0.589844]  [Adversarial loss: 0.456275, acc: 0.808594]\n",
            "1252: [Discriminator loss: 0.607658, acc: 0.589844]  [Adversarial loss: 0.416377, acc: 0.789062]\n",
            "1253: [Discriminator loss: 0.553726, acc: 0.613281]  [Adversarial loss: 0.398245, acc: 0.812500]\n",
            "1254: [Discriminator loss: 0.631275, acc: 0.546875]  [Adversarial loss: 0.419418, acc: 0.792969]\n",
            "1255: [Discriminator loss: 0.595307, acc: 0.562500]  [Adversarial loss: 0.446593, acc: 0.765625]\n",
            "1256: [Discriminator loss: 0.649730, acc: 0.527344]  [Adversarial loss: 0.445499, acc: 0.835938]\n",
            "1257: [Discriminator loss: 0.633593, acc: 0.585938]  [Adversarial loss: 0.402580, acc: 0.832031]\n",
            "1258: [Discriminator loss: 0.586054, acc: 0.574219]  [Adversarial loss: 0.413450, acc: 0.824219]\n",
            "1259: [Discriminator loss: 0.596553, acc: 0.617188]  [Adversarial loss: 0.398269, acc: 0.808594]\n",
            "1260: [Discriminator loss: 0.650644, acc: 0.558594]  [Adversarial loss: 0.429927, acc: 0.800781]\n",
            "1261: [Discriminator loss: 0.654262, acc: 0.558594]  [Adversarial loss: 0.421666, acc: 0.812500]\n",
            "1262: [Discriminator loss: 0.580486, acc: 0.652344]  [Adversarial loss: 0.423395, acc: 0.796875]\n",
            "1263: [Discriminator loss: 0.578722, acc: 0.562500]  [Adversarial loss: 0.453669, acc: 0.800781]\n",
            "1264: [Discriminator loss: 0.680759, acc: 0.542969]  [Adversarial loss: 0.483320, acc: 0.789062]\n",
            "1265: [Discriminator loss: 0.551209, acc: 0.671875]  [Adversarial loss: 0.457016, acc: 0.777344]\n",
            "1266: [Discriminator loss: 0.647218, acc: 0.570312]  [Adversarial loss: 0.451924, acc: 0.792969]\n",
            "1267: [Discriminator loss: 0.637842, acc: 0.542969]  [Adversarial loss: 0.492930, acc: 0.769531]\n",
            "1268: [Discriminator loss: 0.602931, acc: 0.617188]  [Adversarial loss: 0.504336, acc: 0.777344]\n",
            "1269: [Discriminator loss: 0.558475, acc: 0.609375]  [Adversarial loss: 0.437364, acc: 0.839844]\n",
            "1270: [Discriminator loss: 0.631371, acc: 0.562500]  [Adversarial loss: 0.403677, acc: 0.832031]\n",
            "1271: [Discriminator loss: 0.636886, acc: 0.562500]  [Adversarial loss: 0.450870, acc: 0.800781]\n",
            "1272: [Discriminator loss: 0.605687, acc: 0.523438]  [Adversarial loss: 0.440173, acc: 0.808594]\n",
            "1273: [Discriminator loss: 0.637443, acc: 0.566406]  [Adversarial loss: 0.423879, acc: 0.816406]\n",
            "1274: [Discriminator loss: 0.614277, acc: 0.554688]  [Adversarial loss: 0.445408, acc: 0.832031]\n",
            "1275: [Discriminator loss: 0.554689, acc: 0.632812]  [Adversarial loss: 0.407716, acc: 0.832031]\n",
            "1276: [Discriminator loss: 0.547938, acc: 0.640625]  [Adversarial loss: 0.432664, acc: 0.816406]\n",
            "1277: [Discriminator loss: 0.560701, acc: 0.648438]  [Adversarial loss: 0.413166, acc: 0.839844]\n",
            "1278: [Discriminator loss: 0.664628, acc: 0.562500]  [Adversarial loss: 0.412397, acc: 0.828125]\n",
            "1279: [Discriminator loss: 0.637784, acc: 0.574219]  [Adversarial loss: 0.414170, acc: 0.824219]\n",
            "1280: [Discriminator loss: 0.614478, acc: 0.570312]  [Adversarial loss: 0.440565, acc: 0.812500]\n",
            "1281: [Discriminator loss: 0.669489, acc: 0.546875]  [Adversarial loss: 0.437768, acc: 0.816406]\n",
            "1282: [Discriminator loss: 0.615043, acc: 0.578125]  [Adversarial loss: 0.405552, acc: 0.824219]\n",
            "1283: [Discriminator loss: 0.616838, acc: 0.589844]  [Adversarial loss: 0.450042, acc: 0.812500]\n",
            "1284: [Discriminator loss: 0.658849, acc: 0.562500]  [Adversarial loss: 0.425203, acc: 0.812500]\n",
            "1285: [Discriminator loss: 0.660029, acc: 0.558594]  [Adversarial loss: 0.429793, acc: 0.800781]\n",
            "1286: [Discriminator loss: 0.639493, acc: 0.601562]  [Adversarial loss: 0.408992, acc: 0.816406]\n",
            "1287: [Discriminator loss: 0.598578, acc: 0.570312]  [Adversarial loss: 0.444232, acc: 0.792969]\n",
            "1288: [Discriminator loss: 0.667405, acc: 0.574219]  [Adversarial loss: 0.457983, acc: 0.804688]\n",
            "1289: [Discriminator loss: 0.623411, acc: 0.578125]  [Adversarial loss: 0.396028, acc: 0.835938]\n",
            "1290: [Discriminator loss: 0.608284, acc: 0.613281]  [Adversarial loss: 0.431451, acc: 0.816406]\n",
            "1291: [Discriminator loss: 0.614060, acc: 0.609375]  [Adversarial loss: 0.419815, acc: 0.835938]\n",
            "1292: [Discriminator loss: 0.635627, acc: 0.566406]  [Adversarial loss: 0.405113, acc: 0.851562]\n",
            "1293: [Discriminator loss: 0.641488, acc: 0.625000]  [Adversarial loss: 0.397612, acc: 0.859375]\n",
            "1294: [Discriminator loss: 0.516984, acc: 0.664062]  [Adversarial loss: 0.390483, acc: 0.824219]\n",
            "1295: [Discriminator loss: 0.641471, acc: 0.597656]  [Adversarial loss: 0.421405, acc: 0.785156]\n",
            "1296: [Discriminator loss: 0.633958, acc: 0.574219]  [Adversarial loss: 0.476093, acc: 0.785156]\n",
            "1297: [Discriminator loss: 0.608238, acc: 0.625000]  [Adversarial loss: 0.403854, acc: 0.800781]\n",
            "1298: [Discriminator loss: 0.606740, acc: 0.589844]  [Adversarial loss: 0.405486, acc: 0.800781]\n",
            "1299: [Discriminator loss: 0.587126, acc: 0.589844]  [Adversarial loss: 0.461697, acc: 0.757812]\n",
            "1300: [Discriminator loss: 0.616790, acc: 0.589844]  [Adversarial loss: 0.458964, acc: 0.777344]\n",
            "1301: [Discriminator loss: 0.548668, acc: 0.625000]  [Adversarial loss: 0.385936, acc: 0.808594]\n",
            "1302: [Discriminator loss: 0.615936, acc: 0.625000]  [Adversarial loss: 0.450033, acc: 0.777344]\n",
            "1303: [Discriminator loss: 0.614874, acc: 0.582031]  [Adversarial loss: 0.452317, acc: 0.792969]\n",
            "1304: [Discriminator loss: 0.629199, acc: 0.582031]  [Adversarial loss: 0.427117, acc: 0.816406]\n",
            "1305: [Discriminator loss: 0.556332, acc: 0.628906]  [Adversarial loss: 0.416638, acc: 0.789062]\n",
            "1306: [Discriminator loss: 0.655907, acc: 0.554688]  [Adversarial loss: 0.473135, acc: 0.781250]\n",
            "1307: [Discriminator loss: 0.525140, acc: 0.636719]  [Adversarial loss: 0.392516, acc: 0.820312]\n",
            "1308: [Discriminator loss: 0.599590, acc: 0.578125]  [Adversarial loss: 0.396429, acc: 0.847656]\n",
            "1309: [Discriminator loss: 0.618404, acc: 0.570312]  [Adversarial loss: 0.423805, acc: 0.769531]\n",
            "1310: [Discriminator loss: 0.614583, acc: 0.617188]  [Adversarial loss: 0.364949, acc: 0.812500]\n",
            "1311: [Discriminator loss: 0.609070, acc: 0.605469]  [Adversarial loss: 0.430424, acc: 0.812500]\n",
            "1312: [Discriminator loss: 0.600976, acc: 0.593750]  [Adversarial loss: 0.379640, acc: 0.843750]\n",
            "1313: [Discriminator loss: 0.637847, acc: 0.562500]  [Adversarial loss: 0.408903, acc: 0.824219]\n",
            "1314: [Discriminator loss: 0.644839, acc: 0.554688]  [Adversarial loss: 0.479149, acc: 0.742188]\n",
            "1315: [Discriminator loss: 0.594002, acc: 0.578125]  [Adversarial loss: 0.379467, acc: 0.847656]\n",
            "1316: [Discriminator loss: 0.579265, acc: 0.593750]  [Adversarial loss: 0.470077, acc: 0.761719]\n",
            "1317: [Discriminator loss: 0.577822, acc: 0.617188]  [Adversarial loss: 0.395662, acc: 0.785156]\n",
            "1318: [Discriminator loss: 0.633215, acc: 0.535156]  [Adversarial loss: 0.432859, acc: 0.812500]\n",
            "1319: [Discriminator loss: 0.618653, acc: 0.597656]  [Adversarial loss: 0.354390, acc: 0.839844]\n",
            "1320: [Discriminator loss: 0.612231, acc: 0.609375]  [Adversarial loss: 0.381053, acc: 0.812500]\n",
            "1321: [Discriminator loss: 0.658165, acc: 0.566406]  [Adversarial loss: 0.389935, acc: 0.832031]\n",
            "1322: [Discriminator loss: 0.575916, acc: 0.597656]  [Adversarial loss: 0.443660, acc: 0.812500]\n",
            "1323: [Discriminator loss: 0.658630, acc: 0.531250]  [Adversarial loss: 0.437102, acc: 0.800781]\n",
            "1324: [Discriminator loss: 0.658128, acc: 0.562500]  [Adversarial loss: 0.423470, acc: 0.816406]\n",
            "1325: [Discriminator loss: 0.624468, acc: 0.589844]  [Adversarial loss: 0.412701, acc: 0.835938]\n",
            "1326: [Discriminator loss: 0.663215, acc: 0.562500]  [Adversarial loss: 0.445645, acc: 0.769531]\n",
            "1327: [Discriminator loss: 0.672427, acc: 0.558594]  [Adversarial loss: 0.419852, acc: 0.808594]\n",
            "1328: [Discriminator loss: 0.631590, acc: 0.550781]  [Adversarial loss: 0.406884, acc: 0.812500]\n",
            "1329: [Discriminator loss: 0.576754, acc: 0.597656]  [Adversarial loss: 0.424883, acc: 0.820312]\n",
            "1330: [Discriminator loss: 0.583103, acc: 0.617188]  [Adversarial loss: 0.473297, acc: 0.761719]\n",
            "1331: [Discriminator loss: 0.641625, acc: 0.558594]  [Adversarial loss: 0.380110, acc: 0.859375]\n",
            "1332: [Discriminator loss: 0.568257, acc: 0.621094]  [Adversarial loss: 0.401302, acc: 0.816406]\n",
            "1333: [Discriminator loss: 0.550763, acc: 0.613281]  [Adversarial loss: 0.396080, acc: 0.832031]\n",
            "1334: [Discriminator loss: 0.586089, acc: 0.597656]  [Adversarial loss: 0.463195, acc: 0.773438]\n",
            "1335: [Discriminator loss: 0.683034, acc: 0.578125]  [Adversarial loss: 0.462970, acc: 0.769531]\n",
            "1336: [Discriminator loss: 0.590186, acc: 0.585938]  [Adversarial loss: 0.429912, acc: 0.796875]\n",
            "1337: [Discriminator loss: 0.574643, acc: 0.597656]  [Adversarial loss: 0.454401, acc: 0.757812]\n",
            "1338: [Discriminator loss: 0.604259, acc: 0.613281]  [Adversarial loss: 0.407423, acc: 0.820312]\n",
            "1339: [Discriminator loss: 0.748329, acc: 0.492188]  [Adversarial loss: 0.444136, acc: 0.820312]\n",
            "1340: [Discriminator loss: 0.614207, acc: 0.585938]  [Adversarial loss: 0.423610, acc: 0.820312]\n",
            "1341: [Discriminator loss: 0.599913, acc: 0.585938]  [Adversarial loss: 0.426452, acc: 0.828125]\n",
            "1342: [Discriminator loss: 0.646184, acc: 0.570312]  [Adversarial loss: 0.454728, acc: 0.761719]\n",
            "1343: [Discriminator loss: 0.622665, acc: 0.609375]  [Adversarial loss: 0.452526, acc: 0.765625]\n",
            "1344: [Discriminator loss: 0.599319, acc: 0.593750]  [Adversarial loss: 0.409319, acc: 0.785156]\n",
            "1345: [Discriminator loss: 0.741533, acc: 0.488281]  [Adversarial loss: 0.437382, acc: 0.800781]\n",
            "1346: [Discriminator loss: 0.642119, acc: 0.578125]  [Adversarial loss: 0.464363, acc: 0.765625]\n",
            "1347: [Discriminator loss: 0.628327, acc: 0.578125]  [Adversarial loss: 0.411093, acc: 0.808594]\n",
            "1348: [Discriminator loss: 0.603637, acc: 0.601562]  [Adversarial loss: 0.438184, acc: 0.761719]\n",
            "1349: [Discriminator loss: 0.600090, acc: 0.566406]  [Adversarial loss: 0.467954, acc: 0.757812]\n",
            "1350: [Discriminator loss: 0.609543, acc: 0.585938]  [Adversarial loss: 0.457195, acc: 0.785156]\n",
            "1351: [Discriminator loss: 0.590492, acc: 0.621094]  [Adversarial loss: 0.431073, acc: 0.792969]\n",
            "1352: [Discriminator loss: 0.592078, acc: 0.589844]  [Adversarial loss: 0.437143, acc: 0.789062]\n",
            "1353: [Discriminator loss: 0.685865, acc: 0.574219]  [Adversarial loss: 0.432467, acc: 0.792969]\n",
            "1354: [Discriminator loss: 0.517961, acc: 0.656250]  [Adversarial loss: 0.427865, acc: 0.789062]\n",
            "1355: [Discriminator loss: 0.630371, acc: 0.562500]  [Adversarial loss: 0.417752, acc: 0.816406]\n",
            "1356: [Discriminator loss: 0.595466, acc: 0.593750]  [Adversarial loss: 0.392347, acc: 0.828125]\n",
            "1357: [Discriminator loss: 0.503492, acc: 0.652344]  [Adversarial loss: 0.377323, acc: 0.789062]\n",
            "1358: [Discriminator loss: 0.570255, acc: 0.585938]  [Adversarial loss: 0.386594, acc: 0.847656]\n",
            "1359: [Discriminator loss: 0.551867, acc: 0.625000]  [Adversarial loss: 0.413244, acc: 0.800781]\n",
            "1360: [Discriminator loss: 0.553192, acc: 0.632812]  [Adversarial loss: 0.375614, acc: 0.812500]\n",
            "1361: [Discriminator loss: 0.622417, acc: 0.574219]  [Adversarial loss: 0.380121, acc: 0.835938]\n",
            "1362: [Discriminator loss: 0.576167, acc: 0.589844]  [Adversarial loss: 0.406825, acc: 0.800781]\n",
            "1363: [Discriminator loss: 0.591496, acc: 0.613281]  [Adversarial loss: 0.411891, acc: 0.800781]\n",
            "1364: [Discriminator loss: 0.619556, acc: 0.605469]  [Adversarial loss: 0.412807, acc: 0.800781]\n",
            "1365: [Discriminator loss: 0.623899, acc: 0.585938]  [Adversarial loss: 0.369526, acc: 0.816406]\n",
            "1366: [Discriminator loss: 0.582061, acc: 0.625000]  [Adversarial loss: 0.411005, acc: 0.847656]\n",
            "1367: [Discriminator loss: 0.575450, acc: 0.617188]  [Adversarial loss: 0.372558, acc: 0.781250]\n",
            "1368: [Discriminator loss: 0.600614, acc: 0.578125]  [Adversarial loss: 0.375524, acc: 0.832031]\n",
            "1369: [Discriminator loss: 0.642426, acc: 0.574219]  [Adversarial loss: 0.414231, acc: 0.800781]\n",
            "1370: [Discriminator loss: 0.583688, acc: 0.585938]  [Adversarial loss: 0.408564, acc: 0.789062]\n",
            "1371: [Discriminator loss: 0.643007, acc: 0.515625]  [Adversarial loss: 0.449489, acc: 0.785156]\n",
            "1372: [Discriminator loss: 0.598750, acc: 0.574219]  [Adversarial loss: 0.442156, acc: 0.781250]\n",
            "1373: [Discriminator loss: 0.551533, acc: 0.621094]  [Adversarial loss: 0.378304, acc: 0.835938]\n",
            "1374: [Discriminator loss: 0.514700, acc: 0.667969]  [Adversarial loss: 0.396567, acc: 0.792969]\n",
            "1375: [Discriminator loss: 0.572553, acc: 0.585938]  [Adversarial loss: 0.411878, acc: 0.753906]\n",
            "1376: [Discriminator loss: 0.593492, acc: 0.550781]  [Adversarial loss: 0.390121, acc: 0.789062]\n",
            "1377: [Discriminator loss: 0.550749, acc: 0.625000]  [Adversarial loss: 0.430561, acc: 0.800781]\n",
            "1378: [Discriminator loss: 0.588823, acc: 0.582031]  [Adversarial loss: 0.392972, acc: 0.816406]\n",
            "1379: [Discriminator loss: 0.615162, acc: 0.578125]  [Adversarial loss: 0.421930, acc: 0.777344]\n",
            "1380: [Discriminator loss: 0.600410, acc: 0.621094]  [Adversarial loss: 0.440332, acc: 0.796875]\n",
            "1381: [Discriminator loss: 0.625422, acc: 0.582031]  [Adversarial loss: 0.400697, acc: 0.832031]\n",
            "1382: [Discriminator loss: 0.566035, acc: 0.613281]  [Adversarial loss: 0.417906, acc: 0.796875]\n",
            "1383: [Discriminator loss: 0.605397, acc: 0.640625]  [Adversarial loss: 0.446281, acc: 0.785156]\n",
            "1384: [Discriminator loss: 0.577539, acc: 0.636719]  [Adversarial loss: 0.396703, acc: 0.796875]\n",
            "1385: [Discriminator loss: 0.567462, acc: 0.601562]  [Adversarial loss: 0.409426, acc: 0.792969]\n",
            "1386: [Discriminator loss: 0.537462, acc: 0.644531]  [Adversarial loss: 0.343602, acc: 0.859375]\n",
            "1387: [Discriminator loss: 0.515581, acc: 0.664062]  [Adversarial loss: 0.406876, acc: 0.792969]\n",
            "1388: [Discriminator loss: 0.607255, acc: 0.613281]  [Adversarial loss: 0.358822, acc: 0.843750]\n",
            "1389: [Discriminator loss: 0.622019, acc: 0.593750]  [Adversarial loss: 0.384525, acc: 0.800781]\n",
            "1390: [Discriminator loss: 0.604258, acc: 0.578125]  [Adversarial loss: 0.407293, acc: 0.785156]\n",
            "1391: [Discriminator loss: 0.585621, acc: 0.605469]  [Adversarial loss: 0.431596, acc: 0.843750]\n",
            "1392: [Discriminator loss: 0.552599, acc: 0.609375]  [Adversarial loss: 0.381491, acc: 0.820312]\n",
            "1393: [Discriminator loss: 0.608494, acc: 0.636719]  [Adversarial loss: 0.405958, acc: 0.808594]\n",
            "1394: [Discriminator loss: 0.599508, acc: 0.605469]  [Adversarial loss: 0.458670, acc: 0.769531]\n",
            "1395: [Discriminator loss: 0.546030, acc: 0.648438]  [Adversarial loss: 0.395196, acc: 0.796875]\n",
            "1396: [Discriminator loss: 0.630482, acc: 0.578125]  [Adversarial loss: 0.436055, acc: 0.781250]\n",
            "1397: [Discriminator loss: 0.562565, acc: 0.609375]  [Adversarial loss: 0.385421, acc: 0.824219]\n",
            "1398: [Discriminator loss: 0.574093, acc: 0.601562]  [Adversarial loss: 0.409280, acc: 0.800781]\n",
            "1399: [Discriminator loss: 0.594169, acc: 0.597656]  [Adversarial loss: 0.369565, acc: 0.812500]\n",
            "1400: [Discriminator loss: 0.630710, acc: 0.605469]  [Adversarial loss: 0.393276, acc: 0.832031]\n",
            "1401: [Discriminator loss: 0.631725, acc: 0.550781]  [Adversarial loss: 0.407512, acc: 0.781250]\n",
            "1402: [Discriminator loss: 0.527017, acc: 0.644531]  [Adversarial loss: 0.406362, acc: 0.804688]\n",
            "1403: [Discriminator loss: 0.635712, acc: 0.570312]  [Adversarial loss: 0.395063, acc: 0.800781]\n",
            "1404: [Discriminator loss: 0.589889, acc: 0.593750]  [Adversarial loss: 0.396178, acc: 0.843750]\n",
            "1405: [Discriminator loss: 0.658058, acc: 0.574219]  [Adversarial loss: 0.404137, acc: 0.796875]\n",
            "1406: [Discriminator loss: 0.680384, acc: 0.550781]  [Adversarial loss: 0.426056, acc: 0.812500]\n",
            "1407: [Discriminator loss: 0.584105, acc: 0.636719]  [Adversarial loss: 0.504988, acc: 0.746094]\n",
            "1408: [Discriminator loss: 0.588644, acc: 0.578125]  [Adversarial loss: 0.373366, acc: 0.808594]\n",
            "1409: [Discriminator loss: 0.633075, acc: 0.601562]  [Adversarial loss: 0.370239, acc: 0.843750]\n",
            "1410: [Discriminator loss: 0.668980, acc: 0.488281]  [Adversarial loss: 0.408904, acc: 0.808594]\n",
            "1411: [Discriminator loss: 0.640869, acc: 0.542969]  [Adversarial loss: 0.435468, acc: 0.785156]\n",
            "1412: [Discriminator loss: 0.628157, acc: 0.554688]  [Adversarial loss: 0.411771, acc: 0.789062]\n",
            "1413: [Discriminator loss: 0.631541, acc: 0.578125]  [Adversarial loss: 0.411770, acc: 0.804688]\n",
            "1414: [Discriminator loss: 0.646531, acc: 0.609375]  [Adversarial loss: 0.396138, acc: 0.781250]\n",
            "1415: [Discriminator loss: 0.644999, acc: 0.566406]  [Adversarial loss: 0.395080, acc: 0.828125]\n",
            "1416: [Discriminator loss: 0.564209, acc: 0.613281]  [Adversarial loss: 0.404860, acc: 0.804688]\n",
            "1417: [Discriminator loss: 0.538573, acc: 0.636719]  [Adversarial loss: 0.385840, acc: 0.800781]\n",
            "1418: [Discriminator loss: 0.621222, acc: 0.593750]  [Adversarial loss: 0.383044, acc: 0.800781]\n",
            "1419: [Discriminator loss: 0.595728, acc: 0.589844]  [Adversarial loss: 0.352720, acc: 0.832031]\n",
            "1420: [Discriminator loss: 0.612119, acc: 0.562500]  [Adversarial loss: 0.408716, acc: 0.796875]\n",
            "1421: [Discriminator loss: 0.564177, acc: 0.628906]  [Adversarial loss: 0.376554, acc: 0.816406]\n",
            "1422: [Discriminator loss: 0.646011, acc: 0.570312]  [Adversarial loss: 0.395558, acc: 0.804688]\n",
            "1423: [Discriminator loss: 0.601025, acc: 0.628906]  [Adversarial loss: 0.382445, acc: 0.804688]\n",
            "1424: [Discriminator loss: 0.558985, acc: 0.609375]  [Adversarial loss: 0.417366, acc: 0.773438]\n",
            "1425: [Discriminator loss: 0.659224, acc: 0.539062]  [Adversarial loss: 0.406421, acc: 0.792969]\n",
            "1426: [Discriminator loss: 0.518032, acc: 0.625000]  [Adversarial loss: 0.395627, acc: 0.816406]\n",
            "1427: [Discriminator loss: 0.557761, acc: 0.621094]  [Adversarial loss: 0.414444, acc: 0.800781]\n",
            "1428: [Discriminator loss: 0.588159, acc: 0.636719]  [Adversarial loss: 0.400569, acc: 0.792969]\n",
            "1429: [Discriminator loss: 0.588823, acc: 0.613281]  [Adversarial loss: 0.439324, acc: 0.785156]\n",
            "1430: [Discriminator loss: 0.590736, acc: 0.601562]  [Adversarial loss: 0.407492, acc: 0.765625]\n",
            "1431: [Discriminator loss: 0.633472, acc: 0.609375]  [Adversarial loss: 0.434878, acc: 0.816406]\n",
            "1432: [Discriminator loss: 0.600495, acc: 0.558594]  [Adversarial loss: 0.392089, acc: 0.792969]\n",
            "1433: [Discriminator loss: 0.594842, acc: 0.578125]  [Adversarial loss: 0.366940, acc: 0.839844]\n",
            "1434: [Discriminator loss: 0.615481, acc: 0.613281]  [Adversarial loss: 0.427097, acc: 0.773438]\n",
            "1435: [Discriminator loss: 0.573190, acc: 0.617188]  [Adversarial loss: 0.402066, acc: 0.792969]\n",
            "1436: [Discriminator loss: 0.514381, acc: 0.625000]  [Adversarial loss: 0.387009, acc: 0.789062]\n",
            "1437: [Discriminator loss: 0.555753, acc: 0.621094]  [Adversarial loss: 0.360244, acc: 0.820312]\n",
            "1438: [Discriminator loss: 0.615312, acc: 0.609375]  [Adversarial loss: 0.402977, acc: 0.820312]\n",
            "1439: [Discriminator loss: 0.614692, acc: 0.601562]  [Adversarial loss: 0.376716, acc: 0.835938]\n",
            "1440: [Discriminator loss: 0.695720, acc: 0.515625]  [Adversarial loss: 0.440975, acc: 0.808594]\n",
            "1441: [Discriminator loss: 0.600426, acc: 0.589844]  [Adversarial loss: 0.386261, acc: 0.828125]\n",
            "1442: [Discriminator loss: 0.534763, acc: 0.640625]  [Adversarial loss: 0.350252, acc: 0.855469]\n",
            "1443: [Discriminator loss: 0.575045, acc: 0.589844]  [Adversarial loss: 0.454603, acc: 0.769531]\n",
            "1444: [Discriminator loss: 0.575127, acc: 0.632812]  [Adversarial loss: 0.448706, acc: 0.753906]\n",
            "1445: [Discriminator loss: 0.508740, acc: 0.664062]  [Adversarial loss: 0.419872, acc: 0.738281]\n",
            "1446: [Discriminator loss: 0.560577, acc: 0.667969]  [Adversarial loss: 0.431669, acc: 0.742188]\n",
            "1447: [Discriminator loss: 0.586681, acc: 0.597656]  [Adversarial loss: 0.420231, acc: 0.750000]\n",
            "1448: [Discriminator loss: 0.579216, acc: 0.578125]  [Adversarial loss: 0.426165, acc: 0.707031]\n",
            "1449: [Discriminator loss: 0.595713, acc: 0.589844]  [Adversarial loss: 0.412879, acc: 0.789062]\n",
            "1450: [Discriminator loss: 0.593929, acc: 0.628906]  [Adversarial loss: 0.408634, acc: 0.777344]\n",
            "1451: [Discriminator loss: 0.599445, acc: 0.636719]  [Adversarial loss: 0.398686, acc: 0.804688]\n",
            "1452: [Discriminator loss: 0.555201, acc: 0.617188]  [Adversarial loss: 0.384894, acc: 0.796875]\n",
            "1453: [Discriminator loss: 0.554726, acc: 0.632812]  [Adversarial loss: 0.400561, acc: 0.792969]\n",
            "1454: [Discriminator loss: 0.648196, acc: 0.566406]  [Adversarial loss: 0.364093, acc: 0.839844]\n",
            "1455: [Discriminator loss: 0.559786, acc: 0.644531]  [Adversarial loss: 0.375073, acc: 0.835938]\n",
            "1456: [Discriminator loss: 0.602944, acc: 0.644531]  [Adversarial loss: 0.425302, acc: 0.757812]\n",
            "1457: [Discriminator loss: 0.642955, acc: 0.617188]  [Adversarial loss: 0.416089, acc: 0.753906]\n",
            "1458: [Discriminator loss: 0.512289, acc: 0.675781]  [Adversarial loss: 0.380027, acc: 0.800781]\n",
            "1459: [Discriminator loss: 0.553415, acc: 0.613281]  [Adversarial loss: 0.404787, acc: 0.792969]\n",
            "1460: [Discriminator loss: 0.516172, acc: 0.628906]  [Adversarial loss: 0.423221, acc: 0.769531]\n",
            "1461: [Discriminator loss: 0.577470, acc: 0.640625]  [Adversarial loss: 0.436393, acc: 0.832031]\n",
            "1462: [Discriminator loss: 0.566834, acc: 0.632812]  [Adversarial loss: 0.384606, acc: 0.828125]\n",
            "1463: [Discriminator loss: 0.629746, acc: 0.585938]  [Adversarial loss: 0.411881, acc: 0.757812]\n",
            "1464: [Discriminator loss: 0.555333, acc: 0.632812]  [Adversarial loss: 0.374236, acc: 0.843750]\n",
            "1465: [Discriminator loss: 0.627493, acc: 0.582031]  [Adversarial loss: 0.382875, acc: 0.820312]\n",
            "1466: [Discriminator loss: 0.645694, acc: 0.562500]  [Adversarial loss: 0.419062, acc: 0.757812]\n",
            "1467: [Discriminator loss: 0.633488, acc: 0.613281]  [Adversarial loss: 0.391262, acc: 0.769531]\n",
            "1468: [Discriminator loss: 0.586153, acc: 0.617188]  [Adversarial loss: 0.381259, acc: 0.800781]\n",
            "1469: [Discriminator loss: 0.576673, acc: 0.566406]  [Adversarial loss: 0.385408, acc: 0.851562]\n",
            "1470: [Discriminator loss: 0.654581, acc: 0.582031]  [Adversarial loss: 0.415247, acc: 0.785156]\n",
            "1471: [Discriminator loss: 0.558895, acc: 0.640625]  [Adversarial loss: 0.374504, acc: 0.824219]\n",
            "1472: [Discriminator loss: 0.619025, acc: 0.621094]  [Adversarial loss: 0.377432, acc: 0.785156]\n",
            "1473: [Discriminator loss: 0.572008, acc: 0.593750]  [Adversarial loss: 0.407092, acc: 0.800781]\n",
            "1474: [Discriminator loss: 0.606784, acc: 0.597656]  [Adversarial loss: 0.358121, acc: 0.835938]\n",
            "1475: [Discriminator loss: 0.598993, acc: 0.617188]  [Adversarial loss: 0.364211, acc: 0.812500]\n",
            "1476: [Discriminator loss: 0.670401, acc: 0.554688]  [Adversarial loss: 0.453729, acc: 0.792969]\n",
            "1477: [Discriminator loss: 0.601171, acc: 0.582031]  [Adversarial loss: 0.375350, acc: 0.839844]\n",
            "1478: [Discriminator loss: 0.586537, acc: 0.585938]  [Adversarial loss: 0.401847, acc: 0.851562]\n",
            "1479: [Discriminator loss: 0.571157, acc: 0.613281]  [Adversarial loss: 0.390525, acc: 0.867188]\n",
            "1480: [Discriminator loss: 0.569590, acc: 0.625000]  [Adversarial loss: 0.380321, acc: 0.828125]\n",
            "1481: [Discriminator loss: 0.595467, acc: 0.597656]  [Adversarial loss: 0.388201, acc: 0.796875]\n",
            "1482: [Discriminator loss: 0.586339, acc: 0.636719]  [Adversarial loss: 0.362002, acc: 0.839844]\n",
            "1483: [Discriminator loss: 0.546053, acc: 0.644531]  [Adversarial loss: 0.368679, acc: 0.816406]\n",
            "1484: [Discriminator loss: 0.612567, acc: 0.562500]  [Adversarial loss: 0.388052, acc: 0.828125]\n",
            "1485: [Discriminator loss: 0.614494, acc: 0.570312]  [Adversarial loss: 0.448454, acc: 0.777344]\n",
            "1486: [Discriminator loss: 0.591621, acc: 0.574219]  [Adversarial loss: 0.348331, acc: 0.808594]\n",
            "1487: [Discriminator loss: 0.536308, acc: 0.628906]  [Adversarial loss: 0.402684, acc: 0.824219]\n",
            "1488: [Discriminator loss: 0.599109, acc: 0.582031]  [Adversarial loss: 0.383107, acc: 0.835938]\n",
            "1489: [Discriminator loss: 0.540764, acc: 0.660156]  [Adversarial loss: 0.358765, acc: 0.839844]\n",
            "1490: [Discriminator loss: 0.612599, acc: 0.554688]  [Adversarial loss: 0.401007, acc: 0.820312]\n",
            "1491: [Discriminator loss: 0.626414, acc: 0.570312]  [Adversarial loss: 0.435886, acc: 0.835938]\n",
            "1492: [Discriminator loss: 0.644896, acc: 0.570312]  [Adversarial loss: 0.389667, acc: 0.820312]\n",
            "1493: [Discriminator loss: 0.725317, acc: 0.527344]  [Adversarial loss: 0.412004, acc: 0.824219]\n",
            "1494: [Discriminator loss: 0.581696, acc: 0.589844]  [Adversarial loss: 0.405326, acc: 0.816406]\n",
            "1495: [Discriminator loss: 0.586763, acc: 0.625000]  [Adversarial loss: 0.420996, acc: 0.839844]\n",
            "1496: [Discriminator loss: 0.630578, acc: 0.574219]  [Adversarial loss: 0.397557, acc: 0.816406]\n",
            "1497: [Discriminator loss: 0.590973, acc: 0.585938]  [Adversarial loss: 0.361449, acc: 0.851562]\n",
            "1498: [Discriminator loss: 0.643464, acc: 0.613281]  [Adversarial loss: 0.406741, acc: 0.816406]\n",
            "1499: [Discriminator loss: 0.625439, acc: 0.574219]  [Adversarial loss: 0.388607, acc: 0.851562]\n",
            "1500: [Discriminator loss: 0.567190, acc: 0.582031]  [Adversarial loss: 0.430367, acc: 0.808594]\n",
            "1501: [Discriminator loss: 0.678623, acc: 0.582031]  [Adversarial loss: 0.409697, acc: 0.828125]\n",
            "1502: [Discriminator loss: 0.618428, acc: 0.566406]  [Adversarial loss: 0.370347, acc: 0.851562]\n",
            "1503: [Discriminator loss: 0.620195, acc: 0.605469]  [Adversarial loss: 0.392953, acc: 0.828125]\n",
            "1504: [Discriminator loss: 0.598157, acc: 0.617188]  [Adversarial loss: 0.379824, acc: 0.792969]\n",
            "1505: [Discriminator loss: 0.576059, acc: 0.613281]  [Adversarial loss: 0.445608, acc: 0.785156]\n",
            "1506: [Discriminator loss: 0.606735, acc: 0.566406]  [Adversarial loss: 0.392941, acc: 0.804688]\n",
            "1507: [Discriminator loss: 0.566058, acc: 0.621094]  [Adversarial loss: 0.368393, acc: 0.828125]\n",
            "1508: [Discriminator loss: 0.596084, acc: 0.632812]  [Adversarial loss: 0.430660, acc: 0.820312]\n",
            "1509: [Discriminator loss: 0.539889, acc: 0.601562]  [Adversarial loss: 0.379548, acc: 0.796875]\n",
            "1510: [Discriminator loss: 0.658840, acc: 0.511719]  [Adversarial loss: 0.415249, acc: 0.812500]\n",
            "1511: [Discriminator loss: 0.575722, acc: 0.632812]  [Adversarial loss: 0.379592, acc: 0.812500]\n",
            "1512: [Discriminator loss: 0.584335, acc: 0.574219]  [Adversarial loss: 0.400477, acc: 0.804688]\n",
            "1513: [Discriminator loss: 0.622012, acc: 0.578125]  [Adversarial loss: 0.372375, acc: 0.816406]\n",
            "1514: [Discriminator loss: 0.496927, acc: 0.656250]  [Adversarial loss: 0.370236, acc: 0.820312]\n",
            "1515: [Discriminator loss: 0.601001, acc: 0.582031]  [Adversarial loss: 0.410357, acc: 0.785156]\n",
            "1516: [Discriminator loss: 0.589986, acc: 0.578125]  [Adversarial loss: 0.390207, acc: 0.839844]\n",
            "1517: [Discriminator loss: 0.633968, acc: 0.585938]  [Adversarial loss: 0.347538, acc: 0.839844]\n",
            "1518: [Discriminator loss: 0.614255, acc: 0.550781]  [Adversarial loss: 0.423247, acc: 0.824219]\n",
            "1519: [Discriminator loss: 0.557916, acc: 0.597656]  [Adversarial loss: 0.359604, acc: 0.832031]\n",
            "1520: [Discriminator loss: 0.628386, acc: 0.566406]  [Adversarial loss: 0.398527, acc: 0.808594]\n",
            "1521: [Discriminator loss: 0.593878, acc: 0.589844]  [Adversarial loss: 0.379719, acc: 0.835938]\n",
            "1522: [Discriminator loss: 0.614084, acc: 0.566406]  [Adversarial loss: 0.417673, acc: 0.761719]\n",
            "1523: [Discriminator loss: 0.601726, acc: 0.632812]  [Adversarial loss: 0.405573, acc: 0.824219]\n",
            "1524: [Discriminator loss: 0.546478, acc: 0.648438]  [Adversarial loss: 0.316172, acc: 0.871094]\n",
            "1525: [Discriminator loss: 0.525331, acc: 0.640625]  [Adversarial loss: 0.417079, acc: 0.800781]\n",
            "1526: [Discriminator loss: 0.554774, acc: 0.617188]  [Adversarial loss: 0.372510, acc: 0.855469]\n",
            "1527: [Discriminator loss: 0.640252, acc: 0.574219]  [Adversarial loss: 0.399047, acc: 0.820312]\n",
            "1528: [Discriminator loss: 0.542669, acc: 0.605469]  [Adversarial loss: 0.370750, acc: 0.824219]\n",
            "1529: [Discriminator loss: 0.603227, acc: 0.597656]  [Adversarial loss: 0.365736, acc: 0.808594]\n",
            "1530: [Discriminator loss: 0.591409, acc: 0.613281]  [Adversarial loss: 0.356930, acc: 0.824219]\n",
            "1531: [Discriminator loss: 0.601836, acc: 0.605469]  [Adversarial loss: 0.348539, acc: 0.839844]\n",
            "1532: [Discriminator loss: 0.609339, acc: 0.578125]  [Adversarial loss: 0.396220, acc: 0.835938]\n",
            "1533: [Discriminator loss: 0.575102, acc: 0.617188]  [Adversarial loss: 0.381489, acc: 0.820312]\n",
            "1534: [Discriminator loss: 0.665823, acc: 0.589844]  [Adversarial loss: 0.387993, acc: 0.843750]\n",
            "1535: [Discriminator loss: 0.584284, acc: 0.601562]  [Adversarial loss: 0.421028, acc: 0.812500]\n",
            "1536: [Discriminator loss: 0.573829, acc: 0.605469]  [Adversarial loss: 0.350455, acc: 0.812500]\n",
            "1537: [Discriminator loss: 0.569877, acc: 0.593750]  [Adversarial loss: 0.372942, acc: 0.828125]\n",
            "1538: [Discriminator loss: 0.595688, acc: 0.593750]  [Adversarial loss: 0.389333, acc: 0.804688]\n",
            "1539: [Discriminator loss: 0.635228, acc: 0.582031]  [Adversarial loss: 0.404031, acc: 0.828125]\n",
            "1540: [Discriminator loss: 0.539903, acc: 0.628906]  [Adversarial loss: 0.363606, acc: 0.824219]\n",
            "1541: [Discriminator loss: 0.561748, acc: 0.613281]  [Adversarial loss: 0.414170, acc: 0.812500]\n",
            "1542: [Discriminator loss: 0.522168, acc: 0.636719]  [Adversarial loss: 0.372081, acc: 0.843750]\n",
            "1543: [Discriminator loss: 0.607662, acc: 0.582031]  [Adversarial loss: 0.379054, acc: 0.820312]\n",
            "1544: [Discriminator loss: 0.628237, acc: 0.566406]  [Adversarial loss: 0.390907, acc: 0.812500]\n",
            "1545: [Discriminator loss: 0.625864, acc: 0.597656]  [Adversarial loss: 0.360091, acc: 0.816406]\n",
            "1546: [Discriminator loss: 0.545298, acc: 0.632812]  [Adversarial loss: 0.397325, acc: 0.816406]\n",
            "1547: [Discriminator loss: 0.636456, acc: 0.582031]  [Adversarial loss: 0.436638, acc: 0.796875]\n",
            "1548: [Discriminator loss: 0.568223, acc: 0.613281]  [Adversarial loss: 0.345583, acc: 0.859375]\n",
            "1549: [Discriminator loss: 0.498661, acc: 0.683594]  [Adversarial loss: 0.375044, acc: 0.816406]\n",
            "1550: [Discriminator loss: 0.604544, acc: 0.601562]  [Adversarial loss: 0.490127, acc: 0.742188]\n",
            "1551: [Discriminator loss: 0.572425, acc: 0.621094]  [Adversarial loss: 0.423868, acc: 0.808594]\n",
            "1552: [Discriminator loss: 0.625301, acc: 0.589844]  [Adversarial loss: 0.429571, acc: 0.816406]\n",
            "1553: [Discriminator loss: 0.565727, acc: 0.597656]  [Adversarial loss: 0.401817, acc: 0.828125]\n",
            "1554: [Discriminator loss: 0.583135, acc: 0.609375]  [Adversarial loss: 0.430672, acc: 0.816406]\n",
            "1555: [Discriminator loss: 0.559648, acc: 0.605469]  [Adversarial loss: 0.410119, acc: 0.808594]\n",
            "1556: [Discriminator loss: 0.584019, acc: 0.613281]  [Adversarial loss: 0.398801, acc: 0.816406]\n",
            "1557: [Discriminator loss: 0.577426, acc: 0.578125]  [Adversarial loss: 0.406897, acc: 0.757812]\n",
            "1558: [Discriminator loss: 0.577957, acc: 0.617188]  [Adversarial loss: 0.404917, acc: 0.804688]\n",
            "1559: [Discriminator loss: 0.513113, acc: 0.628906]  [Adversarial loss: 0.409309, acc: 0.773438]\n",
            "1560: [Discriminator loss: 0.577990, acc: 0.582031]  [Adversarial loss: 0.419978, acc: 0.792969]\n",
            "1561: [Discriminator loss: 0.534058, acc: 0.683594]  [Adversarial loss: 0.379962, acc: 0.816406]\n",
            "1562: [Discriminator loss: 0.540595, acc: 0.632812]  [Adversarial loss: 0.375707, acc: 0.824219]\n",
            "1563: [Discriminator loss: 0.567234, acc: 0.636719]  [Adversarial loss: 0.402270, acc: 0.765625]\n",
            "1564: [Discriminator loss: 0.594095, acc: 0.585938]  [Adversarial loss: 0.411000, acc: 0.761719]\n",
            "1565: [Discriminator loss: 0.567347, acc: 0.605469]  [Adversarial loss: 0.382726, acc: 0.804688]\n",
            "1566: [Discriminator loss: 0.573892, acc: 0.562500]  [Adversarial loss: 0.363818, acc: 0.843750]\n",
            "1567: [Discriminator loss: 0.587476, acc: 0.574219]  [Adversarial loss: 0.426173, acc: 0.812500]\n",
            "1568: [Discriminator loss: 0.542512, acc: 0.605469]  [Adversarial loss: 0.405652, acc: 0.777344]\n",
            "1569: [Discriminator loss: 0.584864, acc: 0.605469]  [Adversarial loss: 0.395240, acc: 0.792969]\n",
            "1570: [Discriminator loss: 0.552269, acc: 0.625000]  [Adversarial loss: 0.424306, acc: 0.832031]\n",
            "1571: [Discriminator loss: 0.556387, acc: 0.648438]  [Adversarial loss: 0.368037, acc: 0.812500]\n",
            "1572: [Discriminator loss: 0.559072, acc: 0.613281]  [Adversarial loss: 0.446281, acc: 0.753906]\n",
            "1573: [Discriminator loss: 0.575355, acc: 0.625000]  [Adversarial loss: 0.392647, acc: 0.851562]\n",
            "1574: [Discriminator loss: 0.530441, acc: 0.640625]  [Adversarial loss: 0.437002, acc: 0.761719]\n",
            "1575: [Discriminator loss: 0.564174, acc: 0.609375]  [Adversarial loss: 0.387662, acc: 0.847656]\n",
            "1576: [Discriminator loss: 0.549199, acc: 0.644531]  [Adversarial loss: 0.411290, acc: 0.773438]\n",
            "1577: [Discriminator loss: 0.503306, acc: 0.664062]  [Adversarial loss: 0.377063, acc: 0.820312]\n",
            "1578: [Discriminator loss: 0.522910, acc: 0.667969]  [Adversarial loss: 0.389674, acc: 0.785156]\n",
            "1579: [Discriminator loss: 0.583083, acc: 0.617188]  [Adversarial loss: 0.407345, acc: 0.792969]\n",
            "1580: [Discriminator loss: 0.602371, acc: 0.613281]  [Adversarial loss: 0.369251, acc: 0.828125]\n",
            "1581: [Discriminator loss: 0.559645, acc: 0.640625]  [Adversarial loss: 0.400575, acc: 0.832031]\n",
            "1582: [Discriminator loss: 0.564699, acc: 0.621094]  [Adversarial loss: 0.378955, acc: 0.792969]\n",
            "1583: [Discriminator loss: 0.575719, acc: 0.601562]  [Adversarial loss: 0.410042, acc: 0.824219]\n",
            "1584: [Discriminator loss: 0.620474, acc: 0.570312]  [Adversarial loss: 0.419261, acc: 0.820312]\n",
            "1585: [Discriminator loss: 0.561997, acc: 0.636719]  [Adversarial loss: 0.355369, acc: 0.855469]\n",
            "1586: [Discriminator loss: 0.531573, acc: 0.617188]  [Adversarial loss: 0.416957, acc: 0.773438]\n",
            "1587: [Discriminator loss: 0.539290, acc: 0.656250]  [Adversarial loss: 0.388229, acc: 0.832031]\n",
            "1588: [Discriminator loss: 0.524599, acc: 0.636719]  [Adversarial loss: 0.385758, acc: 0.808594]\n",
            "1589: [Discriminator loss: 0.539178, acc: 0.648438]  [Adversarial loss: 0.378282, acc: 0.792969]\n",
            "1590: [Discriminator loss: 0.569989, acc: 0.664062]  [Adversarial loss: 0.345942, acc: 0.835938]\n",
            "1591: [Discriminator loss: 0.554922, acc: 0.667969]  [Adversarial loss: 0.382970, acc: 0.808594]\n",
            "1592: [Discriminator loss: 0.551688, acc: 0.601562]  [Adversarial loss: 0.382690, acc: 0.796875]\n",
            "1593: [Discriminator loss: 0.489606, acc: 0.679688]  [Adversarial loss: 0.390701, acc: 0.812500]\n",
            "1594: [Discriminator loss: 0.604273, acc: 0.585938]  [Adversarial loss: 0.444934, acc: 0.761719]\n",
            "1595: [Discriminator loss: 0.545228, acc: 0.625000]  [Adversarial loss: 0.356024, acc: 0.800781]\n",
            "1596: [Discriminator loss: 0.639141, acc: 0.566406]  [Adversarial loss: 0.412501, acc: 0.800781]\n",
            "1597: [Discriminator loss: 0.507494, acc: 0.671875]  [Adversarial loss: 0.355555, acc: 0.816406]\n",
            "1598: [Discriminator loss: 0.593555, acc: 0.578125]  [Adversarial loss: 0.365176, acc: 0.855469]\n",
            "1599: [Discriminator loss: 0.502048, acc: 0.621094]  [Adversarial loss: 0.363174, acc: 0.808594]\n",
            "1600: [Discriminator loss: 0.543303, acc: 0.613281]  [Adversarial loss: 0.395048, acc: 0.804688]\n",
            "1601: [Discriminator loss: 0.657290, acc: 0.570312]  [Adversarial loss: 0.417511, acc: 0.773438]\n",
            "1602: [Discriminator loss: 0.576420, acc: 0.605469]  [Adversarial loss: 0.412321, acc: 0.820312]\n",
            "1603: [Discriminator loss: 0.583535, acc: 0.601562]  [Adversarial loss: 0.350601, acc: 0.851562]\n",
            "1604: [Discriminator loss: 0.601922, acc: 0.578125]  [Adversarial loss: 0.399672, acc: 0.855469]\n",
            "1605: [Discriminator loss: 0.640229, acc: 0.582031]  [Adversarial loss: 0.427874, acc: 0.746094]\n",
            "1606: [Discriminator loss: 0.553999, acc: 0.640625]  [Adversarial loss: 0.392423, acc: 0.789062]\n",
            "1607: [Discriminator loss: 0.628620, acc: 0.558594]  [Adversarial loss: 0.405750, acc: 0.824219]\n",
            "1608: [Discriminator loss: 0.575064, acc: 0.593750]  [Adversarial loss: 0.382516, acc: 0.839844]\n",
            "1609: [Discriminator loss: 0.601188, acc: 0.609375]  [Adversarial loss: 0.338200, acc: 0.847656]\n",
            "1610: [Discriminator loss: 0.566581, acc: 0.648438]  [Adversarial loss: 0.365480, acc: 0.832031]\n",
            "1611: [Discriminator loss: 0.595166, acc: 0.609375]  [Adversarial loss: 0.389754, acc: 0.832031]\n",
            "1612: [Discriminator loss: 0.517458, acc: 0.632812]  [Adversarial loss: 0.407449, acc: 0.785156]\n",
            "1613: [Discriminator loss: 0.606009, acc: 0.609375]  [Adversarial loss: 0.411673, acc: 0.808594]\n",
            "1614: [Discriminator loss: 0.562287, acc: 0.644531]  [Adversarial loss: 0.354862, acc: 0.828125]\n",
            "1615: [Discriminator loss: 0.569757, acc: 0.644531]  [Adversarial loss: 0.354001, acc: 0.820312]\n",
            "1616: [Discriminator loss: 0.594673, acc: 0.589844]  [Adversarial loss: 0.364220, acc: 0.781250]\n",
            "1617: [Discriminator loss: 0.579679, acc: 0.617188]  [Adversarial loss: 0.394673, acc: 0.820312]\n",
            "1618: [Discriminator loss: 0.541177, acc: 0.625000]  [Adversarial loss: 0.403722, acc: 0.773438]\n",
            "1619: [Discriminator loss: 0.576443, acc: 0.628906]  [Adversarial loss: 0.412832, acc: 0.777344]\n",
            "1620: [Discriminator loss: 0.630105, acc: 0.621094]  [Adversarial loss: 0.397890, acc: 0.796875]\n",
            "1621: [Discriminator loss: 0.544885, acc: 0.632812]  [Adversarial loss: 0.342174, acc: 0.816406]\n",
            "1622: [Discriminator loss: 0.568491, acc: 0.636719]  [Adversarial loss: 0.367302, acc: 0.835938]\n",
            "1623: [Discriminator loss: 0.582423, acc: 0.636719]  [Adversarial loss: 0.412722, acc: 0.785156]\n",
            "1624: [Discriminator loss: 0.619327, acc: 0.597656]  [Adversarial loss: 0.385417, acc: 0.816406]\n",
            "1625: [Discriminator loss: 0.577922, acc: 0.632812]  [Adversarial loss: 0.378900, acc: 0.773438]\n",
            "1626: [Discriminator loss: 0.505654, acc: 0.656250]  [Adversarial loss: 0.408907, acc: 0.773438]\n",
            "1627: [Discriminator loss: 0.534902, acc: 0.656250]  [Adversarial loss: 0.403934, acc: 0.792969]\n",
            "1628: [Discriminator loss: 0.556307, acc: 0.625000]  [Adversarial loss: 0.432307, acc: 0.792969]\n",
            "1629: [Discriminator loss: 0.554370, acc: 0.664062]  [Adversarial loss: 0.417967, acc: 0.812500]\n",
            "1630: [Discriminator loss: 0.524247, acc: 0.636719]  [Adversarial loss: 0.352522, acc: 0.812500]\n",
            "1631: [Discriminator loss: 0.552945, acc: 0.621094]  [Adversarial loss: 0.433156, acc: 0.808594]\n",
            "1632: [Discriminator loss: 0.566866, acc: 0.652344]  [Adversarial loss: 0.435492, acc: 0.757812]\n",
            "1633: [Discriminator loss: 0.538898, acc: 0.640625]  [Adversarial loss: 0.368135, acc: 0.839844]\n",
            "1634: [Discriminator loss: 0.554767, acc: 0.601562]  [Adversarial loss: 0.404714, acc: 0.824219]\n",
            "1635: [Discriminator loss: 0.508527, acc: 0.648438]  [Adversarial loss: 0.378836, acc: 0.816406]\n",
            "1636: [Discriminator loss: 0.565310, acc: 0.582031]  [Adversarial loss: 0.425099, acc: 0.773438]\n",
            "1637: [Discriminator loss: 0.577276, acc: 0.585938]  [Adversarial loss: 0.454904, acc: 0.753906]\n",
            "1638: [Discriminator loss: 0.536899, acc: 0.632812]  [Adversarial loss: 0.437727, acc: 0.753906]\n",
            "1639: [Discriminator loss: 0.552491, acc: 0.625000]  [Adversarial loss: 0.384760, acc: 0.812500]\n",
            "1640: [Discriminator loss: 0.541236, acc: 0.609375]  [Adversarial loss: 0.381772, acc: 0.800781]\n",
            "1641: [Discriminator loss: 0.549126, acc: 0.625000]  [Adversarial loss: 0.382472, acc: 0.781250]\n",
            "1642: [Discriminator loss: 0.608255, acc: 0.578125]  [Adversarial loss: 0.399985, acc: 0.820312]\n",
            "1643: [Discriminator loss: 0.595379, acc: 0.589844]  [Adversarial loss: 0.397937, acc: 0.835938]\n",
            "1644: [Discriminator loss: 0.553360, acc: 0.628906]  [Adversarial loss: 0.383113, acc: 0.804688]\n",
            "1645: [Discriminator loss: 0.509020, acc: 0.656250]  [Adversarial loss: 0.399639, acc: 0.800781]\n",
            "1646: [Discriminator loss: 0.583887, acc: 0.648438]  [Adversarial loss: 0.410454, acc: 0.792969]\n",
            "1647: [Discriminator loss: 0.584318, acc: 0.628906]  [Adversarial loss: 0.409962, acc: 0.820312]\n",
            "1648: [Discriminator loss: 0.606873, acc: 0.601562]  [Adversarial loss: 0.403227, acc: 0.789062]\n",
            "1649: [Discriminator loss: 0.595348, acc: 0.621094]  [Adversarial loss: 0.396008, acc: 0.816406]\n",
            "1650: [Discriminator loss: 0.612245, acc: 0.621094]  [Adversarial loss: 0.384065, acc: 0.808594]\n",
            "1651: [Discriminator loss: 0.579229, acc: 0.640625]  [Adversarial loss: 0.392762, acc: 0.816406]\n",
            "1652: [Discriminator loss: 0.556049, acc: 0.609375]  [Adversarial loss: 0.372599, acc: 0.816406]\n",
            "1653: [Discriminator loss: 0.577434, acc: 0.585938]  [Adversarial loss: 0.388527, acc: 0.796875]\n",
            "1654: [Discriminator loss: 0.555093, acc: 0.636719]  [Adversarial loss: 0.406036, acc: 0.828125]\n",
            "1655: [Discriminator loss: 0.568392, acc: 0.593750]  [Adversarial loss: 0.425193, acc: 0.773438]\n",
            "1656: [Discriminator loss: 0.550385, acc: 0.589844]  [Adversarial loss: 0.418224, acc: 0.777344]\n",
            "1657: [Discriminator loss: 0.538311, acc: 0.625000]  [Adversarial loss: 0.402075, acc: 0.781250]\n",
            "1658: [Discriminator loss: 0.556291, acc: 0.605469]  [Adversarial loss: 0.382245, acc: 0.847656]\n",
            "1659: [Discriminator loss: 0.539841, acc: 0.640625]  [Adversarial loss: 0.395676, acc: 0.808594]\n",
            "1660: [Discriminator loss: 0.560962, acc: 0.589844]  [Adversarial loss: 0.455642, acc: 0.769531]\n",
            "1661: [Discriminator loss: 0.506512, acc: 0.652344]  [Adversarial loss: 0.391500, acc: 0.781250]\n",
            "1662: [Discriminator loss: 0.585967, acc: 0.617188]  [Adversarial loss: 0.363516, acc: 0.843750]\n",
            "1663: [Discriminator loss: 0.514813, acc: 0.691406]  [Adversarial loss: 0.380747, acc: 0.796875]\n",
            "1664: [Discriminator loss: 0.526048, acc: 0.640625]  [Adversarial loss: 0.346228, acc: 0.828125]\n",
            "1665: [Discriminator loss: 0.482738, acc: 0.652344]  [Adversarial loss: 0.348036, acc: 0.843750]\n",
            "1666: [Discriminator loss: 0.607954, acc: 0.574219]  [Adversarial loss: 0.440948, acc: 0.785156]\n",
            "1667: [Discriminator loss: 0.647025, acc: 0.593750]  [Adversarial loss: 0.364516, acc: 0.824219]\n",
            "1668: [Discriminator loss: 0.614541, acc: 0.609375]  [Adversarial loss: 0.379410, acc: 0.812500]\n",
            "1669: [Discriminator loss: 0.622483, acc: 0.589844]  [Adversarial loss: 0.406909, acc: 0.824219]\n",
            "1670: [Discriminator loss: 0.527123, acc: 0.640625]  [Adversarial loss: 0.378454, acc: 0.828125]\n",
            "1671: [Discriminator loss: 0.526933, acc: 0.679688]  [Adversarial loss: 0.348428, acc: 0.832031]\n",
            "1672: [Discriminator loss: 0.554960, acc: 0.625000]  [Adversarial loss: 0.399848, acc: 0.800781]\n",
            "1673: [Discriminator loss: 0.590445, acc: 0.589844]  [Adversarial loss: 0.422071, acc: 0.765625]\n",
            "1674: [Discriminator loss: 0.530786, acc: 0.621094]  [Adversarial loss: 0.405754, acc: 0.804688]\n",
            "1675: [Discriminator loss: 0.547145, acc: 0.621094]  [Adversarial loss: 0.367643, acc: 0.816406]\n",
            "1676: [Discriminator loss: 0.608312, acc: 0.558594]  [Adversarial loss: 0.371796, acc: 0.796875]\n",
            "1677: [Discriminator loss: 0.506267, acc: 0.636719]  [Adversarial loss: 0.395372, acc: 0.832031]\n",
            "1678: [Discriminator loss: 0.568197, acc: 0.609375]  [Adversarial loss: 0.375782, acc: 0.824219]\n",
            "1679: [Discriminator loss: 0.645510, acc: 0.550781]  [Adversarial loss: 0.411712, acc: 0.796875]\n",
            "1680: [Discriminator loss: 0.561697, acc: 0.578125]  [Adversarial loss: 0.381347, acc: 0.808594]\n",
            "1681: [Discriminator loss: 0.551967, acc: 0.644531]  [Adversarial loss: 0.405114, acc: 0.808594]\n",
            "1682: [Discriminator loss: 0.483201, acc: 0.652344]  [Adversarial loss: 0.360023, acc: 0.808594]\n",
            "1683: [Discriminator loss: 0.542991, acc: 0.652344]  [Adversarial loss: 0.381386, acc: 0.816406]\n",
            "1684: [Discriminator loss: 0.571314, acc: 0.625000]  [Adversarial loss: 0.374685, acc: 0.816406]\n",
            "1685: [Discriminator loss: 0.519462, acc: 0.636719]  [Adversarial loss: 0.388473, acc: 0.828125]\n",
            "1686: [Discriminator loss: 0.620681, acc: 0.593750]  [Adversarial loss: 0.376389, acc: 0.781250]\n",
            "1687: [Discriminator loss: 0.539093, acc: 0.625000]  [Adversarial loss: 0.383455, acc: 0.828125]\n",
            "1688: [Discriminator loss: 0.521813, acc: 0.632812]  [Adversarial loss: 0.398111, acc: 0.792969]\n",
            "1689: [Discriminator loss: 0.607419, acc: 0.574219]  [Adversarial loss: 0.424888, acc: 0.785156]\n",
            "1690: [Discriminator loss: 0.603266, acc: 0.597656]  [Adversarial loss: 0.395832, acc: 0.800781]\n",
            "1691: [Discriminator loss: 0.557504, acc: 0.617188]  [Adversarial loss: 0.410238, acc: 0.789062]\n",
            "1692: [Discriminator loss: 0.661448, acc: 0.558594]  [Adversarial loss: 0.418951, acc: 0.792969]\n",
            "1693: [Discriminator loss: 0.584895, acc: 0.640625]  [Adversarial loss: 0.367921, acc: 0.800781]\n",
            "1694: [Discriminator loss: 0.593663, acc: 0.578125]  [Adversarial loss: 0.387680, acc: 0.835938]\n",
            "1695: [Discriminator loss: 0.549774, acc: 0.601562]  [Adversarial loss: 0.369168, acc: 0.835938]\n",
            "1696: [Discriminator loss: 0.502984, acc: 0.636719]  [Adversarial loss: 0.365454, acc: 0.785156]\n",
            "1697: [Discriminator loss: 0.591280, acc: 0.597656]  [Adversarial loss: 0.400471, acc: 0.781250]\n",
            "1698: [Discriminator loss: 0.511301, acc: 0.660156]  [Adversarial loss: 0.383280, acc: 0.781250]\n",
            "1699: [Discriminator loss: 0.500630, acc: 0.660156]  [Adversarial loss: 0.373241, acc: 0.839844]\n",
            "1700: [Discriminator loss: 0.541269, acc: 0.582031]  [Adversarial loss: 0.385854, acc: 0.796875]\n",
            "1701: [Discriminator loss: 0.515192, acc: 0.656250]  [Adversarial loss: 0.351521, acc: 0.816406]\n",
            "1702: [Discriminator loss: 0.622207, acc: 0.589844]  [Adversarial loss: 0.391740, acc: 0.812500]\n",
            "1703: [Discriminator loss: 0.582109, acc: 0.613281]  [Adversarial loss: 0.404114, acc: 0.816406]\n",
            "1704: [Discriminator loss: 0.518563, acc: 0.589844]  [Adversarial loss: 0.379002, acc: 0.820312]\n",
            "1705: [Discriminator loss: 0.535439, acc: 0.632812]  [Adversarial loss: 0.361949, acc: 0.828125]\n",
            "1706: [Discriminator loss: 0.539362, acc: 0.609375]  [Adversarial loss: 0.421372, acc: 0.773438]\n",
            "1707: [Discriminator loss: 0.562705, acc: 0.585938]  [Adversarial loss: 0.385409, acc: 0.824219]\n",
            "1708: [Discriminator loss: 0.615265, acc: 0.558594]  [Adversarial loss: 0.389975, acc: 0.812500]\n",
            "1709: [Discriminator loss: 0.550114, acc: 0.625000]  [Adversarial loss: 0.402438, acc: 0.789062]\n",
            "1710: [Discriminator loss: 0.579002, acc: 0.613281]  [Adversarial loss: 0.389314, acc: 0.835938]\n",
            "1711: [Discriminator loss: 0.603292, acc: 0.578125]  [Adversarial loss: 0.430935, acc: 0.789062]\n",
            "1712: [Discriminator loss: 0.628900, acc: 0.605469]  [Adversarial loss: 0.399245, acc: 0.816406]\n",
            "1713: [Discriminator loss: 0.569135, acc: 0.593750]  [Adversarial loss: 0.393178, acc: 0.824219]\n",
            "1714: [Discriminator loss: 0.532323, acc: 0.625000]  [Adversarial loss: 0.389287, acc: 0.812500]\n",
            "1715: [Discriminator loss: 0.591549, acc: 0.625000]  [Adversarial loss: 0.350365, acc: 0.867188]\n",
            "1716: [Discriminator loss: 0.625644, acc: 0.593750]  [Adversarial loss: 0.443123, acc: 0.796875]\n",
            "1717: [Discriminator loss: 0.574027, acc: 0.558594]  [Adversarial loss: 0.401645, acc: 0.800781]\n",
            "1718: [Discriminator loss: 0.605126, acc: 0.585938]  [Adversarial loss: 0.427033, acc: 0.800781]\n",
            "1719: [Discriminator loss: 0.669953, acc: 0.523438]  [Adversarial loss: 0.413246, acc: 0.773438]\n",
            "1720: [Discriminator loss: 0.575946, acc: 0.558594]  [Adversarial loss: 0.397547, acc: 0.761719]\n",
            "1721: [Discriminator loss: 0.655996, acc: 0.617188]  [Adversarial loss: 0.379864, acc: 0.808594]\n",
            "1722: [Discriminator loss: 0.571181, acc: 0.601562]  [Adversarial loss: 0.366456, acc: 0.847656]\n",
            "1723: [Discriminator loss: 0.556821, acc: 0.617188]  [Adversarial loss: 0.405034, acc: 0.792969]\n",
            "1724: [Discriminator loss: 0.586144, acc: 0.609375]  [Adversarial loss: 0.406529, acc: 0.773438]\n",
            "1725: [Discriminator loss: 0.623339, acc: 0.582031]  [Adversarial loss: 0.394116, acc: 0.832031]\n",
            "1726: [Discriminator loss: 0.559583, acc: 0.625000]  [Adversarial loss: 0.398729, acc: 0.816406]\n",
            "1727: [Discriminator loss: 0.525360, acc: 0.621094]  [Adversarial loss: 0.412013, acc: 0.777344]\n",
            "1728: [Discriminator loss: 0.559518, acc: 0.593750]  [Adversarial loss: 0.430120, acc: 0.789062]\n",
            "1729: [Discriminator loss: 0.558203, acc: 0.628906]  [Adversarial loss: 0.395769, acc: 0.812500]\n",
            "1730: [Discriminator loss: 0.601831, acc: 0.593750]  [Adversarial loss: 0.394157, acc: 0.828125]\n",
            "1731: [Discriminator loss: 0.567566, acc: 0.609375]  [Adversarial loss: 0.400620, acc: 0.785156]\n",
            "1732: [Discriminator loss: 0.511913, acc: 0.644531]  [Adversarial loss: 0.392070, acc: 0.828125]\n",
            "1733: [Discriminator loss: 0.552970, acc: 0.660156]  [Adversarial loss: 0.392317, acc: 0.789062]\n",
            "1734: [Discriminator loss: 0.604227, acc: 0.609375]  [Adversarial loss: 0.418404, acc: 0.820312]\n",
            "1735: [Discriminator loss: 0.615400, acc: 0.578125]  [Adversarial loss: 0.484361, acc: 0.738281]\n",
            "1736: [Discriminator loss: 0.621212, acc: 0.582031]  [Adversarial loss: 0.399898, acc: 0.808594]\n",
            "1737: [Discriminator loss: 0.596275, acc: 0.605469]  [Adversarial loss: 0.396709, acc: 0.781250]\n",
            "1738: [Discriminator loss: 0.631057, acc: 0.597656]  [Adversarial loss: 0.386216, acc: 0.843750]\n",
            "1739: [Discriminator loss: 0.563654, acc: 0.609375]  [Adversarial loss: 0.389975, acc: 0.800781]\n",
            "1740: [Discriminator loss: 0.630311, acc: 0.546875]  [Adversarial loss: 0.450628, acc: 0.808594]\n",
            "1741: [Discriminator loss: 0.524733, acc: 0.656250]  [Adversarial loss: 0.378747, acc: 0.804688]\n",
            "1742: [Discriminator loss: 0.595353, acc: 0.613281]  [Adversarial loss: 0.426865, acc: 0.789062]\n",
            "1743: [Discriminator loss: 0.589021, acc: 0.570312]  [Adversarial loss: 0.409836, acc: 0.796875]\n",
            "1744: [Discriminator loss: 0.618695, acc: 0.593750]  [Adversarial loss: 0.417570, acc: 0.781250]\n",
            "1745: [Discriminator loss: 0.543056, acc: 0.679688]  [Adversarial loss: 0.413213, acc: 0.789062]\n",
            "1746: [Discriminator loss: 0.536112, acc: 0.644531]  [Adversarial loss: 0.363816, acc: 0.835938]\n",
            "1747: [Discriminator loss: 0.560219, acc: 0.621094]  [Adversarial loss: 0.360047, acc: 0.820312]\n",
            "1748: [Discriminator loss: 0.623327, acc: 0.636719]  [Adversarial loss: 0.328772, acc: 0.863281]\n",
            "1749: [Discriminator loss: 0.589731, acc: 0.617188]  [Adversarial loss: 0.403113, acc: 0.800781]\n",
            "1750: [Discriminator loss: 0.591195, acc: 0.617188]  [Adversarial loss: 0.361400, acc: 0.855469]\n",
            "1751: [Discriminator loss: 0.582617, acc: 0.617188]  [Adversarial loss: 0.350036, acc: 0.828125]\n",
            "1752: [Discriminator loss: 0.616063, acc: 0.601562]  [Adversarial loss: 0.389516, acc: 0.816406]\n",
            "1753: [Discriminator loss: 0.557821, acc: 0.609375]  [Adversarial loss: 0.338835, acc: 0.835938]\n",
            "1754: [Discriminator loss: 0.651874, acc: 0.593750]  [Adversarial loss: 0.403063, acc: 0.785156]\n",
            "1755: [Discriminator loss: 0.567785, acc: 0.683594]  [Adversarial loss: 0.380358, acc: 0.773438]\n",
            "1756: [Discriminator loss: 0.593161, acc: 0.605469]  [Adversarial loss: 0.382865, acc: 0.757812]\n",
            "1757: [Discriminator loss: 0.657792, acc: 0.578125]  [Adversarial loss: 0.400232, acc: 0.796875]\n",
            "1758: [Discriminator loss: 0.616639, acc: 0.578125]  [Adversarial loss: 0.399300, acc: 0.785156]\n",
            "1759: [Discriminator loss: 0.560236, acc: 0.679688]  [Adversarial loss: 0.419206, acc: 0.777344]\n",
            "1760: [Discriminator loss: 0.591037, acc: 0.597656]  [Adversarial loss: 0.411989, acc: 0.816406]\n",
            "1761: [Discriminator loss: 0.560482, acc: 0.605469]  [Adversarial loss: 0.401968, acc: 0.824219]\n",
            "1762: [Discriminator loss: 0.607731, acc: 0.574219]  [Adversarial loss: 0.438191, acc: 0.789062]\n",
            "1763: [Discriminator loss: 0.536990, acc: 0.695312]  [Adversarial loss: 0.350064, acc: 0.843750]\n",
            "1764: [Discriminator loss: 0.576627, acc: 0.597656]  [Adversarial loss: 0.401706, acc: 0.800781]\n",
            "1765: [Discriminator loss: 0.638596, acc: 0.621094]  [Adversarial loss: 0.373705, acc: 0.804688]\n",
            "1766: [Discriminator loss: 0.586636, acc: 0.562500]  [Adversarial loss: 0.419085, acc: 0.800781]\n",
            "1767: [Discriminator loss: 0.610656, acc: 0.554688]  [Adversarial loss: 0.378892, acc: 0.808594]\n",
            "1768: [Discriminator loss: 0.591841, acc: 0.582031]  [Adversarial loss: 0.382649, acc: 0.808594]\n",
            "1769: [Discriminator loss: 0.588667, acc: 0.617188]  [Adversarial loss: 0.356971, acc: 0.828125]\n",
            "1770: [Discriminator loss: 0.548733, acc: 0.640625]  [Adversarial loss: 0.371103, acc: 0.789062]\n",
            "1771: [Discriminator loss: 0.631381, acc: 0.574219]  [Adversarial loss: 0.348500, acc: 0.839844]\n",
            "1772: [Discriminator loss: 0.544718, acc: 0.605469]  [Adversarial loss: 0.381739, acc: 0.796875]\n",
            "1773: [Discriminator loss: 0.572473, acc: 0.632812]  [Adversarial loss: 0.384221, acc: 0.843750]\n",
            "1774: [Discriminator loss: 0.552832, acc: 0.667969]  [Adversarial loss: 0.389891, acc: 0.812500]\n",
            "1775: [Discriminator loss: 0.531192, acc: 0.621094]  [Adversarial loss: 0.360812, acc: 0.855469]\n",
            "1776: [Discriminator loss: 0.575307, acc: 0.609375]  [Adversarial loss: 0.420306, acc: 0.765625]\n",
            "1777: [Discriminator loss: 0.616501, acc: 0.566406]  [Adversarial loss: 0.395151, acc: 0.816406]\n",
            "1778: [Discriminator loss: 0.539186, acc: 0.601562]  [Adversarial loss: 0.348515, acc: 0.855469]\n",
            "1779: [Discriminator loss: 0.536917, acc: 0.671875]  [Adversarial loss: 0.362541, acc: 0.832031]\n",
            "1780: [Discriminator loss: 0.579495, acc: 0.605469]  [Adversarial loss: 0.441679, acc: 0.769531]\n",
            "1781: [Discriminator loss: 0.554806, acc: 0.628906]  [Adversarial loss: 0.409733, acc: 0.816406]\n",
            "1782: [Discriminator loss: 0.550287, acc: 0.593750]  [Adversarial loss: 0.363831, acc: 0.808594]\n",
            "1783: [Discriminator loss: 0.634582, acc: 0.617188]  [Adversarial loss: 0.374362, acc: 0.812500]\n",
            "1784: [Discriminator loss: 0.593505, acc: 0.605469]  [Adversarial loss: 0.324119, acc: 0.851562]\n",
            "1785: [Discriminator loss: 0.619630, acc: 0.597656]  [Adversarial loss: 0.413080, acc: 0.789062]\n",
            "1786: [Discriminator loss: 0.638164, acc: 0.550781]  [Adversarial loss: 0.417274, acc: 0.820312]\n",
            "1787: [Discriminator loss: 0.551877, acc: 0.625000]  [Adversarial loss: 0.371917, acc: 0.824219]\n",
            "1788: [Discriminator loss: 0.603806, acc: 0.574219]  [Adversarial loss: 0.414620, acc: 0.808594]\n",
            "1789: [Discriminator loss: 0.538123, acc: 0.628906]  [Adversarial loss: 0.362239, acc: 0.843750]\n",
            "1790: [Discriminator loss: 0.557479, acc: 0.621094]  [Adversarial loss: 0.361300, acc: 0.812500]\n",
            "1791: [Discriminator loss: 0.599429, acc: 0.558594]  [Adversarial loss: 0.438236, acc: 0.789062]\n",
            "1792: [Discriminator loss: 0.570170, acc: 0.597656]  [Adversarial loss: 0.355334, acc: 0.820312]\n",
            "1793: [Discriminator loss: 0.527866, acc: 0.644531]  [Adversarial loss: 0.362462, acc: 0.835938]\n",
            "1794: [Discriminator loss: 0.567662, acc: 0.625000]  [Adversarial loss: 0.409066, acc: 0.800781]\n",
            "1795: [Discriminator loss: 0.631364, acc: 0.589844]  [Adversarial loss: 0.411010, acc: 0.796875]\n",
            "1796: [Discriminator loss: 0.603288, acc: 0.632812]  [Adversarial loss: 0.392504, acc: 0.824219]\n",
            "1797: [Discriminator loss: 0.533486, acc: 0.660156]  [Adversarial loss: 0.378200, acc: 0.832031]\n",
            "1798: [Discriminator loss: 0.539415, acc: 0.628906]  [Adversarial loss: 0.386892, acc: 0.835938]\n",
            "1799: [Discriminator loss: 0.633621, acc: 0.511719]  [Adversarial loss: 0.358094, acc: 0.847656]\n",
            "1800: [Discriminator loss: 0.624964, acc: 0.593750]  [Adversarial loss: 0.354492, acc: 0.847656]\n",
            "1801: [Discriminator loss: 0.627189, acc: 0.582031]  [Adversarial loss: 0.380209, acc: 0.824219]\n",
            "1802: [Discriminator loss: 0.676995, acc: 0.570312]  [Adversarial loss: 0.426402, acc: 0.816406]\n",
            "1803: [Discriminator loss: 0.609595, acc: 0.585938]  [Adversarial loss: 0.382741, acc: 0.796875]\n",
            "1804: [Discriminator loss: 0.653256, acc: 0.539062]  [Adversarial loss: 0.402646, acc: 0.816406]\n",
            "1805: [Discriminator loss: 0.563847, acc: 0.601562]  [Adversarial loss: 0.391032, acc: 0.796875]\n",
            "1806: [Discriminator loss: 0.600894, acc: 0.585938]  [Adversarial loss: 0.399322, acc: 0.851562]\n",
            "1807: [Discriminator loss: 0.611160, acc: 0.597656]  [Adversarial loss: 0.351466, acc: 0.847656]\n",
            "1808: [Discriminator loss: 0.640484, acc: 0.613281]  [Adversarial loss: 0.399417, acc: 0.832031]\n",
            "1809: [Discriminator loss: 0.563569, acc: 0.617188]  [Adversarial loss: 0.359494, acc: 0.828125]\n",
            "1810: [Discriminator loss: 0.577540, acc: 0.605469]  [Adversarial loss: 0.409037, acc: 0.816406]\n",
            "1811: [Discriminator loss: 0.549322, acc: 0.609375]  [Adversarial loss: 0.358638, acc: 0.820312]\n",
            "1812: [Discriminator loss: 0.608906, acc: 0.589844]  [Adversarial loss: 0.365316, acc: 0.816406]\n",
            "1813: [Discriminator loss: 0.571480, acc: 0.582031]  [Adversarial loss: 0.344377, acc: 0.832031]\n",
            "1814: [Discriminator loss: 0.522601, acc: 0.558594]  [Adversarial loss: 0.411795, acc: 0.789062]\n",
            "1815: [Discriminator loss: 0.606429, acc: 0.566406]  [Adversarial loss: 0.432545, acc: 0.792969]\n",
            "1816: [Discriminator loss: 0.595320, acc: 0.566406]  [Adversarial loss: 0.414109, acc: 0.781250]\n",
            "1817: [Discriminator loss: 0.623903, acc: 0.578125]  [Adversarial loss: 0.479511, acc: 0.777344]\n",
            "1818: [Discriminator loss: 0.577461, acc: 0.562500]  [Adversarial loss: 0.388629, acc: 0.832031]\n",
            "1819: [Discriminator loss: 0.585542, acc: 0.652344]  [Adversarial loss: 0.397572, acc: 0.804688]\n",
            "1820: [Discriminator loss: 0.559916, acc: 0.640625]  [Adversarial loss: 0.343904, acc: 0.851562]\n",
            "1821: [Discriminator loss: 0.465781, acc: 0.675781]  [Adversarial loss: 0.326713, acc: 0.847656]\n",
            "1822: [Discriminator loss: 0.589247, acc: 0.601562]  [Adversarial loss: 0.423844, acc: 0.765625]\n",
            "1823: [Discriminator loss: 0.542765, acc: 0.621094]  [Adversarial loss: 0.394199, acc: 0.812500]\n",
            "1824: [Discriminator loss: 0.634828, acc: 0.589844]  [Adversarial loss: 0.387218, acc: 0.796875]\n",
            "1825: [Discriminator loss: 0.580830, acc: 0.613281]  [Adversarial loss: 0.379992, acc: 0.832031]\n",
            "1826: [Discriminator loss: 0.597000, acc: 0.554688]  [Adversarial loss: 0.387556, acc: 0.800781]\n",
            "1827: [Discriminator loss: 0.513282, acc: 0.621094]  [Adversarial loss: 0.357481, acc: 0.804688]\n",
            "1828: [Discriminator loss: 0.555236, acc: 0.593750]  [Adversarial loss: 0.414064, acc: 0.777344]\n",
            "1829: [Discriminator loss: 0.586689, acc: 0.652344]  [Adversarial loss: 0.343876, acc: 0.808594]\n",
            "1830: [Discriminator loss: 0.604338, acc: 0.574219]  [Adversarial loss: 0.372175, acc: 0.808594]\n",
            "1831: [Discriminator loss: 0.505615, acc: 0.671875]  [Adversarial loss: 0.390979, acc: 0.820312]\n",
            "1832: [Discriminator loss: 0.561985, acc: 0.621094]  [Adversarial loss: 0.401903, acc: 0.800781]\n",
            "1833: [Discriminator loss: 0.495048, acc: 0.660156]  [Adversarial loss: 0.374789, acc: 0.832031]\n",
            "1834: [Discriminator loss: 0.589021, acc: 0.617188]  [Adversarial loss: 0.409240, acc: 0.812500]\n",
            "1835: [Discriminator loss: 0.600608, acc: 0.566406]  [Adversarial loss: 0.403794, acc: 0.789062]\n",
            "1836: [Discriminator loss: 0.518961, acc: 0.628906]  [Adversarial loss: 0.379200, acc: 0.824219]\n",
            "1837: [Discriminator loss: 0.644187, acc: 0.589844]  [Adversarial loss: 0.361708, acc: 0.843750]\n",
            "1838: [Discriminator loss: 0.596747, acc: 0.597656]  [Adversarial loss: 0.382265, acc: 0.796875]\n",
            "1839: [Discriminator loss: 0.577073, acc: 0.617188]  [Adversarial loss: 0.398105, acc: 0.816406]\n",
            "1840: [Discriminator loss: 0.572522, acc: 0.597656]  [Adversarial loss: 0.416426, acc: 0.773438]\n",
            "1841: [Discriminator loss: 0.543989, acc: 0.617188]  [Adversarial loss: 0.367983, acc: 0.820312]\n",
            "1842: [Discriminator loss: 0.547606, acc: 0.609375]  [Adversarial loss: 0.390873, acc: 0.816406]\n",
            "1843: [Discriminator loss: 0.551794, acc: 0.648438]  [Adversarial loss: 0.377690, acc: 0.835938]\n",
            "1844: [Discriminator loss: 0.622476, acc: 0.578125]  [Adversarial loss: 0.330754, acc: 0.867188]\n",
            "1845: [Discriminator loss: 0.544073, acc: 0.628906]  [Adversarial loss: 0.379756, acc: 0.808594]\n",
            "1846: [Discriminator loss: 0.576761, acc: 0.628906]  [Adversarial loss: 0.422608, acc: 0.800781]\n",
            "1847: [Discriminator loss: 0.596093, acc: 0.597656]  [Adversarial loss: 0.420923, acc: 0.816406]\n",
            "1848: [Discriminator loss: 0.620279, acc: 0.593750]  [Adversarial loss: 0.376657, acc: 0.812500]\n",
            "1849: [Discriminator loss: 0.544331, acc: 0.589844]  [Adversarial loss: 0.409599, acc: 0.773438]\n",
            "1850: [Discriminator loss: 0.585309, acc: 0.617188]  [Adversarial loss: 0.385968, acc: 0.816406]\n",
            "1851: [Discriminator loss: 0.544313, acc: 0.636719]  [Adversarial loss: 0.345514, acc: 0.828125]\n",
            "1852: [Discriminator loss: 0.627578, acc: 0.570312]  [Adversarial loss: 0.429556, acc: 0.796875]\n",
            "1853: [Discriminator loss: 0.600381, acc: 0.589844]  [Adversarial loss: 0.390637, acc: 0.832031]\n",
            "1854: [Discriminator loss: 0.568641, acc: 0.597656]  [Adversarial loss: 0.486544, acc: 0.800781]\n",
            "1855: [Discriminator loss: 0.594756, acc: 0.609375]  [Adversarial loss: 0.395466, acc: 0.789062]\n",
            "1856: [Discriminator loss: 0.537487, acc: 0.609375]  [Adversarial loss: 0.361491, acc: 0.839844]\n",
            "1857: [Discriminator loss: 0.593157, acc: 0.566406]  [Adversarial loss: 0.401412, acc: 0.792969]\n",
            "1858: [Discriminator loss: 0.540982, acc: 0.632812]  [Adversarial loss: 0.387949, acc: 0.808594]\n",
            "1859: [Discriminator loss: 0.617356, acc: 0.617188]  [Adversarial loss: 0.433178, acc: 0.785156]\n",
            "1860: [Discriminator loss: 0.654293, acc: 0.601562]  [Adversarial loss: 0.386084, acc: 0.808594]\n",
            "1861: [Discriminator loss: 0.609196, acc: 0.593750]  [Adversarial loss: 0.387853, acc: 0.835938]\n",
            "1862: [Discriminator loss: 0.565883, acc: 0.609375]  [Adversarial loss: 0.381622, acc: 0.796875]\n",
            "1863: [Discriminator loss: 0.593260, acc: 0.601562]  [Adversarial loss: 0.441594, acc: 0.789062]\n",
            "1864: [Discriminator loss: 0.579177, acc: 0.601562]  [Adversarial loss: 0.409567, acc: 0.800781]\n",
            "1865: [Discriminator loss: 0.486649, acc: 0.675781]  [Adversarial loss: 0.445782, acc: 0.808594]\n",
            "1866: [Discriminator loss: 0.535720, acc: 0.640625]  [Adversarial loss: 0.343080, acc: 0.824219]\n",
            "1867: [Discriminator loss: 0.586660, acc: 0.609375]  [Adversarial loss: 0.403245, acc: 0.816406]\n",
            "1868: [Discriminator loss: 0.510943, acc: 0.632812]  [Adversarial loss: 0.379709, acc: 0.812500]\n",
            "1869: [Discriminator loss: 0.594551, acc: 0.585938]  [Adversarial loss: 0.433298, acc: 0.777344]\n",
            "1870: [Discriminator loss: 0.521339, acc: 0.640625]  [Adversarial loss: 0.331694, acc: 0.855469]\n",
            "1871: [Discriminator loss: 0.551713, acc: 0.585938]  [Adversarial loss: 0.380412, acc: 0.800781]\n",
            "1872: [Discriminator loss: 0.552001, acc: 0.660156]  [Adversarial loss: 0.387156, acc: 0.773438]\n",
            "1873: [Discriminator loss: 0.577622, acc: 0.589844]  [Adversarial loss: 0.387579, acc: 0.824219]\n",
            "1874: [Discriminator loss: 0.555820, acc: 0.613281]  [Adversarial loss: 0.392171, acc: 0.812500]\n",
            "1875: [Discriminator loss: 0.537699, acc: 0.625000]  [Adversarial loss: 0.398028, acc: 0.796875]\n",
            "1876: [Discriminator loss: 0.544098, acc: 0.609375]  [Adversarial loss: 0.390773, acc: 0.792969]\n",
            "1877: [Discriminator loss: 0.601596, acc: 0.605469]  [Adversarial loss: 0.378256, acc: 0.812500]\n",
            "1878: [Discriminator loss: 0.565358, acc: 0.597656]  [Adversarial loss: 0.380093, acc: 0.808594]\n",
            "1879: [Discriminator loss: 0.585984, acc: 0.574219]  [Adversarial loss: 0.376569, acc: 0.843750]\n",
            "1880: [Discriminator loss: 0.574456, acc: 0.585938]  [Adversarial loss: 0.460311, acc: 0.773438]\n",
            "1881: [Discriminator loss: 0.566898, acc: 0.625000]  [Adversarial loss: 0.369829, acc: 0.808594]\n",
            "1882: [Discriminator loss: 0.557716, acc: 0.609375]  [Adversarial loss: 0.449621, acc: 0.796875]\n",
            "1883: [Discriminator loss: 0.577255, acc: 0.582031]  [Adversarial loss: 0.382580, acc: 0.855469]\n",
            "1884: [Discriminator loss: 0.577479, acc: 0.625000]  [Adversarial loss: 0.402555, acc: 0.792969]\n",
            "1885: [Discriminator loss: 0.657743, acc: 0.562500]  [Adversarial loss: 0.434500, acc: 0.820312]\n",
            "1886: [Discriminator loss: 0.592315, acc: 0.593750]  [Adversarial loss: 0.394603, acc: 0.820312]\n",
            "1887: [Discriminator loss: 0.538011, acc: 0.601562]  [Adversarial loss: 0.373244, acc: 0.839844]\n",
            "1888: [Discriminator loss: 0.531268, acc: 0.617188]  [Adversarial loss: 0.320949, acc: 0.871094]\n",
            "1889: [Discriminator loss: 0.572435, acc: 0.593750]  [Adversarial loss: 0.352892, acc: 0.808594]\n",
            "1890: [Discriminator loss: 0.595311, acc: 0.550781]  [Adversarial loss: 0.337582, acc: 0.847656]\n",
            "1891: [Discriminator loss: 0.561234, acc: 0.628906]  [Adversarial loss: 0.375113, acc: 0.828125]\n",
            "1892: [Discriminator loss: 0.563923, acc: 0.597656]  [Adversarial loss: 0.402407, acc: 0.785156]\n",
            "1893: [Discriminator loss: 0.565040, acc: 0.632812]  [Adversarial loss: 0.409949, acc: 0.792969]\n",
            "1894: [Discriminator loss: 0.553177, acc: 0.636719]  [Adversarial loss: 0.360770, acc: 0.855469]\n",
            "1895: [Discriminator loss: 0.536799, acc: 0.605469]  [Adversarial loss: 0.390362, acc: 0.773438]\n",
            "1896: [Discriminator loss: 0.599583, acc: 0.605469]  [Adversarial loss: 0.369964, acc: 0.820312]\n",
            "1897: [Discriminator loss: 0.564822, acc: 0.589844]  [Adversarial loss: 0.442016, acc: 0.734375]\n",
            "1898: [Discriminator loss: 0.580766, acc: 0.578125]  [Adversarial loss: 0.438893, acc: 0.789062]\n",
            "1899: [Discriminator loss: 0.513279, acc: 0.589844]  [Adversarial loss: 0.370426, acc: 0.855469]\n",
            "1900: [Discriminator loss: 0.589353, acc: 0.582031]  [Adversarial loss: 0.409907, acc: 0.761719]\n",
            "1901: [Discriminator loss: 0.566323, acc: 0.628906]  [Adversarial loss: 0.397170, acc: 0.824219]\n",
            "1902: [Discriminator loss: 0.615799, acc: 0.613281]  [Adversarial loss: 0.382804, acc: 0.824219]\n",
            "1903: [Discriminator loss: 0.596624, acc: 0.574219]  [Adversarial loss: 0.407575, acc: 0.796875]\n",
            "1904: [Discriminator loss: 0.608819, acc: 0.582031]  [Adversarial loss: 0.409180, acc: 0.796875]\n",
            "1905: [Discriminator loss: 0.560165, acc: 0.621094]  [Adversarial loss: 0.400040, acc: 0.824219]\n",
            "1906: [Discriminator loss: 0.558122, acc: 0.613281]  [Adversarial loss: 0.406274, acc: 0.792969]\n",
            "1907: [Discriminator loss: 0.550250, acc: 0.648438]  [Adversarial loss: 0.330083, acc: 0.847656]\n",
            "1908: [Discriminator loss: 0.555988, acc: 0.636719]  [Adversarial loss: 0.321806, acc: 0.835938]\n",
            "1909: [Discriminator loss: 0.567809, acc: 0.636719]  [Adversarial loss: 0.404861, acc: 0.800781]\n",
            "1910: [Discriminator loss: 0.571879, acc: 0.617188]  [Adversarial loss: 0.368008, acc: 0.816406]\n",
            "1911: [Discriminator loss: 0.597846, acc: 0.574219]  [Adversarial loss: 0.399253, acc: 0.746094]\n",
            "1912: [Discriminator loss: 0.581644, acc: 0.597656]  [Adversarial loss: 0.337195, acc: 0.804688]\n",
            "1913: [Discriminator loss: 0.547987, acc: 0.636719]  [Adversarial loss: 0.389809, acc: 0.835938]\n",
            "1914: [Discriminator loss: 0.572489, acc: 0.582031]  [Adversarial loss: 0.369823, acc: 0.855469]\n",
            "1915: [Discriminator loss: 0.595872, acc: 0.601562]  [Adversarial loss: 0.405849, acc: 0.812500]\n",
            "1916: [Discriminator loss: 0.642926, acc: 0.582031]  [Adversarial loss: 0.396333, acc: 0.789062]\n",
            "1917: [Discriminator loss: 0.586488, acc: 0.574219]  [Adversarial loss: 0.353309, acc: 0.808594]\n",
            "1918: [Discriminator loss: 0.654072, acc: 0.582031]  [Adversarial loss: 0.404243, acc: 0.777344]\n",
            "1919: [Discriminator loss: 0.582481, acc: 0.589844]  [Adversarial loss: 0.399734, acc: 0.808594]\n",
            "1920: [Discriminator loss: 0.558858, acc: 0.640625]  [Adversarial loss: 0.360659, acc: 0.847656]\n",
            "1921: [Discriminator loss: 0.677614, acc: 0.593750]  [Adversarial loss: 0.406368, acc: 0.792969]\n",
            "1922: [Discriminator loss: 0.582088, acc: 0.593750]  [Adversarial loss: 0.418130, acc: 0.792969]\n",
            "1923: [Discriminator loss: 0.546882, acc: 0.648438]  [Adversarial loss: 0.378272, acc: 0.804688]\n",
            "1924: [Discriminator loss: 0.530424, acc: 0.648438]  [Adversarial loss: 0.336879, acc: 0.859375]\n",
            "1925: [Discriminator loss: 0.577599, acc: 0.585938]  [Adversarial loss: 0.341514, acc: 0.863281]\n",
            "1926: [Discriminator loss: 0.568080, acc: 0.628906]  [Adversarial loss: 0.377655, acc: 0.812500]\n",
            "1927: [Discriminator loss: 0.583760, acc: 0.578125]  [Adversarial loss: 0.375466, acc: 0.839844]\n",
            "1928: [Discriminator loss: 0.568456, acc: 0.621094]  [Adversarial loss: 0.378159, acc: 0.800781]\n",
            "1929: [Discriminator loss: 0.591715, acc: 0.593750]  [Adversarial loss: 0.376470, acc: 0.832031]\n",
            "1930: [Discriminator loss: 0.567805, acc: 0.593750]  [Adversarial loss: 0.380797, acc: 0.824219]\n",
            "1931: [Discriminator loss: 0.563735, acc: 0.589844]  [Adversarial loss: 0.442848, acc: 0.804688]\n",
            "1932: [Discriminator loss: 0.571990, acc: 0.628906]  [Adversarial loss: 0.423803, acc: 0.839844]\n",
            "1933: [Discriminator loss: 0.589210, acc: 0.585938]  [Adversarial loss: 0.430194, acc: 0.785156]\n",
            "1934: [Discriminator loss: 0.563839, acc: 0.628906]  [Adversarial loss: 0.388256, acc: 0.812500]\n",
            "1935: [Discriminator loss: 0.606694, acc: 0.605469]  [Adversarial loss: 0.415347, acc: 0.816406]\n",
            "1936: [Discriminator loss: 0.566292, acc: 0.625000]  [Adversarial loss: 0.362357, acc: 0.835938]\n",
            "1937: [Discriminator loss: 0.580939, acc: 0.605469]  [Adversarial loss: 0.395133, acc: 0.777344]\n",
            "1938: [Discriminator loss: 0.592567, acc: 0.570312]  [Adversarial loss: 0.342244, acc: 0.824219]\n",
            "1939: [Discriminator loss: 0.594997, acc: 0.617188]  [Adversarial loss: 0.325379, acc: 0.839844]\n",
            "1940: [Discriminator loss: 0.522043, acc: 0.656250]  [Adversarial loss: 0.352805, acc: 0.808594]\n",
            "1941: [Discriminator loss: 0.611452, acc: 0.621094]  [Adversarial loss: 0.471642, acc: 0.753906]\n",
            "1942: [Discriminator loss: 0.551548, acc: 0.605469]  [Adversarial loss: 0.368343, acc: 0.824219]\n",
            "1943: [Discriminator loss: 0.614586, acc: 0.613281]  [Adversarial loss: 0.367335, acc: 0.792969]\n",
            "1944: [Discriminator loss: 0.627152, acc: 0.562500]  [Adversarial loss: 0.390884, acc: 0.812500]\n",
            "1945: [Discriminator loss: 0.622637, acc: 0.601562]  [Adversarial loss: 0.366673, acc: 0.835938]\n",
            "1946: [Discriminator loss: 0.629231, acc: 0.617188]  [Adversarial loss: 0.346146, acc: 0.824219]\n",
            "1947: [Discriminator loss: 0.603154, acc: 0.585938]  [Adversarial loss: 0.364231, acc: 0.792969]\n",
            "1948: [Discriminator loss: 0.543268, acc: 0.632812]  [Adversarial loss: 0.347912, acc: 0.785156]\n",
            "1949: [Discriminator loss: 0.578810, acc: 0.628906]  [Adversarial loss: 0.422129, acc: 0.781250]\n",
            "1950: [Discriminator loss: 0.558628, acc: 0.617188]  [Adversarial loss: 0.386772, acc: 0.847656]\n",
            "1951: [Discriminator loss: 0.558325, acc: 0.621094]  [Adversarial loss: 0.353763, acc: 0.839844]\n",
            "1952: [Discriminator loss: 0.598789, acc: 0.546875]  [Adversarial loss: 0.429966, acc: 0.804688]\n",
            "1953: [Discriminator loss: 0.557910, acc: 0.644531]  [Adversarial loss: 0.359193, acc: 0.843750]\n",
            "1954: [Discriminator loss: 0.602010, acc: 0.570312]  [Adversarial loss: 0.476342, acc: 0.804688]\n",
            "1955: [Discriminator loss: 0.589701, acc: 0.589844]  [Adversarial loss: 0.407531, acc: 0.800781]\n",
            "1956: [Discriminator loss: 0.587059, acc: 0.574219]  [Adversarial loss: 0.426297, acc: 0.769531]\n",
            "1957: [Discriminator loss: 0.603655, acc: 0.628906]  [Adversarial loss: 0.366276, acc: 0.808594]\n",
            "1958: [Discriminator loss: 0.569079, acc: 0.605469]  [Adversarial loss: 0.395488, acc: 0.820312]\n",
            "1959: [Discriminator loss: 0.558235, acc: 0.640625]  [Adversarial loss: 0.399323, acc: 0.781250]\n",
            "1960: [Discriminator loss: 0.611248, acc: 0.621094]  [Adversarial loss: 0.375845, acc: 0.796875]\n",
            "1961: [Discriminator loss: 0.594905, acc: 0.582031]  [Adversarial loss: 0.404759, acc: 0.789062]\n",
            "1962: [Discriminator loss: 0.609506, acc: 0.589844]  [Adversarial loss: 0.424567, acc: 0.792969]\n",
            "1963: [Discriminator loss: 0.611066, acc: 0.593750]  [Adversarial loss: 0.391175, acc: 0.777344]\n",
            "1964: [Discriminator loss: 0.523048, acc: 0.632812]  [Adversarial loss: 0.357165, acc: 0.832031]\n",
            "1965: [Discriminator loss: 0.523431, acc: 0.636719]  [Adversarial loss: 0.381130, acc: 0.816406]\n",
            "1966: [Discriminator loss: 0.569058, acc: 0.636719]  [Adversarial loss: 0.381631, acc: 0.785156]\n",
            "1967: [Discriminator loss: 0.622701, acc: 0.574219]  [Adversarial loss: 0.396184, acc: 0.812500]\n",
            "1968: [Discriminator loss: 0.609702, acc: 0.585938]  [Adversarial loss: 0.424087, acc: 0.773438]\n",
            "1969: [Discriminator loss: 0.545871, acc: 0.613281]  [Adversarial loss: 0.355180, acc: 0.816406]\n",
            "1970: [Discriminator loss: 0.582379, acc: 0.605469]  [Adversarial loss: 0.452516, acc: 0.792969]\n",
            "1971: [Discriminator loss: 0.572261, acc: 0.589844]  [Adversarial loss: 0.356209, acc: 0.804688]\n",
            "1972: [Discriminator loss: 0.592132, acc: 0.601562]  [Adversarial loss: 0.368780, acc: 0.808594]\n",
            "1973: [Discriminator loss: 0.573334, acc: 0.632812]  [Adversarial loss: 0.373025, acc: 0.796875]\n",
            "1974: [Discriminator loss: 0.555754, acc: 0.660156]  [Adversarial loss: 0.335667, acc: 0.847656]\n",
            "1975: [Discriminator loss: 0.535862, acc: 0.644531]  [Adversarial loss: 0.369581, acc: 0.804688]\n",
            "1976: [Discriminator loss: 0.572978, acc: 0.625000]  [Adversarial loss: 0.394058, acc: 0.792969]\n",
            "1977: [Discriminator loss: 0.560066, acc: 0.609375]  [Adversarial loss: 0.384166, acc: 0.820312]\n",
            "1978: [Discriminator loss: 0.556721, acc: 0.617188]  [Adversarial loss: 0.334552, acc: 0.839844]\n",
            "1979: [Discriminator loss: 0.586386, acc: 0.617188]  [Adversarial loss: 0.370529, acc: 0.824219]\n",
            "1980: [Discriminator loss: 0.558395, acc: 0.648438]  [Adversarial loss: 0.368419, acc: 0.796875]\n",
            "1981: [Discriminator loss: 0.560221, acc: 0.605469]  [Adversarial loss: 0.460761, acc: 0.781250]\n",
            "1982: [Discriminator loss: 0.517102, acc: 0.632812]  [Adversarial loss: 0.416761, acc: 0.808594]\n",
            "1983: [Discriminator loss: 0.552345, acc: 0.585938]  [Adversarial loss: 0.379916, acc: 0.777344]\n",
            "1984: [Discriminator loss: 0.590869, acc: 0.632812]  [Adversarial loss: 0.421385, acc: 0.792969]\n",
            "1985: [Discriminator loss: 0.580369, acc: 0.660156]  [Adversarial loss: 0.394290, acc: 0.796875]\n",
            "1986: [Discriminator loss: 0.629223, acc: 0.585938]  [Adversarial loss: 0.399296, acc: 0.785156]\n",
            "1987: [Discriminator loss: 0.658044, acc: 0.562500]  [Adversarial loss: 0.439965, acc: 0.761719]\n",
            "1988: [Discriminator loss: 0.550254, acc: 0.609375]  [Adversarial loss: 0.371095, acc: 0.808594]\n",
            "1989: [Discriminator loss: 0.621483, acc: 0.617188]  [Adversarial loss: 0.471077, acc: 0.753906]\n",
            "1990: [Discriminator loss: 0.540387, acc: 0.660156]  [Adversarial loss: 0.378330, acc: 0.808594]\n",
            "1991: [Discriminator loss: 0.526967, acc: 0.621094]  [Adversarial loss: 0.388482, acc: 0.792969]\n",
            "1992: [Discriminator loss: 0.598217, acc: 0.582031]  [Adversarial loss: 0.427883, acc: 0.808594]\n",
            "1993: [Discriminator loss: 0.494543, acc: 0.675781]  [Adversarial loss: 0.349723, acc: 0.824219]\n",
            "1994: [Discriminator loss: 0.571907, acc: 0.566406]  [Adversarial loss: 0.382322, acc: 0.777344]\n",
            "1995: [Discriminator loss: 0.556848, acc: 0.660156]  [Adversarial loss: 0.356578, acc: 0.828125]\n",
            "1996: [Discriminator loss: 0.566918, acc: 0.605469]  [Adversarial loss: 0.418344, acc: 0.792969]\n",
            "1997: [Discriminator loss: 0.558479, acc: 0.644531]  [Adversarial loss: 0.344801, acc: 0.828125]\n",
            "1998: [Discriminator loss: 0.544397, acc: 0.636719]  [Adversarial loss: 0.371492, acc: 0.804688]\n",
            "1999: [Discriminator loss: 0.502447, acc: 0.652344]  [Adversarial loss: 0.356621, acc: 0.812500]\n",
            "2000: [Discriminator loss: 0.532926, acc: 0.582031]  [Adversarial loss: 0.414998, acc: 0.765625]\n",
            "2001: [Discriminator loss: 0.614192, acc: 0.593750]  [Adversarial loss: 0.402667, acc: 0.792969]\n",
            "2002: [Discriminator loss: 0.541816, acc: 0.632812]  [Adversarial loss: 0.383929, acc: 0.808594]\n",
            "2003: [Discriminator loss: 0.529448, acc: 0.609375]  [Adversarial loss: 0.348212, acc: 0.816406]\n",
            "2004: [Discriminator loss: 0.542995, acc: 0.660156]  [Adversarial loss: 0.335247, acc: 0.843750]\n",
            "2005: [Discriminator loss: 0.593693, acc: 0.636719]  [Adversarial loss: 0.406150, acc: 0.808594]\n",
            "2006: [Discriminator loss: 0.573120, acc: 0.613281]  [Adversarial loss: 0.372775, acc: 0.808594]\n",
            "2007: [Discriminator loss: 0.523276, acc: 0.632812]  [Adversarial loss: 0.375213, acc: 0.808594]\n",
            "2008: [Discriminator loss: 0.565986, acc: 0.636719]  [Adversarial loss: 0.327309, acc: 0.871094]\n",
            "2009: [Discriminator loss: 0.506335, acc: 0.636719]  [Adversarial loss: 0.356863, acc: 0.796875]\n",
            "2010: [Discriminator loss: 0.554947, acc: 0.593750]  [Adversarial loss: 0.354891, acc: 0.839844]\n",
            "2011: [Discriminator loss: 0.604861, acc: 0.578125]  [Adversarial loss: 0.346074, acc: 0.828125]\n",
            "2012: [Discriminator loss: 0.543515, acc: 0.625000]  [Adversarial loss: 0.386563, acc: 0.843750]\n",
            "2013: [Discriminator loss: 0.534755, acc: 0.632812]  [Adversarial loss: 0.312589, acc: 0.835938]\n",
            "2014: [Discriminator loss: 0.560400, acc: 0.582031]  [Adversarial loss: 0.342667, acc: 0.828125]\n",
            "2015: [Discriminator loss: 0.598741, acc: 0.550781]  [Adversarial loss: 0.373626, acc: 0.851562]\n",
            "2016: [Discriminator loss: 0.555618, acc: 0.609375]  [Adversarial loss: 0.353847, acc: 0.824219]\n",
            "2017: [Discriminator loss: 0.593259, acc: 0.597656]  [Adversarial loss: 0.357588, acc: 0.796875]\n",
            "2018: [Discriminator loss: 0.555618, acc: 0.605469]  [Adversarial loss: 0.342062, acc: 0.839844]\n",
            "2019: [Discriminator loss: 0.599148, acc: 0.597656]  [Adversarial loss: 0.401751, acc: 0.812500]\n",
            "2020: [Discriminator loss: 0.515113, acc: 0.617188]  [Adversarial loss: 0.308906, acc: 0.875000]\n",
            "2021: [Discriminator loss: 0.617054, acc: 0.558594]  [Adversarial loss: 0.431849, acc: 0.765625]\n",
            "2022: [Discriminator loss: 0.503228, acc: 0.632812]  [Adversarial loss: 0.372440, acc: 0.847656]\n",
            "2023: [Discriminator loss: 0.584019, acc: 0.601562]  [Adversarial loss: 0.382083, acc: 0.832031]\n",
            "2024: [Discriminator loss: 0.547496, acc: 0.621094]  [Adversarial loss: 0.390512, acc: 0.800781]\n",
            "2025: [Discriminator loss: 0.518032, acc: 0.625000]  [Adversarial loss: 0.348114, acc: 0.824219]\n",
            "2026: [Discriminator loss: 0.526544, acc: 0.632812]  [Adversarial loss: 0.364293, acc: 0.789062]\n",
            "2027: [Discriminator loss: 0.481553, acc: 0.687500]  [Adversarial loss: 0.354046, acc: 0.820312]\n",
            "2028: [Discriminator loss: 0.530170, acc: 0.632812]  [Adversarial loss: 0.379321, acc: 0.796875]\n",
            "2029: [Discriminator loss: 0.569694, acc: 0.621094]  [Adversarial loss: 0.380660, acc: 0.828125]\n",
            "2030: [Discriminator loss: 0.611828, acc: 0.582031]  [Adversarial loss: 0.357097, acc: 0.863281]\n",
            "2031: [Discriminator loss: 0.535642, acc: 0.652344]  [Adversarial loss: 0.330461, acc: 0.828125]\n",
            "2032: [Discriminator loss: 0.635880, acc: 0.554688]  [Adversarial loss: 0.396764, acc: 0.835938]\n",
            "2033: [Discriminator loss: 0.556474, acc: 0.625000]  [Adversarial loss: 0.361432, acc: 0.808594]\n",
            "2034: [Discriminator loss: 0.545547, acc: 0.625000]  [Adversarial loss: 0.344099, acc: 0.835938]\n",
            "2035: [Discriminator loss: 0.483521, acc: 0.656250]  [Adversarial loss: 0.372812, acc: 0.796875]\n",
            "2036: [Discriminator loss: 0.566131, acc: 0.582031]  [Adversarial loss: 0.367509, acc: 0.847656]\n",
            "2037: [Discriminator loss: 0.604302, acc: 0.578125]  [Adversarial loss: 0.394313, acc: 0.777344]\n",
            "2038: [Discriminator loss: 0.517385, acc: 0.617188]  [Adversarial loss: 0.400413, acc: 0.820312]\n",
            "2039: [Discriminator loss: 0.493778, acc: 0.617188]  [Adversarial loss: 0.366307, acc: 0.785156]\n",
            "2040: [Discriminator loss: 0.507808, acc: 0.644531]  [Adversarial loss: 0.345425, acc: 0.824219]\n",
            "2041: [Discriminator loss: 0.534586, acc: 0.570312]  [Adversarial loss: 0.427450, acc: 0.792969]\n",
            "2042: [Discriminator loss: 0.614092, acc: 0.605469]  [Adversarial loss: 0.385461, acc: 0.800781]\n",
            "2043: [Discriminator loss: 0.545630, acc: 0.628906]  [Adversarial loss: 0.337084, acc: 0.820312]\n",
            "2044: [Discriminator loss: 0.588883, acc: 0.558594]  [Adversarial loss: 0.370146, acc: 0.820312]\n",
            "2045: [Discriminator loss: 0.510511, acc: 0.640625]  [Adversarial loss: 0.371472, acc: 0.832031]\n",
            "2046: [Discriminator loss: 0.595816, acc: 0.625000]  [Adversarial loss: 0.434164, acc: 0.769531]\n",
            "2047: [Discriminator loss: 0.604888, acc: 0.589844]  [Adversarial loss: 0.447223, acc: 0.738281]\n",
            "2048: [Discriminator loss: 0.573183, acc: 0.589844]  [Adversarial loss: 0.388594, acc: 0.804688]\n",
            "2049: [Discriminator loss: 0.603322, acc: 0.574219]  [Adversarial loss: 0.402819, acc: 0.808594]\n",
            "2050: [Discriminator loss: 0.607300, acc: 0.585938]  [Adversarial loss: 0.418000, acc: 0.800781]\n",
            "2051: [Discriminator loss: 0.534530, acc: 0.613281]  [Adversarial loss: 0.394647, acc: 0.769531]\n",
            "2052: [Discriminator loss: 0.546137, acc: 0.648438]  [Adversarial loss: 0.376427, acc: 0.812500]\n",
            "2053: [Discriminator loss: 0.540984, acc: 0.660156]  [Adversarial loss: 0.371989, acc: 0.789062]\n",
            "2054: [Discriminator loss: 0.461558, acc: 0.660156]  [Adversarial loss: 0.365925, acc: 0.820312]\n",
            "2055: [Discriminator loss: 0.575551, acc: 0.652344]  [Adversarial loss: 0.399354, acc: 0.753906]\n",
            "2056: [Discriminator loss: 0.503189, acc: 0.605469]  [Adversarial loss: 0.377273, acc: 0.820312]\n",
            "2057: [Discriminator loss: 0.584816, acc: 0.554688]  [Adversarial loss: 0.442641, acc: 0.773438]\n",
            "2058: [Discriminator loss: 0.608042, acc: 0.593750]  [Adversarial loss: 0.362833, acc: 0.812500]\n",
            "2059: [Discriminator loss: 0.499623, acc: 0.667969]  [Adversarial loss: 0.373034, acc: 0.789062]\n",
            "2060: [Discriminator loss: 0.588297, acc: 0.605469]  [Adversarial loss: 0.422825, acc: 0.792969]\n",
            "2061: [Discriminator loss: 0.592620, acc: 0.609375]  [Adversarial loss: 0.404039, acc: 0.773438]\n",
            "2062: [Discriminator loss: 0.588614, acc: 0.632812]  [Adversarial loss: 0.372807, acc: 0.785156]\n",
            "2063: [Discriminator loss: 0.521636, acc: 0.636719]  [Adversarial loss: 0.365871, acc: 0.824219]\n",
            "2064: [Discriminator loss: 0.526726, acc: 0.671875]  [Adversarial loss: 0.403848, acc: 0.769531]\n",
            "2065: [Discriminator loss: 0.656502, acc: 0.593750]  [Adversarial loss: 0.423434, acc: 0.769531]\n",
            "2066: [Discriminator loss: 0.547884, acc: 0.644531]  [Adversarial loss: 0.361083, acc: 0.812500]\n",
            "2067: [Discriminator loss: 0.615375, acc: 0.609375]  [Adversarial loss: 0.394385, acc: 0.761719]\n",
            "2068: [Discriminator loss: 0.578477, acc: 0.652344]  [Adversarial loss: 0.381922, acc: 0.812500]\n",
            "2069: [Discriminator loss: 0.523490, acc: 0.632812]  [Adversarial loss: 0.366152, acc: 0.765625]\n",
            "2070: [Discriminator loss: 0.554815, acc: 0.656250]  [Adversarial loss: 0.396704, acc: 0.808594]\n",
            "2071: [Discriminator loss: 0.511045, acc: 0.656250]  [Adversarial loss: 0.381084, acc: 0.792969]\n",
            "2072: [Discriminator loss: 0.582708, acc: 0.621094]  [Adversarial loss: 0.428641, acc: 0.765625]\n",
            "2073: [Discriminator loss: 0.526226, acc: 0.675781]  [Adversarial loss: 0.396702, acc: 0.796875]\n",
            "2074: [Discriminator loss: 0.562198, acc: 0.640625]  [Adversarial loss: 0.384993, acc: 0.777344]\n",
            "2075: [Discriminator loss: 0.623110, acc: 0.566406]  [Adversarial loss: 0.427792, acc: 0.800781]\n",
            "2076: [Discriminator loss: 0.591928, acc: 0.585938]  [Adversarial loss: 0.400477, acc: 0.800781]\n",
            "2077: [Discriminator loss: 0.585460, acc: 0.566406]  [Adversarial loss: 0.401222, acc: 0.800781]\n",
            "2078: [Discriminator loss: 0.525954, acc: 0.652344]  [Adversarial loss: 0.386868, acc: 0.792969]\n",
            "2079: [Discriminator loss: 0.557018, acc: 0.648438]  [Adversarial loss: 0.419483, acc: 0.812500]\n",
            "2080: [Discriminator loss: 0.477095, acc: 0.710938]  [Adversarial loss: 0.337425, acc: 0.835938]\n",
            "2081: [Discriminator loss: 0.560864, acc: 0.644531]  [Adversarial loss: 0.355065, acc: 0.820312]\n",
            "2082: [Discriminator loss: 0.537775, acc: 0.621094]  [Adversarial loss: 0.410299, acc: 0.757812]\n",
            "2083: [Discriminator loss: 0.584110, acc: 0.613281]  [Adversarial loss: 0.438528, acc: 0.777344]\n",
            "2084: [Discriminator loss: 0.613570, acc: 0.578125]  [Adversarial loss: 0.402421, acc: 0.769531]\n",
            "2085: [Discriminator loss: 0.593887, acc: 0.621094]  [Adversarial loss: 0.415506, acc: 0.785156]\n",
            "2086: [Discriminator loss: 0.556513, acc: 0.601562]  [Adversarial loss: 0.406971, acc: 0.785156]\n",
            "2087: [Discriminator loss: 0.537853, acc: 0.613281]  [Adversarial loss: 0.366566, acc: 0.792969]\n",
            "2088: [Discriminator loss: 0.535165, acc: 0.621094]  [Adversarial loss: 0.425710, acc: 0.804688]\n",
            "2089: [Discriminator loss: 0.658266, acc: 0.558594]  [Adversarial loss: 0.401620, acc: 0.765625]\n",
            "2090: [Discriminator loss: 0.530367, acc: 0.652344]  [Adversarial loss: 0.349835, acc: 0.851562]\n",
            "2091: [Discriminator loss: 0.531128, acc: 0.640625]  [Adversarial loss: 0.417091, acc: 0.808594]\n",
            "2092: [Discriminator loss: 0.569186, acc: 0.613281]  [Adversarial loss: 0.377198, acc: 0.796875]\n",
            "2093: [Discriminator loss: 0.594759, acc: 0.636719]  [Adversarial loss: 0.458999, acc: 0.777344]\n",
            "2094: [Discriminator loss: 0.612538, acc: 0.613281]  [Adversarial loss: 0.372408, acc: 0.796875]\n",
            "2095: [Discriminator loss: 0.522358, acc: 0.617188]  [Adversarial loss: 0.346787, acc: 0.847656]\n",
            "2096: [Discriminator loss: 0.560941, acc: 0.582031]  [Adversarial loss: 0.399284, acc: 0.796875]\n",
            "2097: [Discriminator loss: 0.612301, acc: 0.558594]  [Adversarial loss: 0.426191, acc: 0.792969]\n",
            "2098: [Discriminator loss: 0.545350, acc: 0.675781]  [Adversarial loss: 0.372272, acc: 0.816406]\n",
            "2099: [Discriminator loss: 0.566046, acc: 0.640625]  [Adversarial loss: 0.395187, acc: 0.816406]\n",
            "2100: [Discriminator loss: 0.600846, acc: 0.605469]  [Adversarial loss: 0.417618, acc: 0.789062]\n",
            "2101: [Discriminator loss: 0.574663, acc: 0.605469]  [Adversarial loss: 0.390544, acc: 0.820312]\n",
            "2102: [Discriminator loss: 0.599353, acc: 0.601562]  [Adversarial loss: 0.398684, acc: 0.777344]\n",
            "2103: [Discriminator loss: 0.573031, acc: 0.601562]  [Adversarial loss: 0.392196, acc: 0.796875]\n",
            "2104: [Discriminator loss: 0.545875, acc: 0.648438]  [Adversarial loss: 0.392463, acc: 0.800781]\n",
            "2105: [Discriminator loss: 0.527880, acc: 0.632812]  [Adversarial loss: 0.379800, acc: 0.800781]\n",
            "2106: [Discriminator loss: 0.560502, acc: 0.617188]  [Adversarial loss: 0.410230, acc: 0.781250]\n",
            "2107: [Discriminator loss: 0.528312, acc: 0.621094]  [Adversarial loss: 0.392860, acc: 0.796875]\n",
            "2108: [Discriminator loss: 0.586520, acc: 0.605469]  [Adversarial loss: 0.365916, acc: 0.824219]\n",
            "2109: [Discriminator loss: 0.531819, acc: 0.632812]  [Adversarial loss: 0.390209, acc: 0.800781]\n",
            "2110: [Discriminator loss: 0.570656, acc: 0.613281]  [Adversarial loss: 0.364842, acc: 0.828125]\n",
            "2111: [Discriminator loss: 0.564536, acc: 0.625000]  [Adversarial loss: 0.401358, acc: 0.761719]\n",
            "2112: [Discriminator loss: 0.588697, acc: 0.578125]  [Adversarial loss: 0.395102, acc: 0.781250]\n",
            "2113: [Discriminator loss: 0.571903, acc: 0.609375]  [Adversarial loss: 0.365212, acc: 0.812500]\n",
            "2114: [Discriminator loss: 0.537923, acc: 0.628906]  [Adversarial loss: 0.380503, acc: 0.847656]\n",
            "2115: [Discriminator loss: 0.589280, acc: 0.613281]  [Adversarial loss: 0.404371, acc: 0.781250]\n",
            "2116: [Discriminator loss: 0.525781, acc: 0.652344]  [Adversarial loss: 0.434942, acc: 0.812500]\n",
            "2117: [Discriminator loss: 0.547224, acc: 0.609375]  [Adversarial loss: 0.356906, acc: 0.808594]\n",
            "2118: [Discriminator loss: 0.473838, acc: 0.632812]  [Adversarial loss: 0.348420, acc: 0.816406]\n",
            "2119: [Discriminator loss: 0.548007, acc: 0.609375]  [Adversarial loss: 0.354937, acc: 0.808594]\n",
            "2120: [Discriminator loss: 0.599665, acc: 0.554688]  [Adversarial loss: 0.420937, acc: 0.781250]\n",
            "2121: [Discriminator loss: 0.577765, acc: 0.585938]  [Adversarial loss: 0.414304, acc: 0.789062]\n",
            "2122: [Discriminator loss: 0.568043, acc: 0.625000]  [Adversarial loss: 0.399682, acc: 0.781250]\n",
            "2123: [Discriminator loss: 0.569090, acc: 0.632812]  [Adversarial loss: 0.401458, acc: 0.777344]\n",
            "2124: [Discriminator loss: 0.540778, acc: 0.582031]  [Adversarial loss: 0.400677, acc: 0.773438]\n",
            "2125: [Discriminator loss: 0.557102, acc: 0.636719]  [Adversarial loss: 0.337116, acc: 0.796875]\n",
            "2126: [Discriminator loss: 0.549927, acc: 0.613281]  [Adversarial loss: 0.393495, acc: 0.792969]\n",
            "2127: [Discriminator loss: 0.515681, acc: 0.613281]  [Adversarial loss: 0.358647, acc: 0.828125]\n",
            "2128: [Discriminator loss: 0.615080, acc: 0.558594]  [Adversarial loss: 0.436894, acc: 0.761719]\n",
            "2129: [Discriminator loss: 0.548077, acc: 0.613281]  [Adversarial loss: 0.404019, acc: 0.777344]\n",
            "2130: [Discriminator loss: 0.545966, acc: 0.609375]  [Adversarial loss: 0.415630, acc: 0.785156]\n",
            "2131: [Discriminator loss: 0.522982, acc: 0.621094]  [Adversarial loss: 0.376314, acc: 0.812500]\n",
            "2132: [Discriminator loss: 0.576346, acc: 0.574219]  [Adversarial loss: 0.412082, acc: 0.765625]\n",
            "2133: [Discriminator loss: 0.532154, acc: 0.605469]  [Adversarial loss: 0.445234, acc: 0.800781]\n",
            "2134: [Discriminator loss: 0.583179, acc: 0.625000]  [Adversarial loss: 0.406993, acc: 0.785156]\n",
            "2135: [Discriminator loss: 0.532631, acc: 0.652344]  [Adversarial loss: 0.386251, acc: 0.777344]\n",
            "2136: [Discriminator loss: 0.583329, acc: 0.648438]  [Adversarial loss: 0.383962, acc: 0.800781]\n",
            "2137: [Discriminator loss: 0.533253, acc: 0.679688]  [Adversarial loss: 0.351823, acc: 0.839844]\n",
            "2138: [Discriminator loss: 0.582795, acc: 0.628906]  [Adversarial loss: 0.368823, acc: 0.781250]\n",
            "2139: [Discriminator loss: 0.562678, acc: 0.648438]  [Adversarial loss: 0.376677, acc: 0.808594]\n",
            "2140: [Discriminator loss: 0.516274, acc: 0.648438]  [Adversarial loss: 0.381695, acc: 0.812500]\n",
            "2141: [Discriminator loss: 0.491890, acc: 0.609375]  [Adversarial loss: 0.406508, acc: 0.812500]\n",
            "2142: [Discriminator loss: 0.596038, acc: 0.609375]  [Adversarial loss: 0.373369, acc: 0.812500]\n",
            "2143: [Discriminator loss: 0.568762, acc: 0.636719]  [Adversarial loss: 0.489435, acc: 0.769531]\n",
            "2144: [Discriminator loss: 0.541969, acc: 0.644531]  [Adversarial loss: 0.372562, acc: 0.804688]\n",
            "2145: [Discriminator loss: 0.489830, acc: 0.671875]  [Adversarial loss: 0.363598, acc: 0.816406]\n",
            "2146: [Discriminator loss: 0.480401, acc: 0.710938]  [Adversarial loss: 0.378330, acc: 0.792969]\n",
            "2147: [Discriminator loss: 0.563026, acc: 0.609375]  [Adversarial loss: 0.417076, acc: 0.789062]\n",
            "2148: [Discriminator loss: 0.565523, acc: 0.601562]  [Adversarial loss: 0.377860, acc: 0.800781]\n",
            "2149: [Discriminator loss: 0.562300, acc: 0.605469]  [Adversarial loss: 0.413603, acc: 0.773438]\n",
            "2150: [Discriminator loss: 0.564777, acc: 0.636719]  [Adversarial loss: 0.369266, acc: 0.828125]\n",
            "2151: [Discriminator loss: 0.540221, acc: 0.648438]  [Adversarial loss: 0.341556, acc: 0.820312]\n",
            "2152: [Discriminator loss: 0.537096, acc: 0.605469]  [Adversarial loss: 0.415596, acc: 0.812500]\n",
            "2153: [Discriminator loss: 0.554521, acc: 0.613281]  [Adversarial loss: 0.415678, acc: 0.796875]\n",
            "2154: [Discriminator loss: 0.566312, acc: 0.613281]  [Adversarial loss: 0.369942, acc: 0.812500]\n",
            "2155: [Discriminator loss: 0.439013, acc: 0.656250]  [Adversarial loss: 0.428405, acc: 0.792969]\n",
            "2156: [Discriminator loss: 0.522651, acc: 0.675781]  [Adversarial loss: 0.449408, acc: 0.761719]\n",
            "2157: [Discriminator loss: 0.501075, acc: 0.640625]  [Adversarial loss: 0.409134, acc: 0.808594]\n",
            "2158: [Discriminator loss: 0.531428, acc: 0.597656]  [Adversarial loss: 0.348270, acc: 0.812500]\n",
            "2159: [Discriminator loss: 0.528328, acc: 0.632812]  [Adversarial loss: 0.368171, acc: 0.800781]\n",
            "2160: [Discriminator loss: 0.573258, acc: 0.617188]  [Adversarial loss: 0.379735, acc: 0.761719]\n",
            "2161: [Discriminator loss: 0.488129, acc: 0.648438]  [Adversarial loss: 0.426026, acc: 0.769531]\n",
            "2162: [Discriminator loss: 0.489755, acc: 0.683594]  [Adversarial loss: 0.425056, acc: 0.738281]\n",
            "2163: [Discriminator loss: 0.572662, acc: 0.613281]  [Adversarial loss: 0.414254, acc: 0.796875]\n",
            "2164: [Discriminator loss: 0.577202, acc: 0.621094]  [Adversarial loss: 0.409364, acc: 0.812500]\n",
            "2165: [Discriminator loss: 0.561845, acc: 0.593750]  [Adversarial loss: 0.404258, acc: 0.785156]\n",
            "2166: [Discriminator loss: 0.566733, acc: 0.632812]  [Adversarial loss: 0.403272, acc: 0.761719]\n",
            "2167: [Discriminator loss: 0.587801, acc: 0.582031]  [Adversarial loss: 0.417111, acc: 0.792969]\n",
            "2168: [Discriminator loss: 0.573344, acc: 0.640625]  [Adversarial loss: 0.401413, acc: 0.792969]\n",
            "2169: [Discriminator loss: 0.493802, acc: 0.660156]  [Adversarial loss: 0.360776, acc: 0.812500]\n",
            "2170: [Discriminator loss: 0.526509, acc: 0.675781]  [Adversarial loss: 0.439649, acc: 0.773438]\n",
            "2171: [Discriminator loss: 0.565424, acc: 0.625000]  [Adversarial loss: 0.408525, acc: 0.789062]\n",
            "2172: [Discriminator loss: 0.560769, acc: 0.628906]  [Adversarial loss: 0.358839, acc: 0.789062]\n",
            "2173: [Discriminator loss: 0.583868, acc: 0.632812]  [Adversarial loss: 0.363470, acc: 0.832031]\n",
            "2174: [Discriminator loss: 0.520979, acc: 0.644531]  [Adversarial loss: 0.392921, acc: 0.792969]\n",
            "2175: [Discriminator loss: 0.531277, acc: 0.628906]  [Adversarial loss: 0.413516, acc: 0.753906]\n",
            "2176: [Discriminator loss: 0.541938, acc: 0.632812]  [Adversarial loss: 0.381493, acc: 0.855469]\n",
            "2177: [Discriminator loss: 0.624525, acc: 0.617188]  [Adversarial loss: 0.414377, acc: 0.808594]\n",
            "2178: [Discriminator loss: 0.528314, acc: 0.625000]  [Adversarial loss: 0.378999, acc: 0.824219]\n",
            "2179: [Discriminator loss: 0.544966, acc: 0.609375]  [Adversarial loss: 0.374814, acc: 0.769531]\n",
            "2180: [Discriminator loss: 0.542325, acc: 0.632812]  [Adversarial loss: 0.368763, acc: 0.820312]\n",
            "2181: [Discriminator loss: 0.570342, acc: 0.628906]  [Adversarial loss: 0.385391, acc: 0.792969]\n",
            "2182: [Discriminator loss: 0.548150, acc: 0.636719]  [Adversarial loss: 0.356974, acc: 0.839844]\n",
            "2183: [Discriminator loss: 0.536825, acc: 0.636719]  [Adversarial loss: 0.351526, acc: 0.804688]\n",
            "2184: [Discriminator loss: 0.504675, acc: 0.617188]  [Adversarial loss: 0.361377, acc: 0.808594]\n",
            "2185: [Discriminator loss: 0.537549, acc: 0.636719]  [Adversarial loss: 0.378867, acc: 0.765625]\n",
            "2186: [Discriminator loss: 0.517375, acc: 0.625000]  [Adversarial loss: 0.404893, acc: 0.781250]\n",
            "2187: [Discriminator loss: 0.552433, acc: 0.625000]  [Adversarial loss: 0.374652, acc: 0.839844]\n",
            "2188: [Discriminator loss: 0.593651, acc: 0.585938]  [Adversarial loss: 0.387426, acc: 0.761719]\n",
            "2189: [Discriminator loss: 0.574506, acc: 0.593750]  [Adversarial loss: 0.406559, acc: 0.773438]\n",
            "2190: [Discriminator loss: 0.613593, acc: 0.613281]  [Adversarial loss: 0.357660, acc: 0.835938]\n",
            "2191: [Discriminator loss: 0.580957, acc: 0.621094]  [Adversarial loss: 0.389975, acc: 0.785156]\n",
            "2192: [Discriminator loss: 0.529327, acc: 0.648438]  [Adversarial loss: 0.366222, acc: 0.789062]\n",
            "2193: [Discriminator loss: 0.526091, acc: 0.683594]  [Adversarial loss: 0.348929, acc: 0.839844]\n",
            "2194: [Discriminator loss: 0.566108, acc: 0.593750]  [Adversarial loss: 0.367874, acc: 0.781250]\n",
            "2195: [Discriminator loss: 0.552403, acc: 0.636719]  [Adversarial loss: 0.348991, acc: 0.835938]\n",
            "2196: [Discriminator loss: 0.542019, acc: 0.628906]  [Adversarial loss: 0.366169, acc: 0.812500]\n",
            "2197: [Discriminator loss: 0.525090, acc: 0.625000]  [Adversarial loss: 0.383474, acc: 0.789062]\n",
            "2198: [Discriminator loss: 0.538620, acc: 0.632812]  [Adversarial loss: 0.351410, acc: 0.843750]\n",
            "2199: [Discriminator loss: 0.569586, acc: 0.609375]  [Adversarial loss: 0.331729, acc: 0.859375]\n",
            "2200: [Discriminator loss: 0.599333, acc: 0.621094]  [Adversarial loss: 0.381672, acc: 0.824219]\n",
            "2201: [Discriminator loss: 0.582693, acc: 0.585938]  [Adversarial loss: 0.427035, acc: 0.785156]\n",
            "2202: [Discriminator loss: 0.568897, acc: 0.609375]  [Adversarial loss: 0.361701, acc: 0.828125]\n",
            "2203: [Discriminator loss: 0.513318, acc: 0.660156]  [Adversarial loss: 0.398294, acc: 0.789062]\n",
            "2204: [Discriminator loss: 0.599588, acc: 0.613281]  [Adversarial loss: 0.406030, acc: 0.804688]\n",
            "2205: [Discriminator loss: 0.552714, acc: 0.625000]  [Adversarial loss: 0.355329, acc: 0.820312]\n",
            "2206: [Discriminator loss: 0.490460, acc: 0.652344]  [Adversarial loss: 0.358722, acc: 0.835938]\n",
            "2207: [Discriminator loss: 0.611108, acc: 0.578125]  [Adversarial loss: 0.412351, acc: 0.804688]\n",
            "2208: [Discriminator loss: 0.605312, acc: 0.621094]  [Adversarial loss: 0.394397, acc: 0.832031]\n",
            "2209: [Discriminator loss: 0.571773, acc: 0.582031]  [Adversarial loss: 0.431454, acc: 0.757812]\n",
            "2210: [Discriminator loss: 0.571268, acc: 0.589844]  [Adversarial loss: 0.416474, acc: 0.796875]\n",
            "2211: [Discriminator loss: 0.562759, acc: 0.617188]  [Adversarial loss: 0.412383, acc: 0.785156]\n",
            "2212: [Discriminator loss: 0.524905, acc: 0.660156]  [Adversarial loss: 0.358764, acc: 0.820312]\n",
            "2213: [Discriminator loss: 0.540407, acc: 0.656250]  [Adversarial loss: 0.375637, acc: 0.828125]\n",
            "2214: [Discriminator loss: 0.544823, acc: 0.667969]  [Adversarial loss: 0.368353, acc: 0.796875]\n",
            "2215: [Discriminator loss: 0.604530, acc: 0.593750]  [Adversarial loss: 0.397792, acc: 0.804688]\n",
            "2216: [Discriminator loss: 0.587511, acc: 0.628906]  [Adversarial loss: 0.386287, acc: 0.824219]\n",
            "2217: [Discriminator loss: 0.546093, acc: 0.625000]  [Adversarial loss: 0.395534, acc: 0.773438]\n",
            "2218: [Discriminator loss: 0.544851, acc: 0.628906]  [Adversarial loss: 0.381714, acc: 0.789062]\n",
            "2219: [Discriminator loss: 0.627088, acc: 0.578125]  [Adversarial loss: 0.458169, acc: 0.804688]\n",
            "2220: [Discriminator loss: 0.569131, acc: 0.625000]  [Adversarial loss: 0.430151, acc: 0.777344]\n",
            "2221: [Discriminator loss: 0.595281, acc: 0.628906]  [Adversarial loss: 0.398876, acc: 0.808594]\n",
            "2222: [Discriminator loss: 0.531173, acc: 0.632812]  [Adversarial loss: 0.374038, acc: 0.800781]\n",
            "2223: [Discriminator loss: 0.539900, acc: 0.632812]  [Adversarial loss: 0.389647, acc: 0.757812]\n",
            "2224: [Discriminator loss: 0.559714, acc: 0.656250]  [Adversarial loss: 0.404362, acc: 0.800781]\n",
            "2225: [Discriminator loss: 0.548993, acc: 0.660156]  [Adversarial loss: 0.355289, acc: 0.828125]\n",
            "2226: [Discriminator loss: 0.578382, acc: 0.632812]  [Adversarial loss: 0.336695, acc: 0.843750]\n",
            "2227: [Discriminator loss: 0.578373, acc: 0.589844]  [Adversarial loss: 0.409529, acc: 0.769531]\n",
            "2228: [Discriminator loss: 0.548733, acc: 0.593750]  [Adversarial loss: 0.408153, acc: 0.796875]\n",
            "2229: [Discriminator loss: 0.576189, acc: 0.601562]  [Adversarial loss: 0.412126, acc: 0.769531]\n",
            "2230: [Discriminator loss: 0.556744, acc: 0.648438]  [Adversarial loss: 0.388425, acc: 0.785156]\n",
            "2231: [Discriminator loss: 0.516565, acc: 0.644531]  [Adversarial loss: 0.389138, acc: 0.785156]\n",
            "2232: [Discriminator loss: 0.512189, acc: 0.621094]  [Adversarial loss: 0.361908, acc: 0.816406]\n",
            "2233: [Discriminator loss: 0.568790, acc: 0.597656]  [Adversarial loss: 0.449880, acc: 0.757812]\n",
            "2234: [Discriminator loss: 0.565910, acc: 0.593750]  [Adversarial loss: 0.379597, acc: 0.820312]\n",
            "2235: [Discriminator loss: 0.546498, acc: 0.625000]  [Adversarial loss: 0.372034, acc: 0.800781]\n",
            "2236: [Discriminator loss: 0.568330, acc: 0.621094]  [Adversarial loss: 0.376333, acc: 0.796875]\n",
            "2237: [Discriminator loss: 0.617982, acc: 0.605469]  [Adversarial loss: 0.422316, acc: 0.777344]\n",
            "2238: [Discriminator loss: 0.562235, acc: 0.656250]  [Adversarial loss: 0.397154, acc: 0.789062]\n",
            "2239: [Discriminator loss: 0.603013, acc: 0.617188]  [Adversarial loss: 0.412304, acc: 0.726562]\n",
            "2240: [Discriminator loss: 0.584338, acc: 0.632812]  [Adversarial loss: 0.397065, acc: 0.816406]\n",
            "2241: [Discriminator loss: 0.581475, acc: 0.675781]  [Adversarial loss: 0.385189, acc: 0.812500]\n",
            "2242: [Discriminator loss: 0.567724, acc: 0.617188]  [Adversarial loss: 0.411296, acc: 0.746094]\n",
            "2243: [Discriminator loss: 0.475225, acc: 0.667969]  [Adversarial loss: 0.369978, acc: 0.796875]\n",
            "2244: [Discriminator loss: 0.497777, acc: 0.683594]  [Adversarial loss: 0.397289, acc: 0.781250]\n",
            "2245: [Discriminator loss: 0.509530, acc: 0.664062]  [Adversarial loss: 0.403458, acc: 0.792969]\n",
            "2246: [Discriminator loss: 0.562205, acc: 0.617188]  [Adversarial loss: 0.387273, acc: 0.746094]\n",
            "2247: [Discriminator loss: 0.535170, acc: 0.660156]  [Adversarial loss: 0.367826, acc: 0.789062]\n",
            "2248: [Discriminator loss: 0.480813, acc: 0.648438]  [Adversarial loss: 0.385473, acc: 0.792969]\n",
            "2249: [Discriminator loss: 0.492141, acc: 0.671875]  [Adversarial loss: 0.407550, acc: 0.757812]\n",
            "2250: [Discriminator loss: 0.536037, acc: 0.660156]  [Adversarial loss: 0.389418, acc: 0.761719]\n",
            "2251: [Discriminator loss: 0.558816, acc: 0.667969]  [Adversarial loss: 0.356968, acc: 0.789062]\n",
            "2252: [Discriminator loss: 0.548298, acc: 0.652344]  [Adversarial loss: 0.370569, acc: 0.808594]\n",
            "2253: [Discriminator loss: 0.566211, acc: 0.628906]  [Adversarial loss: 0.387941, acc: 0.777344]\n",
            "2254: [Discriminator loss: 0.544130, acc: 0.644531]  [Adversarial loss: 0.378299, acc: 0.816406]\n",
            "2255: [Discriminator loss: 0.540825, acc: 0.621094]  [Adversarial loss: 0.419766, acc: 0.742188]\n",
            "2256: [Discriminator loss: 0.502208, acc: 0.679688]  [Adversarial loss: 0.365863, acc: 0.832031]\n",
            "2257: [Discriminator loss: 0.568750, acc: 0.671875]  [Adversarial loss: 0.473141, acc: 0.769531]\n",
            "2258: [Discriminator loss: 0.509738, acc: 0.671875]  [Adversarial loss: 0.350533, acc: 0.832031]\n",
            "2259: [Discriminator loss: 0.559704, acc: 0.656250]  [Adversarial loss: 0.399420, acc: 0.804688]\n",
            "2260: [Discriminator loss: 0.547875, acc: 0.625000]  [Adversarial loss: 0.465374, acc: 0.773438]\n",
            "2261: [Discriminator loss: 0.552589, acc: 0.628906]  [Adversarial loss: 0.393791, acc: 0.773438]\n",
            "2262: [Discriminator loss: 0.509749, acc: 0.687500]  [Adversarial loss: 0.362288, acc: 0.812500]\n",
            "2263: [Discriminator loss: 0.487855, acc: 0.691406]  [Adversarial loss: 0.386397, acc: 0.804688]\n",
            "2264: [Discriminator loss: 0.508918, acc: 0.679688]  [Adversarial loss: 0.367019, acc: 0.789062]\n",
            "2265: [Discriminator loss: 0.529584, acc: 0.660156]  [Adversarial loss: 0.391510, acc: 0.769531]\n",
            "2266: [Discriminator loss: 0.611078, acc: 0.605469]  [Adversarial loss: 0.444955, acc: 0.753906]\n",
            "2267: [Discriminator loss: 0.536880, acc: 0.671875]  [Adversarial loss: 0.403679, acc: 0.792969]\n",
            "2268: [Discriminator loss: 0.550340, acc: 0.617188]  [Adversarial loss: 0.381128, acc: 0.769531]\n",
            "2269: [Discriminator loss: 0.469147, acc: 0.703125]  [Adversarial loss: 0.412521, acc: 0.734375]\n",
            "2270: [Discriminator loss: 0.537472, acc: 0.613281]  [Adversarial loss: 0.397540, acc: 0.765625]\n",
            "2271: [Discriminator loss: 0.500910, acc: 0.671875]  [Adversarial loss: 0.336743, acc: 0.808594]\n",
            "2272: [Discriminator loss: 0.567312, acc: 0.644531]  [Adversarial loss: 0.363536, acc: 0.851562]\n",
            "2273: [Discriminator loss: 0.539620, acc: 0.679688]  [Adversarial loss: 0.363816, acc: 0.796875]\n",
            "2274: [Discriminator loss: 0.505712, acc: 0.699219]  [Adversarial loss: 0.391268, acc: 0.773438]\n",
            "2275: [Discriminator loss: 0.574785, acc: 0.605469]  [Adversarial loss: 0.423887, acc: 0.785156]\n",
            "2276: [Discriminator loss: 0.541768, acc: 0.613281]  [Adversarial loss: 0.355823, acc: 0.828125]\n",
            "2277: [Discriminator loss: 0.457448, acc: 0.644531]  [Adversarial loss: 0.308727, acc: 0.855469]\n",
            "2278: [Discriminator loss: 0.497106, acc: 0.671875]  [Adversarial loss: 0.372461, acc: 0.812500]\n",
            "2279: [Discriminator loss: 0.545492, acc: 0.613281]  [Adversarial loss: 0.357361, acc: 0.839844]\n",
            "2280: [Discriminator loss: 0.486831, acc: 0.679688]  [Adversarial loss: 0.329903, acc: 0.835938]\n",
            "2281: [Discriminator loss: 0.528709, acc: 0.628906]  [Adversarial loss: 0.350975, acc: 0.835938]\n",
            "2282: [Discriminator loss: 0.524632, acc: 0.621094]  [Adversarial loss: 0.381724, acc: 0.792969]\n",
            "2283: [Discriminator loss: 0.529486, acc: 0.640625]  [Adversarial loss: 0.383588, acc: 0.789062]\n",
            "2284: [Discriminator loss: 0.510161, acc: 0.656250]  [Adversarial loss: 0.393480, acc: 0.796875]\n",
            "2285: [Discriminator loss: 0.563842, acc: 0.625000]  [Adversarial loss: 0.390780, acc: 0.792969]\n",
            "2286: [Discriminator loss: 0.508352, acc: 0.675781]  [Adversarial loss: 0.385948, acc: 0.820312]\n",
            "2287: [Discriminator loss: 0.575571, acc: 0.589844]  [Adversarial loss: 0.355040, acc: 0.824219]\n",
            "2288: [Discriminator loss: 0.538000, acc: 0.628906]  [Adversarial loss: 0.357204, acc: 0.804688]\n",
            "2289: [Discriminator loss: 0.564717, acc: 0.628906]  [Adversarial loss: 0.393777, acc: 0.808594]\n",
            "2290: [Discriminator loss: 0.546543, acc: 0.589844]  [Adversarial loss: 0.377051, acc: 0.757812]\n",
            "2291: [Discriminator loss: 0.531116, acc: 0.671875]  [Adversarial loss: 0.356951, acc: 0.781250]\n",
            "2292: [Discriminator loss: 0.642408, acc: 0.546875]  [Adversarial loss: 0.412806, acc: 0.781250]\n",
            "2293: [Discriminator loss: 0.521257, acc: 0.640625]  [Adversarial loss: 0.407865, acc: 0.796875]\n",
            "2294: [Discriminator loss: 0.592354, acc: 0.589844]  [Adversarial loss: 0.461014, acc: 0.722656]\n",
            "2295: [Discriminator loss: 0.538161, acc: 0.632812]  [Adversarial loss: 0.435850, acc: 0.773438]\n",
            "2296: [Discriminator loss: 0.531145, acc: 0.636719]  [Adversarial loss: 0.393574, acc: 0.812500]\n",
            "2297: [Discriminator loss: 0.556741, acc: 0.589844]  [Adversarial loss: 0.425887, acc: 0.773438]\n",
            "2298: [Discriminator loss: 0.473456, acc: 0.667969]  [Adversarial loss: 0.385660, acc: 0.800781]\n",
            "2299: [Discriminator loss: 0.573844, acc: 0.613281]  [Adversarial loss: 0.436257, acc: 0.761719]\n",
            "2300: [Discriminator loss: 0.494796, acc: 0.628906]  [Adversarial loss: 0.424344, acc: 0.750000]\n",
            "2301: [Discriminator loss: 0.531158, acc: 0.621094]  [Adversarial loss: 0.410717, acc: 0.789062]\n",
            "2302: [Discriminator loss: 0.631151, acc: 0.562500]  [Adversarial loss: 0.407370, acc: 0.808594]\n",
            "2303: [Discriminator loss: 0.536671, acc: 0.656250]  [Adversarial loss: 0.416327, acc: 0.781250]\n",
            "2304: [Discriminator loss: 0.511542, acc: 0.644531]  [Adversarial loss: 0.436801, acc: 0.781250]\n",
            "2305: [Discriminator loss: 0.610507, acc: 0.597656]  [Adversarial loss: 0.470931, acc: 0.710938]\n",
            "2306: [Discriminator loss: 0.549702, acc: 0.617188]  [Adversarial loss: 0.413514, acc: 0.769531]\n",
            "2307: [Discriminator loss: 0.606352, acc: 0.589844]  [Adversarial loss: 0.418671, acc: 0.761719]\n",
            "2308: [Discriminator loss: 0.563599, acc: 0.632812]  [Adversarial loss: 0.416644, acc: 0.757812]\n",
            "2309: [Discriminator loss: 0.524443, acc: 0.660156]  [Adversarial loss: 0.446383, acc: 0.742188]\n",
            "2310: [Discriminator loss: 0.574587, acc: 0.632812]  [Adversarial loss: 0.433291, acc: 0.796875]\n",
            "2311: [Discriminator loss: 0.509250, acc: 0.671875]  [Adversarial loss: 0.416540, acc: 0.765625]\n",
            "2312: [Discriminator loss: 0.563325, acc: 0.648438]  [Adversarial loss: 0.407517, acc: 0.800781]\n",
            "2313: [Discriminator loss: 0.577074, acc: 0.632812]  [Adversarial loss: 0.474113, acc: 0.707031]\n",
            "2314: [Discriminator loss: 0.564228, acc: 0.597656]  [Adversarial loss: 0.448448, acc: 0.722656]\n",
            "2315: [Discriminator loss: 0.508282, acc: 0.648438]  [Adversarial loss: 0.425210, acc: 0.761719]\n",
            "2316: [Discriminator loss: 0.578272, acc: 0.625000]  [Adversarial loss: 0.400269, acc: 0.757812]\n",
            "2317: [Discriminator loss: 0.547901, acc: 0.648438]  [Adversarial loss: 0.400327, acc: 0.761719]\n",
            "2318: [Discriminator loss: 0.563366, acc: 0.625000]  [Adversarial loss: 0.399733, acc: 0.792969]\n",
            "2319: [Discriminator loss: 0.468214, acc: 0.660156]  [Adversarial loss: 0.358220, acc: 0.816406]\n",
            "2320: [Discriminator loss: 0.471953, acc: 0.644531]  [Adversarial loss: 0.372446, acc: 0.753906]\n",
            "2321: [Discriminator loss: 0.505545, acc: 0.667969]  [Adversarial loss: 0.388095, acc: 0.785156]\n",
            "2322: [Discriminator loss: 0.499273, acc: 0.644531]  [Adversarial loss: 0.343229, acc: 0.828125]\n",
            "2323: [Discriminator loss: 0.525440, acc: 0.625000]  [Adversarial loss: 0.369141, acc: 0.832031]\n",
            "2324: [Discriminator loss: 0.592307, acc: 0.582031]  [Adversarial loss: 0.405765, acc: 0.812500]\n",
            "2325: [Discriminator loss: 0.494768, acc: 0.652344]  [Adversarial loss: 0.378053, acc: 0.777344]\n",
            "2326: [Discriminator loss: 0.496465, acc: 0.652344]  [Adversarial loss: 0.394028, acc: 0.785156]\n",
            "2327: [Discriminator loss: 0.503339, acc: 0.656250]  [Adversarial loss: 0.381673, acc: 0.800781]\n",
            "2328: [Discriminator loss: 0.520764, acc: 0.617188]  [Adversarial loss: 0.360285, acc: 0.777344]\n",
            "2329: [Discriminator loss: 0.479267, acc: 0.667969]  [Adversarial loss: 0.374131, acc: 0.761719]\n",
            "2330: [Discriminator loss: 0.619862, acc: 0.605469]  [Adversarial loss: 0.385308, acc: 0.777344]\n",
            "2331: [Discriminator loss: 0.568915, acc: 0.664062]  [Adversarial loss: 0.332725, acc: 0.808594]\n",
            "2332: [Discriminator loss: 0.613558, acc: 0.613281]  [Adversarial loss: 0.368734, acc: 0.769531]\n",
            "2333: [Discriminator loss: 0.541214, acc: 0.667969]  [Adversarial loss: 0.421116, acc: 0.750000]\n",
            "2334: [Discriminator loss: 0.527319, acc: 0.652344]  [Adversarial loss: 0.427069, acc: 0.773438]\n",
            "2335: [Discriminator loss: 0.502477, acc: 0.699219]  [Adversarial loss: 0.378254, acc: 0.808594]\n",
            "2336: [Discriminator loss: 0.544613, acc: 0.632812]  [Adversarial loss: 0.369839, acc: 0.789062]\n",
            "2337: [Discriminator loss: 0.567476, acc: 0.628906]  [Adversarial loss: 0.407017, acc: 0.730469]\n",
            "2338: [Discriminator loss: 0.511661, acc: 0.687500]  [Adversarial loss: 0.371723, acc: 0.792969]\n",
            "2339: [Discriminator loss: 0.543336, acc: 0.664062]  [Adversarial loss: 0.417370, acc: 0.773438]\n",
            "2340: [Discriminator loss: 0.603655, acc: 0.582031]  [Adversarial loss: 0.425045, acc: 0.777344]\n",
            "2341: [Discriminator loss: 0.565846, acc: 0.613281]  [Adversarial loss: 0.380497, acc: 0.757812]\n",
            "2342: [Discriminator loss: 0.492499, acc: 0.648438]  [Adversarial loss: 0.349986, acc: 0.812500]\n",
            "2343: [Discriminator loss: 0.562435, acc: 0.621094]  [Adversarial loss: 0.438480, acc: 0.746094]\n",
            "2344: [Discriminator loss: 0.535709, acc: 0.660156]  [Adversarial loss: 0.399132, acc: 0.785156]\n",
            "2345: [Discriminator loss: 0.532550, acc: 0.664062]  [Adversarial loss: 0.417701, acc: 0.757812]\n",
            "2346: [Discriminator loss: 0.489842, acc: 0.710938]  [Adversarial loss: 0.381046, acc: 0.796875]\n",
            "2347: [Discriminator loss: 0.485696, acc: 0.664062]  [Adversarial loss: 0.412333, acc: 0.769531]\n",
            "2348: [Discriminator loss: 0.514655, acc: 0.699219]  [Adversarial loss: 0.357510, acc: 0.843750]\n",
            "2349: [Discriminator loss: 0.511310, acc: 0.687500]  [Adversarial loss: 0.355589, acc: 0.820312]\n",
            "2350: [Discriminator loss: 0.524232, acc: 0.656250]  [Adversarial loss: 0.367506, acc: 0.785156]\n",
            "2351: [Discriminator loss: 0.481628, acc: 0.667969]  [Adversarial loss: 0.386551, acc: 0.792969]\n",
            "2352: [Discriminator loss: 0.526697, acc: 0.656250]  [Adversarial loss: 0.366713, acc: 0.773438]\n",
            "2353: [Discriminator loss: 0.536105, acc: 0.648438]  [Adversarial loss: 0.356730, acc: 0.785156]\n",
            "2354: [Discriminator loss: 0.524123, acc: 0.636719]  [Adversarial loss: 0.384209, acc: 0.738281]\n",
            "2355: [Discriminator loss: 0.472451, acc: 0.679688]  [Adversarial loss: 0.387620, acc: 0.777344]\n",
            "2356: [Discriminator loss: 0.508131, acc: 0.636719]  [Adversarial loss: 0.392067, acc: 0.746094]\n",
            "2357: [Discriminator loss: 0.497067, acc: 0.660156]  [Adversarial loss: 0.363941, acc: 0.734375]\n",
            "2358: [Discriminator loss: 0.590755, acc: 0.609375]  [Adversarial loss: 0.384116, acc: 0.773438]\n",
            "2359: [Discriminator loss: 0.527820, acc: 0.675781]  [Adversarial loss: 0.402199, acc: 0.773438]\n",
            "2360: [Discriminator loss: 0.549035, acc: 0.609375]  [Adversarial loss: 0.362119, acc: 0.792969]\n",
            "2361: [Discriminator loss: 0.547458, acc: 0.640625]  [Adversarial loss: 0.449836, acc: 0.742188]\n",
            "2362: [Discriminator loss: 0.480648, acc: 0.679688]  [Adversarial loss: 0.344668, acc: 0.812500]\n",
            "2363: [Discriminator loss: 0.542848, acc: 0.640625]  [Adversarial loss: 0.371850, acc: 0.800781]\n",
            "2364: [Discriminator loss: 0.549238, acc: 0.632812]  [Adversarial loss: 0.401194, acc: 0.757812]\n",
            "2365: [Discriminator loss: 0.554341, acc: 0.617188]  [Adversarial loss: 0.368865, acc: 0.812500]\n",
            "2366: [Discriminator loss: 0.592028, acc: 0.605469]  [Adversarial loss: 0.459924, acc: 0.761719]\n",
            "2367: [Discriminator loss: 0.532563, acc: 0.667969]  [Adversarial loss: 0.408479, acc: 0.773438]\n",
            "2368: [Discriminator loss: 0.582484, acc: 0.636719]  [Adversarial loss: 0.378030, acc: 0.792969]\n",
            "2369: [Discriminator loss: 0.577206, acc: 0.617188]  [Adversarial loss: 0.416707, acc: 0.785156]\n",
            "2370: [Discriminator loss: 0.620416, acc: 0.578125]  [Adversarial loss: 0.415809, acc: 0.781250]\n",
            "2371: [Discriminator loss: 0.551897, acc: 0.625000]  [Adversarial loss: 0.390776, acc: 0.761719]\n",
            "2372: [Discriminator loss: 0.499057, acc: 0.660156]  [Adversarial loss: 0.390181, acc: 0.789062]\n",
            "2373: [Discriminator loss: 0.570195, acc: 0.640625]  [Adversarial loss: 0.444839, acc: 0.722656]\n",
            "2374: [Discriminator loss: 0.538840, acc: 0.667969]  [Adversarial loss: 0.396330, acc: 0.765625]\n",
            "2375: [Discriminator loss: 0.556485, acc: 0.652344]  [Adversarial loss: 0.419591, acc: 0.738281]\n",
            "2376: [Discriminator loss: 0.540921, acc: 0.652344]  [Adversarial loss: 0.407665, acc: 0.753906]\n",
            "2377: [Discriminator loss: 0.520341, acc: 0.644531]  [Adversarial loss: 0.384322, acc: 0.804688]\n",
            "2378: [Discriminator loss: 0.544305, acc: 0.644531]  [Adversarial loss: 0.392817, acc: 0.789062]\n",
            "2379: [Discriminator loss: 0.552526, acc: 0.683594]  [Adversarial loss: 0.412913, acc: 0.777344]\n",
            "2380: [Discriminator loss: 0.500928, acc: 0.656250]  [Adversarial loss: 0.366614, acc: 0.792969]\n",
            "2381: [Discriminator loss: 0.547251, acc: 0.640625]  [Adversarial loss: 0.390288, acc: 0.824219]\n",
            "2382: [Discriminator loss: 0.555931, acc: 0.636719]  [Adversarial loss: 0.429655, acc: 0.722656]\n",
            "2383: [Discriminator loss: 0.537434, acc: 0.648438]  [Adversarial loss: 0.372672, acc: 0.789062]\n",
            "2384: [Discriminator loss: 0.592379, acc: 0.617188]  [Adversarial loss: 0.457416, acc: 0.761719]\n",
            "2385: [Discriminator loss: 0.545764, acc: 0.632812]  [Adversarial loss: 0.427207, acc: 0.765625]\n",
            "2386: [Discriminator loss: 0.536831, acc: 0.679688]  [Adversarial loss: 0.397825, acc: 0.765625]\n",
            "2387: [Discriminator loss: 0.523170, acc: 0.628906]  [Adversarial loss: 0.387684, acc: 0.804688]\n",
            "2388: [Discriminator loss: 0.542124, acc: 0.632812]  [Adversarial loss: 0.382607, acc: 0.800781]\n",
            "2389: [Discriminator loss: 0.556232, acc: 0.656250]  [Adversarial loss: 0.372896, acc: 0.757812]\n",
            "2390: [Discriminator loss: 0.588806, acc: 0.609375]  [Adversarial loss: 0.425417, acc: 0.789062]\n",
            "2391: [Discriminator loss: 0.518309, acc: 0.656250]  [Adversarial loss: 0.373761, acc: 0.781250]\n",
            "2392: [Discriminator loss: 0.550016, acc: 0.617188]  [Adversarial loss: 0.359008, acc: 0.792969]\n",
            "2393: [Discriminator loss: 0.588295, acc: 0.636719]  [Adversarial loss: 0.361827, acc: 0.796875]\n",
            "2394: [Discriminator loss: 0.512470, acc: 0.648438]  [Adversarial loss: 0.367288, acc: 0.777344]\n",
            "2395: [Discriminator loss: 0.557290, acc: 0.628906]  [Adversarial loss: 0.321353, acc: 0.859375]\n",
            "2396: [Discriminator loss: 0.581467, acc: 0.625000]  [Adversarial loss: 0.349285, acc: 0.839844]\n",
            "2397: [Discriminator loss: 0.556552, acc: 0.605469]  [Adversarial loss: 0.379572, acc: 0.816406]\n",
            "2398: [Discriminator loss: 0.555487, acc: 0.613281]  [Adversarial loss: 0.344917, acc: 0.796875]\n",
            "2399: [Discriminator loss: 0.566955, acc: 0.652344]  [Adversarial loss: 0.348995, acc: 0.800781]\n",
            "2400: [Discriminator loss: 0.594605, acc: 0.628906]  [Adversarial loss: 0.353618, acc: 0.812500]\n",
            "2401: [Discriminator loss: 0.639354, acc: 0.621094]  [Adversarial loss: 0.422564, acc: 0.765625]\n",
            "2402: [Discriminator loss: 0.571311, acc: 0.640625]  [Adversarial loss: 0.439545, acc: 0.738281]\n",
            "2403: [Discriminator loss: 0.496924, acc: 0.664062]  [Adversarial loss: 0.418957, acc: 0.789062]\n",
            "2404: [Discriminator loss: 0.662381, acc: 0.585938]  [Adversarial loss: 0.384214, acc: 0.765625]\n",
            "2405: [Discriminator loss: 0.679191, acc: 0.585938]  [Adversarial loss: 0.405350, acc: 0.765625]\n",
            "2406: [Discriminator loss: 0.576762, acc: 0.613281]  [Adversarial loss: 0.429395, acc: 0.753906]\n",
            "2407: [Discriminator loss: 0.652260, acc: 0.601562]  [Adversarial loss: 0.458456, acc: 0.769531]\n",
            "2408: [Discriminator loss: 0.506127, acc: 0.640625]  [Adversarial loss: 0.406392, acc: 0.804688]\n",
            "2409: [Discriminator loss: 0.582678, acc: 0.628906]  [Adversarial loss: 0.421833, acc: 0.796875]\n",
            "2410: [Discriminator loss: 0.593070, acc: 0.613281]  [Adversarial loss: 0.472503, acc: 0.785156]\n",
            "2411: [Discriminator loss: 0.575017, acc: 0.625000]  [Adversarial loss: 0.439384, acc: 0.746094]\n",
            "2412: [Discriminator loss: 0.619648, acc: 0.640625]  [Adversarial loss: 0.447223, acc: 0.718750]\n",
            "2413: [Discriminator loss: 0.547788, acc: 0.640625]  [Adversarial loss: 0.410050, acc: 0.812500]\n",
            "2414: [Discriminator loss: 0.557285, acc: 0.660156]  [Adversarial loss: 0.433934, acc: 0.742188]\n",
            "2415: [Discriminator loss: 0.572862, acc: 0.679688]  [Adversarial loss: 0.419650, acc: 0.746094]\n",
            "2416: [Discriminator loss: 0.501024, acc: 0.679688]  [Adversarial loss: 0.426485, acc: 0.726562]\n",
            "2417: [Discriminator loss: 0.558140, acc: 0.644531]  [Adversarial loss: 0.411046, acc: 0.753906]\n",
            "2418: [Discriminator loss: 0.533696, acc: 0.644531]  [Adversarial loss: 0.382760, acc: 0.781250]\n",
            "2419: [Discriminator loss: 0.502696, acc: 0.671875]  [Adversarial loss: 0.389752, acc: 0.773438]\n",
            "2420: [Discriminator loss: 0.481465, acc: 0.703125]  [Adversarial loss: 0.373736, acc: 0.789062]\n",
            "2421: [Discriminator loss: 0.544885, acc: 0.648438]  [Adversarial loss: 0.391547, acc: 0.792969]\n",
            "2422: [Discriminator loss: 0.509539, acc: 0.683594]  [Adversarial loss: 0.379057, acc: 0.792969]\n",
            "2423: [Discriminator loss: 0.625310, acc: 0.632812]  [Adversarial loss: 0.397744, acc: 0.789062]\n",
            "2424: [Discriminator loss: 0.601600, acc: 0.621094]  [Adversarial loss: 0.370590, acc: 0.804688]\n",
            "2425: [Discriminator loss: 0.559576, acc: 0.625000]  [Adversarial loss: 0.383572, acc: 0.820312]\n",
            "2426: [Discriminator loss: 0.534507, acc: 0.636719]  [Adversarial loss: 0.363128, acc: 0.765625]\n",
            "2427: [Discriminator loss: 0.537903, acc: 0.660156]  [Adversarial loss: 0.345667, acc: 0.839844]\n",
            "2428: [Discriminator loss: 0.612650, acc: 0.613281]  [Adversarial loss: 0.374346, acc: 0.796875]\n",
            "2429: [Discriminator loss: 0.554382, acc: 0.652344]  [Adversarial loss: 0.372140, acc: 0.765625]\n",
            "2430: [Discriminator loss: 0.499064, acc: 0.632812]  [Adversarial loss: 0.381651, acc: 0.820312]\n",
            "2431: [Discriminator loss: 0.521540, acc: 0.644531]  [Adversarial loss: 0.425646, acc: 0.746094]\n",
            "2432: [Discriminator loss: 0.516295, acc: 0.648438]  [Adversarial loss: 0.372670, acc: 0.765625]\n",
            "2433: [Discriminator loss: 0.460383, acc: 0.714844]  [Adversarial loss: 0.331119, acc: 0.828125]\n",
            "2434: [Discriminator loss: 0.533788, acc: 0.660156]  [Adversarial loss: 0.364610, acc: 0.804688]\n",
            "2435: [Discriminator loss: 0.523183, acc: 0.660156]  [Adversarial loss: 0.339054, acc: 0.832031]\n",
            "2436: [Discriminator loss: 0.580507, acc: 0.609375]  [Adversarial loss: 0.396794, acc: 0.781250]\n",
            "2437: [Discriminator loss: 0.513519, acc: 0.664062]  [Adversarial loss: 0.368611, acc: 0.832031]\n",
            "2438: [Discriminator loss: 0.567246, acc: 0.636719]  [Adversarial loss: 0.426470, acc: 0.765625]\n",
            "2439: [Discriminator loss: 0.533630, acc: 0.632812]  [Adversarial loss: 0.394328, acc: 0.777344]\n",
            "2440: [Discriminator loss: 0.550279, acc: 0.609375]  [Adversarial loss: 0.413617, acc: 0.804688]\n",
            "2441: [Discriminator loss: 0.553157, acc: 0.625000]  [Adversarial loss: 0.360462, acc: 0.785156]\n",
            "2442: [Discriminator loss: 0.496060, acc: 0.628906]  [Adversarial loss: 0.375405, acc: 0.808594]\n",
            "2443: [Discriminator loss: 0.553932, acc: 0.601562]  [Adversarial loss: 0.378805, acc: 0.800781]\n",
            "2444: [Discriminator loss: 0.538630, acc: 0.628906]  [Adversarial loss: 0.365212, acc: 0.812500]\n",
            "2445: [Discriminator loss: 0.569247, acc: 0.617188]  [Adversarial loss: 0.410761, acc: 0.765625]\n",
            "2446: [Discriminator loss: 0.609532, acc: 0.617188]  [Adversarial loss: 0.371746, acc: 0.816406]\n",
            "2447: [Discriminator loss: 0.517314, acc: 0.660156]  [Adversarial loss: 0.362128, acc: 0.800781]\n",
            "2448: [Discriminator loss: 0.637127, acc: 0.605469]  [Adversarial loss: 0.434676, acc: 0.804688]\n",
            "2449: [Discriminator loss: 0.561336, acc: 0.628906]  [Adversarial loss: 0.379311, acc: 0.828125]\n",
            "2450: [Discriminator loss: 0.492263, acc: 0.675781]  [Adversarial loss: 0.383632, acc: 0.824219]\n",
            "2451: [Discriminator loss: 0.601786, acc: 0.574219]  [Adversarial loss: 0.431973, acc: 0.746094]\n",
            "2452: [Discriminator loss: 0.532289, acc: 0.667969]  [Adversarial loss: 0.355825, acc: 0.808594]\n",
            "2453: [Discriminator loss: 0.528347, acc: 0.632812]  [Adversarial loss: 0.364860, acc: 0.796875]\n",
            "2454: [Discriminator loss: 0.624025, acc: 0.636719]  [Adversarial loss: 0.375712, acc: 0.820312]\n",
            "2455: [Discriminator loss: 0.529157, acc: 0.652344]  [Adversarial loss: 0.340345, acc: 0.828125]\n",
            "2456: [Discriminator loss: 0.534199, acc: 0.648438]  [Adversarial loss: 0.396999, acc: 0.785156]\n",
            "2457: [Discriminator loss: 0.585651, acc: 0.625000]  [Adversarial loss: 0.404978, acc: 0.773438]\n",
            "2458: [Discriminator loss: 0.575234, acc: 0.582031]  [Adversarial loss: 0.362317, acc: 0.808594]\n",
            "2459: [Discriminator loss: 0.513817, acc: 0.636719]  [Adversarial loss: 0.350465, acc: 0.820312]\n",
            "2460: [Discriminator loss: 0.531936, acc: 0.652344]  [Adversarial loss: 0.391220, acc: 0.804688]\n",
            "2461: [Discriminator loss: 0.554216, acc: 0.660156]  [Adversarial loss: 0.369357, acc: 0.785156]\n",
            "2462: [Discriminator loss: 0.468805, acc: 0.687500]  [Adversarial loss: 0.334731, acc: 0.812500]\n",
            "2463: [Discriminator loss: 0.549352, acc: 0.660156]  [Adversarial loss: 0.353801, acc: 0.785156]\n",
            "2464: [Discriminator loss: 0.524169, acc: 0.640625]  [Adversarial loss: 0.386159, acc: 0.765625]\n",
            "2465: [Discriminator loss: 0.509424, acc: 0.667969]  [Adversarial loss: 0.379797, acc: 0.792969]\n",
            "2466: [Discriminator loss: 0.548664, acc: 0.628906]  [Adversarial loss: 0.422666, acc: 0.738281]\n",
            "2467: [Discriminator loss: 0.541302, acc: 0.605469]  [Adversarial loss: 0.383761, acc: 0.796875]\n",
            "2468: [Discriminator loss: 0.530240, acc: 0.664062]  [Adversarial loss: 0.397930, acc: 0.753906]\n",
            "2469: [Discriminator loss: 0.548844, acc: 0.671875]  [Adversarial loss: 0.417987, acc: 0.781250]\n",
            "2470: [Discriminator loss: 0.475288, acc: 0.699219]  [Adversarial loss: 0.417302, acc: 0.789062]\n",
            "2471: [Discriminator loss: 0.537126, acc: 0.660156]  [Adversarial loss: 0.407610, acc: 0.761719]\n",
            "2472: [Discriminator loss: 0.568590, acc: 0.625000]  [Adversarial loss: 0.525751, acc: 0.710938]\n",
            "2473: [Discriminator loss: 0.635157, acc: 0.609375]  [Adversarial loss: 0.447367, acc: 0.734375]\n",
            "2474: [Discriminator loss: 0.581210, acc: 0.648438]  [Adversarial loss: 0.404324, acc: 0.785156]\n",
            "2475: [Discriminator loss: 0.573519, acc: 0.644531]  [Adversarial loss: 0.503159, acc: 0.695312]\n",
            "2476: [Discriminator loss: 0.602295, acc: 0.593750]  [Adversarial loss: 0.431753, acc: 0.769531]\n",
            "2477: [Discriminator loss: 0.541145, acc: 0.621094]  [Adversarial loss: 0.400597, acc: 0.789062]\n",
            "2478: [Discriminator loss: 0.561258, acc: 0.621094]  [Adversarial loss: 0.470390, acc: 0.703125]\n",
            "2479: [Discriminator loss: 0.527495, acc: 0.656250]  [Adversarial loss: 0.470412, acc: 0.734375]\n",
            "2480: [Discriminator loss: 0.552762, acc: 0.636719]  [Adversarial loss: 0.461258, acc: 0.750000]\n",
            "2481: [Discriminator loss: 0.507465, acc: 0.675781]  [Adversarial loss: 0.443204, acc: 0.730469]\n",
            "2482: [Discriminator loss: 0.498824, acc: 0.636719]  [Adversarial loss: 0.437117, acc: 0.773438]\n",
            "2483: [Discriminator loss: 0.591508, acc: 0.589844]  [Adversarial loss: 0.469656, acc: 0.761719]\n",
            "2484: [Discriminator loss: 0.526493, acc: 0.632812]  [Adversarial loss: 0.461436, acc: 0.734375]\n",
            "2485: [Discriminator loss: 0.525550, acc: 0.636719]  [Adversarial loss: 0.465044, acc: 0.804688]\n",
            "2486: [Discriminator loss: 0.484504, acc: 0.660156]  [Adversarial loss: 0.431378, acc: 0.789062]\n",
            "2487: [Discriminator loss: 0.529608, acc: 0.640625]  [Adversarial loss: 0.426022, acc: 0.781250]\n",
            "2488: [Discriminator loss: 0.550516, acc: 0.660156]  [Adversarial loss: 0.474625, acc: 0.726562]\n",
            "2489: [Discriminator loss: 0.591153, acc: 0.621094]  [Adversarial loss: 0.449183, acc: 0.742188]\n",
            "2490: [Discriminator loss: 0.592018, acc: 0.601562]  [Adversarial loss: 0.493508, acc: 0.714844]\n",
            "2491: [Discriminator loss: 0.510386, acc: 0.660156]  [Adversarial loss: 0.443442, acc: 0.734375]\n",
            "2492: [Discriminator loss: 0.580179, acc: 0.562500]  [Adversarial loss: 0.448324, acc: 0.730469]\n",
            "2493: [Discriminator loss: 0.489610, acc: 0.667969]  [Adversarial loss: 0.421632, acc: 0.785156]\n",
            "2494: [Discriminator loss: 0.554957, acc: 0.632812]  [Adversarial loss: 0.395198, acc: 0.761719]\n",
            "2495: [Discriminator loss: 0.553793, acc: 0.601562]  [Adversarial loss: 0.385742, acc: 0.773438]\n",
            "2496: [Discriminator loss: 0.567428, acc: 0.636719]  [Adversarial loss: 0.460748, acc: 0.742188]\n",
            "2497: [Discriminator loss: 0.489015, acc: 0.675781]  [Adversarial loss: 0.391171, acc: 0.843750]\n",
            "2498: [Discriminator loss: 0.512825, acc: 0.671875]  [Adversarial loss: 0.390905, acc: 0.785156]\n",
            "2499: [Discriminator loss: 0.524994, acc: 0.632812]  [Adversarial loss: 0.398129, acc: 0.804688]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXeijM5E9R7y"
      },
      "source": [
        "####Plot German Credit BBGAN Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "t76jNkM89RWA",
        "outputId": "c4d27a3f-a646-4902-a7c1-b6d5a8383469"
      },
      "source": [
        "plot_graphs(discriminator_losses, adversarial_losses, \"discriminator_loss\", \"adversarial_loss\", \"Discriminator and adversarial losses\", \"steps\", \"loss\")\n",
        "plot_graphs(discriminator_accuracies, adversarial_accuracies, \"discriminator_accuracy\", \"adversarial_accuracy\", \"Discriminator and adversarial accuracies\", \"steps\", \"accuracy\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tddr3ha69dbD"
      },
      "source": [
        "####Generate Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "UcDX0EbW9gb6",
        "outputId": "16a4324c-7b5c-4f50-a469-dc5818cdc1bb"
      },
      "source": [
        "gen_samples, c = german_credit_bbgan.generate_samples(num_samples=1000)\n",
        "gen_samples = pd.DataFrame(gen_samples)\n",
        "descaled_gen_samples = descale_german_credit_data(gen_samples, german_data_scaler)\n",
        "descaled_gen_samples = pd.DataFrame(descaled_gen_samples)\n",
        "decoded_descaled_gen_samples = decode_cat_features(descaled_gen_samples, bins_dict, group_names_dict)\n",
        "\n",
        "decoded_descaled_gen_samples\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A48'</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A72'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'A102'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A171'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>13.476525</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>6652.326172</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>2.167371</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>3.246438</td>\n",
              "      <td>b'A122'</td>\n",
              "      <td>23.653664</td>\n",
              "      <td>b'A141'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.302127</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.101174</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A202'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A48'</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A72'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'A102'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A171'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A48'</td>\n",
              "      <td>250.001236</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A72'</td>\n",
              "      <td>1.000001</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A102'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A171'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'A48'</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'A72'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'A102'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A171'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A94'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A202'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A48'</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A72'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'A102'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A171'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A48'</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A72'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A102'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A171'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A48'</td>\n",
              "      <td>250.000015</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A72'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A102'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A171'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>4.005726</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>263.781708</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>3.986575</td>\n",
              "      <td>b'A94'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>1.003180</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>19.002514</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.000009</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.000133</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A202'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0          1       2       3   ...       16        17       18       19\n",
              "0    b'A11'   4.000000  b'A34'  b'A48'  ...  b'A171'  1.000000      NaN  b'A201'\n",
              "1    b'A11'  13.476525  b'A34'  b'A43'  ...  b'A173'  1.101174  b'A192'  b'A202'\n",
              "2       NaN   4.000000  b'A34'  b'A48'  ...  b'A171'  1.000000      NaN      NaN\n",
              "3    b'A11'   4.000000  b'A34'  b'A48'  ...  b'A171'  1.000000  b'A192'  b'A201'\n",
              "4       NaN   4.000000     NaN  b'A48'  ...  b'A171'  1.000000      NaN      NaN\n",
              "..      ...        ...     ...     ...  ...      ...       ...      ...      ...\n",
              "995  b'A11'   4.000000  b'A34'  b'A43'  ...  b'A173'  1.000000  b'A192'  b'A202'\n",
              "996  b'A11'   4.000000  b'A34'  b'A48'  ...  b'A171'  1.000000      NaN  b'A201'\n",
              "997  b'A11'   4.000000  b'A34'  b'A48'  ...  b'A171'  1.000000  b'A192'  b'A201'\n",
              "998  b'A11'   4.000000  b'A34'  b'A48'  ...  b'A171'  1.000000  b'A192'  b'A201'\n",
              "999  b'A11'   4.005726  b'A34'  b'A43'  ...  b'A173'  1.000133  b'A192'  b'A202'\n",
              "\n",
              "[1000 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v3NyVxhXe7Z"
      },
      "source": [
        "y = german_credit_rf_clf.predict_proba(descaled_gen_samples)[:, 1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "KjQCZktQXd2_",
        "outputId": "9dd84f43-94ac-48bf-94ed-c0e08970fdb0"
      },
      "source": [
        "plt.hist(y, rwidth=0.8)\n",
        "plt.xlabel(\"Confidence\")\n",
        "plt.ylabel(\"Number of cases\")\n",
        "plt.title(\"Confidence scores distribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}